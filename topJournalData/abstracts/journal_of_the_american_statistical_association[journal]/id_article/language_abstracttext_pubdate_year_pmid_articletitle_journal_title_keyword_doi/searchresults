{"index":{"0":0,"1":1,"2":2,"3":3,"4":4,"5":5,"6":6,"7":7,"8":8,"9":9,"10":10,"11":11,"12":12,"13":13,"14":14,"15":15,"16":16,"17":17,"18":18,"19":19,"20":20,"21":21,"22":22,"23":23,"24":24,"25":25,"26":26,"27":27,"28":28,"29":29,"30":30,"31":31,"32":32,"33":33,"34":34,"35":35,"36":36,"37":37,"38":38,"39":39,"40":40,"41":41,"42":42,"43":43,"44":44,"45":45,"46":46,"47":47,"48":48,"49":49,"50":50,"51":51,"52":52,"53":53,"54":54,"55":55,"56":56,"57":57,"58":58,"59":59,"60":60,"61":61,"62":62,"63":63,"64":64,"65":65,"66":66,"67":67,"68":68,"69":69,"70":70,"71":71,"72":72,"73":73,"74":74,"75":75,"76":76,"77":77,"78":78,"79":79,"80":80,"81":81,"82":82,"83":83,"84":84,"85":85,"86":86,"87":87,"88":88,"89":89,"90":90,"91":91,"92":92,"93":93,"94":94,"95":95,"96":96,"97":97,"98":98,"99":99,"100":100,"101":101,"102":102,"103":103,"104":104,"105":105,"106":106,"107":107,"108":108,"109":109,"110":110,"111":111,"112":112,"113":113,"114":114,"115":115,"116":116,"117":117,"118":118,"119":119,"120":120,"121":121,"122":122,"123":123,"124":124,"125":125,"126":126,"127":127,"128":128,"129":129,"130":130,"131":131,"132":132,"133":133,"134":134,"135":135,"136":136,"137":137,"138":138,"139":139,"140":140,"141":141,"142":142,"143":143,"144":144,"145":145,"146":146,"147":147,"148":148,"149":149,"150":150,"151":151,"152":152,"153":153,"154":154,"155":155,"156":156,"157":157,"158":158,"159":159,"160":160,"161":161,"162":162,"163":163,"164":164,"165":165,"166":166,"167":167,"168":168,"169":169,"170":170,"171":171,"172":172,"173":173,"174":174,"175":175,"176":176,"177":177,"178":178,"179":179,"180":180,"181":181,"182":182,"183":183,"184":184,"185":185,"186":186,"187":187,"188":188,"189":189,"190":190,"191":191,"192":192,"193":193,"194":194,"195":195,"196":196,"197":197,"198":198,"199":199,"200":200,"201":201,"202":202,"203":203,"204":204,"205":205,"206":206,"207":207,"208":208,"209":209,"210":210,"211":211,"212":212,"213":213,"214":214,"215":215,"216":216,"217":217,"218":218,"219":219,"220":220,"221":221,"222":222,"223":223,"224":224,"225":225,"226":226,"227":227,"228":228,"229":229,"230":230,"231":231,"232":232,"233":233,"234":234,"235":235,"236":236,"237":237,"238":238,"239":239,"240":240,"241":241,"242":242,"243":243,"244":244,"245":245,"246":246,"247":247,"248":248,"249":249,"250":250,"251":251,"252":252,"253":253,"254":254,"255":255,"256":256,"257":257,"258":258,"259":259,"260":260,"261":261,"262":262,"263":263,"264":264,"265":265,"266":266,"267":267,"268":268,"269":269,"270":270,"271":271,"272":272,"273":273,"274":274,"275":275,"276":276,"277":277,"278":278,"279":279,"280":280,"281":281,"282":282,"283":283,"284":284,"285":285,"286":286,"287":287,"288":288,"289":289,"290":290,"291":291,"292":292,"293":293,"294":294,"295":295,"296":296,"297":297,"298":298,"299":299,"300":300,"301":301,"302":302,"303":303,"304":304,"305":305,"306":306,"307":307,"308":308,"309":309,"310":310,"311":311,"312":312,"313":313,"314":314,"315":315,"316":316,"317":317,"318":318,"319":319,"320":320,"321":321,"322":322,"323":323,"324":324,"325":325,"326":326,"327":327,"328":328,"329":329,"330":330,"331":331,"332":332,"333":333,"334":334,"335":335,"336":336,"337":337,"338":338,"339":339,"340":340,"341":341,"342":342,"343":343,"344":344,"345":345,"346":346,"347":347,"348":348,"349":349,"350":350,"351":351,"352":352,"353":353,"354":354,"355":355,"356":356,"357":357,"358":358,"359":359,"360":360,"361":361,"362":362,"363":363,"364":364,"365":365,"366":366,"367":367,"368":368,"369":369,"370":370,"371":371,"372":372,"373":373,"374":374,"375":375,"376":376,"377":377,"378":378,"379":379,"380":380,"381":381,"382":382,"383":383,"384":384,"385":385,"386":386,"387":387,"388":388,"389":389,"390":390,"391":391,"392":392,"393":393,"394":394,"395":395,"396":396,"397":397,"398":398,"399":399,"400":400,"401":401,"402":402,"403":403,"404":404,"405":405,"406":406,"407":407,"408":408,"409":409,"410":410,"411":411,"412":412,"413":413,"414":414,"415":415,"416":416,"417":417,"418":418,"419":419,"420":420,"421":421,"422":422,"423":423,"424":424,"425":425,"426":426,"427":427,"428":428,"429":429,"430":430,"431":431,"432":432,"433":433,"434":434,"435":435,"436":436,"437":437,"438":438,"439":439,"440":440,"441":441,"442":442,"443":443,"444":444,"445":445,"446":446,"447":447,"448":448,"449":449,"450":450,"451":451,"452":452,"453":453,"454":454,"455":455,"456":456,"457":457,"458":458,"459":459,"460":460,"461":461,"462":462,"463":463,"464":464,"465":465,"466":466,"467":467,"468":468,"469":469,"470":470,"471":471,"472":472,"473":473,"474":474,"475":475,"476":476,"477":477,"478":478,"479":479,"480":480,"481":481,"482":482,"483":483,"484":484,"485":485,"486":486,"487":487,"488":488,"489":489,"490":490,"491":491,"492":492,"493":493,"494":494,"495":495,"496":496,"497":497,"498":498,"499":499,"500":500,"501":501,"502":502,"503":503,"504":504,"505":505,"506":506,"507":507,"508":508,"509":509,"510":510,"511":511,"512":512,"513":513,"514":514,"515":515,"516":516,"517":517,"518":518,"519":519,"520":520,"521":521,"522":522,"523":523,"524":524,"525":525,"526":526,"527":527,"528":528,"529":529,"530":530,"531":531,"532":532,"533":533,"534":534,"535":535,"536":536,"537":537,"538":538,"539":539,"540":540,"541":541,"542":542,"543":543,"544":544,"545":545,"546":546,"547":547,"548":548,"549":549,"550":550,"551":551,"552":552,"553":553,"554":554,"555":555,"556":556,"557":557,"558":558,"559":559,"560":560,"561":561,"562":562,"563":563,"564":564,"565":565,"566":566,"567":567,"568":568,"569":569,"570":570,"571":571,"572":572,"573":573,"574":574,"575":575,"576":576,"577":577,"578":578,"579":579,"580":580,"581":581,"582":582,"583":583,"584":584,"585":585,"586":586,"587":587,"588":588,"589":589,"590":590,"591":591,"592":592,"593":593,"594":594,"595":595,"596":596,"597":597,"598":598,"599":599,"600":600,"601":601,"602":602,"603":603,"604":604,"605":605,"606":606,"607":607,"608":608,"609":609,"610":610,"611":611,"612":612,"613":613,"614":614,"615":615,"616":616,"617":617,"618":618,"619":619,"620":620,"621":621,"622":622,"623":623,"624":624,"625":625,"626":626,"627":627,"628":628,"629":629,"630":630,"631":631,"632":632,"633":633,"634":634,"635":635,"636":636,"637":637,"638":638,"639":639,"640":640,"641":641,"642":642,"643":643,"644":644,"645":645,"646":646,"647":647,"648":648,"649":649,"650":650,"651":651,"652":652,"653":653,"654":654,"655":655,"656":656,"657":657,"658":658,"659":659,"660":660,"661":661,"662":662,"663":663,"664":664,"665":665,"666":666,"667":667,"668":668,"669":669,"670":670,"671":671,"672":672,"673":673,"674":674,"675":675,"676":676,"677":677,"678":678,"679":679,"680":680,"681":681,"682":682,"683":683,"684":684,"685":685,"686":686,"687":687,"688":688,"689":689,"690":690,"691":691,"692":692,"693":693,"694":694,"695":695,"696":696,"697":697,"698":698,"699":699,"700":700,"701":701,"702":702,"703":703,"704":704,"705":705,"706":706,"707":707,"708":708,"709":709,"710":710,"711":711,"712":712,"713":713,"714":714,"715":715,"716":716,"717":717,"718":718,"719":719,"720":720,"721":721,"722":722,"723":723,"724":724,"725":725,"726":726,"727":727,"728":728,"729":729,"730":730,"731":731,"732":732,"733":733,"734":734,"735":735,"736":736,"737":737,"738":738,"739":739,"740":740,"741":741,"742":742,"743":743,"744":744,"745":745,"746":746,"747":747,"748":748,"749":749,"750":750,"751":751,"752":752},"articleID":{"0":0,"1":1,"2":2,"3":3,"4":4,"5":5,"6":6,"7":7,"8":8,"9":9,"10":10,"11":11,"12":12,"13":13,"14":14,"15":15,"16":16,"17":17,"18":18,"19":19,"20":20,"21":21,"22":22,"23":23,"24":24,"25":25,"26":26,"27":27,"28":28,"29":29,"30":30,"31":31,"32":32,"33":33,"34":34,"35":35,"36":36,"37":37,"38":38,"39":39,"40":40,"41":41,"42":42,"43":43,"44":44,"45":45,"46":46,"47":47,"48":48,"49":49,"50":50,"51":51,"52":52,"53":53,"54":54,"55":55,"56":56,"57":57,"58":58,"59":59,"60":60,"61":61,"62":62,"63":63,"64":64,"65":65,"66":66,"67":67,"68":68,"69":69,"70":70,"71":71,"72":72,"73":73,"74":74,"75":75,"76":76,"77":77,"78":78,"79":79,"80":80,"81":81,"82":82,"83":83,"84":84,"85":85,"86":86,"87":87,"88":88,"89":89,"90":90,"91":91,"92":92,"93":93,"94":94,"95":95,"96":96,"97":97,"98":98,"99":99,"100":100,"101":101,"102":102,"103":103,"104":104,"105":105,"106":106,"107":107,"108":108,"109":109,"110":110,"111":111,"112":112,"113":113,"114":114,"115":115,"116":116,"117":117,"118":118,"119":119,"120":120,"121":121,"122":122,"123":123,"124":124,"125":125,"126":126,"127":127,"128":128,"129":129,"130":130,"131":131,"132":132,"133":133,"134":134,"135":135,"136":136,"137":137,"138":138,"139":139,"140":140,"141":141,"142":142,"143":143,"144":144,"145":145,"146":146,"147":147,"148":148,"149":149,"150":150,"151":151,"152":152,"153":153,"154":154,"155":155,"156":156,"157":157,"158":158,"159":159,"160":160,"161":161,"162":162,"163":163,"164":164,"165":165,"166":166,"167":167,"168":168,"169":169,"170":170,"171":171,"172":172,"173":173,"174":174,"175":175,"176":176,"177":177,"178":178,"179":179,"180":180,"181":181,"182":182,"183":183,"184":184,"185":185,"186":186,"187":187,"188":188,"189":189,"190":190,"191":191,"192":192,"193":193,"194":194,"195":195,"196":196,"197":197,"198":198,"199":199,"200":200,"201":201,"202":202,"203":203,"204":204,"205":205,"206":206,"207":207,"208":208,"209":209,"210":210,"211":211,"212":212,"213":213,"214":214,"215":215,"216":216,"217":217,"218":218,"219":219,"220":220,"221":221,"222":222,"223":223,"224":224,"225":225,"226":226,"227":227,"228":228,"229":229,"230":230,"231":231,"232":232,"233":233,"234":234,"235":235,"236":236,"237":237,"238":238,"239":239,"240":240,"241":241,"242":242,"243":243,"244":244,"245":245,"246":246,"247":247,"248":248,"249":249,"250":250,"251":251,"252":252,"253":253,"254":254,"255":255,"256":256,"257":257,"258":258,"259":259,"260":260,"261":261,"262":262,"263":263,"264":264,"265":265,"266":266,"267":267,"268":268,"269":269,"270":270,"271":271,"272":272,"273":273,"274":274,"275":275,"276":276,"277":277,"278":278,"279":279,"280":280,"281":281,"282":282,"283":283,"284":284,"285":285,"286":286,"287":287,"288":288,"289":289,"290":290,"291":291,"292":292,"293":293,"294":294,"295":295,"296":296,"297":297,"298":298,"299":299,"300":300,"301":301,"302":302,"303":303,"304":304,"305":305,"306":306,"307":307,"308":308,"309":309,"310":310,"311":311,"312":312,"313":313,"314":314,"315":315,"316":316,"317":317,"318":318,"319":319,"320":320,"321":321,"322":322,"323":323,"324":324,"325":325,"326":326,"327":327,"328":328,"329":329,"330":330,"331":331,"332":332,"333":333,"334":334,"335":335,"336":336,"337":337,"338":338,"339":339,"340":340,"341":341,"342":342,"343":343,"344":344,"345":345,"346":346,"347":347,"348":348,"349":349,"350":350,"351":351,"352":352,"353":353,"354":354,"355":355,"356":356,"357":357,"358":358,"359":359,"360":360,"361":361,"362":362,"363":363,"364":364,"365":365,"366":366,"367":367,"368":368,"369":369,"370":370,"371":371,"372":372,"373":373,"374":374,"375":375,"376":376,"377":377,"378":378,"379":379,"380":380,"381":381,"382":382,"383":383,"384":384,"385":385,"386":386,"387":387,"388":388,"389":389,"390":390,"391":391,"392":392,"393":393,"394":394,"395":395,"396":396,"397":397,"398":398,"399":399,"400":400,"401":401,"402":402,"403":403,"404":404,"405":405,"406":406,"407":407,"408":408,"409":409,"410":410,"411":411,"412":412,"413":413,"414":414,"415":415,"416":416,"417":417,"418":418,"419":419,"420":420,"421":421,"422":422,"423":423,"424":424,"425":425,"426":426,"427":427,"428":428,"429":429,"430":430,"431":431,"432":432,"433":433,"434":434,"435":435,"436":436,"437":437,"438":438,"439":439,"440":440,"441":441,"442":442,"443":443,"444":444,"445":445,"446":446,"447":447,"448":448,"449":449,"450":450,"451":451,"452":452,"453":453,"454":454,"455":455,"456":456,"457":457,"458":458,"459":459,"460":460,"461":461,"462":462,"463":463,"464":464,"465":465,"466":466,"467":467,"468":468,"469":469,"470":470,"471":471,"472":472,"473":473,"474":474,"475":475,"476":476,"477":477,"478":478,"479":479,"480":480,"481":481,"482":482,"483":483,"484":484,"485":485,"486":486,"487":487,"488":488,"489":489,"490":490,"491":491,"492":492,"493":493,"494":494,"495":495,"496":496,"497":497,"498":498,"499":499,"500":500,"501":501,"502":502,"503":503,"504":504,"505":505,"506":506,"507":507,"508":508,"509":509,"510":510,"511":511,"512":512,"513":513,"514":514,"515":515,"516":516,"517":517,"518":518,"519":519,"520":520,"521":521,"522":522,"523":523,"524":524,"525":525,"526":526,"527":527,"528":528,"529":529,"530":530,"531":531,"532":532,"533":533,"534":534,"535":535,"536":536,"537":537,"538":538,"539":539,"540":540,"541":541,"542":542,"543":543,"544":544,"545":545,"546":546,"547":547,"548":548,"549":549,"550":550,"551":551,"552":552,"553":553,"554":554,"555":555,"556":556,"557":557,"558":558,"559":559,"560":560,"561":561,"562":562,"563":563,"564":564,"565":565,"566":566,"567":567,"568":568,"569":569,"570":570,"571":571,"572":572,"573":573,"574":574,"575":575,"576":576,"577":577,"578":578,"579":579,"580":580,"581":581,"582":582,"583":583,"584":584,"585":585,"586":586,"587":587,"588":588,"589":589,"590":590,"591":591,"592":592,"593":593,"594":594,"595":595,"596":596,"597":597,"598":598,"599":599,"600":600,"601":601,"602":602,"603":603,"604":604,"605":605,"606":606,"607":607,"608":608,"609":609,"610":610,"611":611,"612":612,"613":613,"614":614,"615":615,"616":616,"617":617,"618":618,"619":619,"620":620,"621":621,"622":622,"623":623,"624":624,"625":625,"626":626,"627":627,"628":628,"629":629,"630":630,"631":631,"632":632,"633":633,"634":634,"635":635,"636":636,"637":637,"638":638,"639":639,"640":640,"641":641,"642":642,"643":643,"644":644,"645":645,"646":646,"647":647,"648":648,"649":649,"650":650,"651":651,"652":652,"653":653,"654":654,"655":655,"656":656,"657":657,"658":658,"659":659,"660":660,"661":661,"662":662,"663":663,"664":664,"665":665,"666":666,"667":667,"668":668,"669":669,"670":670,"671":671,"672":672,"673":673,"674":674,"675":675,"676":676,"677":677,"678":678,"679":679,"680":680,"681":681,"682":682,"683":683,"684":684,"685":685,"686":686,"687":687,"688":688,"689":689,"690":690,"691":691,"692":692,"693":693,"694":694,"695":695,"696":696,"697":697,"698":698,"699":699,"700":700,"701":701,"702":702,"703":703,"704":704,"705":705,"706":706,"707":707,"708":708,"709":709,"710":710,"711":711,"712":712,"713":713,"714":714,"715":715,"716":716,"717":717,"718":718,"719":719,"720":720,"721":721,"722":722,"723":723,"724":724,"725":725,"726":726,"727":727,"728":728,"729":729,"730":730,"731":731,"732":732,"733":733,"734":734,"735":735,"736":736,"737":737,"738":738,"739":739,"740":740,"741":741,"742":742,"743":743,"744":744,"745":745,"746":746,"747":747,"748":748,"749":749,"750":750,"751":751,"752":752},"abstracttext":{"0":"Hundreds of autism risk genes have been reported recently, mainly based on genetic studies where these risk genes have more de novo mutations in autism subjects than healthy controls. However, as a complex disease, autism is likely associated with more risk genes and many of them may not be identifiable through de novo mutations. We hypothesize that more autism risk genes can be identified through their connections with known autism risk genes in personalized gene-gene interaction graphs. We estimate such personalized graphs using single cell RNA sequencing (scRNA-seq) while appropriately modeling the cell dependence and possible zero-inflation in the scRNA-seq data. The sample size, which is the number of cells per individual, ranges from 891 to 1,241 in our case study using scRNA-seq data in autism subjects and controls. We consider 1,500 genes in our analysis. Since the number of genes is larger or comparable to the sample size, we perform penalized estimation. We score each gene's relevance by applying a simple graph kernel smoothing method to each personalized graph. The molecular functions of the top-scored genes are related to autism diseases. For example, a candidate gene RYR2 that encodes protein ryanodine receptor 2 is involved in neurotransmission, a process that is impaired in ASD patients. While our method provides a systemic and unbiased approach to prioritize autism risk genes, the relevance of these genes needs to be further validated in functional studies.","1":"Mediation analysis is of rising interest in epidemiology and clinical trials. Among existing methods, the joint significance (JS) test yields an overly conservative type I error rate and low power, particularly for high-dimensional mediation hypotheses. In this article we develop a multiple-testing procedure that accurately controls the family-wise error rate (FWER) and the false discovery rate (FDR) when testing high-dimensional mediation hypotheses. The core of our procedure is based on estimating the proportions of component null hypotheses and the underlying mixture null distribution of p-values. Theoretical developments and simulation experiments prove that the proposed procedure effectively controls FWER and FDR. Two mediation analyses on DNA methylation and cancer research are presented: assessing the mediation role of DNA methylation in genLetic regulation of gene expression in primary prostate cancer samples; exploring the possibility of DNA methylation mediating the effect of exercise on prostate cancer progression. Results of data examples include wellL-behaved quantile-quantile plots and improved power to detect novel mediation relationships. An R package HDMT implementing the proposed procedure is freely accessible in CRAN.","2":null,"3":"Brain-imaging data have been increasingly used to understand intellectual disabilities. Despite significant progress in biomedical research, the mechanisms for most of the intellectual disabilities remain unknown. Finding the underlying neurological mechanisms has been proved difficult, especially in children due to the rapid development of their brains. We investigate verbal reasoning, which is a reliable measure of individuals' general intellectual abilities, and develop a class of high-order imaging regression models to identify brain subregions which might be associated with this specific intellectual ability. A key novelty of our method is to take advantage of spatial brain structures, and specifically the piecewise smooth nature of most imaging coefficients in the form of high-order tensors. Our approach provides an effective and urgently needed method for identifying brain subregions potentially underlying certain intellectual disabilities. The idea behind our approach is a carefully constructed concept called Internal Variation (IV). The IV employs tensor decomposition and provides a computationally feasible substitution for Total Variation (TV), which has been considered in the literature to deal with similar problems but is problematic in high order tensor regression. Before applying our method to analyze the real data, we conduct comprehensive simulation studies to demonstrate the validity of our method in imaging signal identification. Then, we present our results from the analysis of a dataset based on the Philadelphia Neurodevelopmental Cohort for which we preprocessed the data including re-orienting, bias-field correcting, extracting, normalizing and registering the magnetic resonance images from 978 individuals. Our analysis identified a subregion across the cingulate cortex and the corpus callosum as being associated with individuals' verbal reasoning ability, which, to the best of our knowledge, is a novel region that has not been reported in the literature. This finding is useful in further investigation of functional mechansims for verbal reasoning.","4":"This article is motivated by the problem of inference on interactions among chemical exposures impacting human health outcomes. Chemicals often co-occur in the environment or in synthetic mixtures and as a result exposure levels can be highly correlated. We propose a latent factor joint model, which includes shared factors in both the predictor and response components while assuming conditional independence. By including a quadratic regression in the latent variables in the response component, we induce flexible dimension reduction in characterizing main effects and interactions. We propose a Bayesian approach to inference under this Factor analysis for INteractions (FIN) framework. Through appropriate modifications of the factor modeling structure, FIN can accommodate higher order interactions. We evaluate the performance using a simulation study and data from the National Health and Nutrition Examination Survey (NHANES). Code is available on GitHub.","5":"Estimating the marginal and joint densities of the long-term average intakes of different dietary components is an important problem in nutritional epidemiology. Since these variables cannot be directly measured, data are usually collected in the form of 24-hour recalls of the intakes, which show marked patterns of conditional heteroscedasticity. Significantly compounding the challenges, the recalls for episodically consumed dietary components also include exact zeros. The problem of estimating the density of the latent long-time intakes from their observed measurement error contaminated proxies is then a problem of deconvolution of densities with zero-inflated data. We propose a Bayesian semiparametric solution to the problem, building on a novel hierarchical latent variable framework that translates the problem to one involving continuous surrogates only. Crucial to accommodating important aspects of the problem, we then design a copula based approach to model the involved joint distributions, adopting different modeling strategies for the marginals of the different dietary components. We design efficient Markov chain Monte Carlo algorithms for posterior inference and illustrate the efficacy of the proposed method through simulation experiments. Applied to our motivating nutritional epidemiology problems, compared to other approaches, our method provides more realistic estimates of the consumption patterns of episodically consumed dietary components.","6":null,"7":"For many mental disorders, latent mental status from multiple-domain psychological or clinical symptoms may perform as a better characterization of the underlying disorder status than a simple summary score of the symptoms, and they may also serve as more reliable and representative features to differentiate treatment responses. Therefore, in order to address the complexity and heterogeneity of treatment responses for mental disorders, we provide a new paradigm for learning optimal individualized treatment rules (ITRs) by modeling patients' latent mental status. We first learn the multi-domain latent states at baseline from the observed symptoms under a restricted Boltzmann machine (RBM) model, through which patients' heterogeneous symptoms are represented using an economical number of latent variables and yet remains flexible. We then optimize a value function defined by the latent states after treatment by exploiting a transformation of the observed symptoms based on the RBM without modeling the relationship between the latent mental states before and after treatment. The optimal treatment rules are derived using a weighted large margin classifier. We derive the convergence rate of the proposed estimator under the latent models. Simulation studies are conducted to test the performance of the proposed method. Finally, we apply the developed method to real world studies and we demonstrate the utility and advantage of our method in tailoring treatments for patients with major depression, and identify patient subgroups informative for treatment recommendations.","8":"Exponential-family singular value decomposition (eSVD) is a new approach for embedding multivariate data into a lower-dimensional space. It provides an elegant dimension reduction framework with flexibility to handle one-parameter exponential family distributions and proven consistency. This approach adds a valuable new tool to the toolbox of data analysts. Here we discuss a number of open problems and challenges that remain to be addressed in the future in order to unleash the full potential of eSVD and other similar approaches.","9":"Understanding how adult humans learn nonnative speech categories such as tone information has shed novel insights into the mechanisms underlying experience-dependent brain plasticity. Scientists have traditionally examined these questions using longitudinal learning experiments under a multi-category decision making paradigm. Drift-diffusion processes are popular in such contexts for their ability to mimic underlying neural mechanisms. Motivated by these problems, we develop a novel Bayesian semiparametric inverse Gaussian drift-diffusion mixed model for multi-alternative decision making in longitudinal settings. We design a Markov chain Monte Carlo algorithm for posterior computation. We evaluate the method's empirical performances through synthetic experiments. Applied to our motivating longitudinal tone learning study, the method provides novel insights into how the biologically interpretable model parameters evolve with learning, differ between input-response tone combinations, and differ between well and poorly performing adults. supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","10":null,"11":"The complexity of human cancer often results in significant heterogeneity in response to treatment. Precision medicine offers the potential to improve patient outcomes by leveraging this heterogeneity. Individualized treatment rules (ITRs) formalize precision medicine as maps from the patient covariate space into the space of allowable treatments. The optimal ITR is that which maximizes the mean of a clinical outcome in a population of interest. Patient-derived xenograft (PDX) studies permit the evaluation of multiple treatments within a single tumor, and thus are ideally suited for estimating optimal ITRs. PDX data are characterized by correlated outcomes, a high-dimensional feature space, and a large number of treatments. Here we explore machine learning methods for estimating optimal ITRs from PDX data. We analyze data from a large PDX study to identify biomarkers that are informative for developing personalized treatment recommendations in multiple cancers. We estimate optimal ITRs using regression-based (Q-learning) and direct-search methods (outcome weighted learning). Finally, we implement a superlearner approach to combine multiple estimated ITRs and show that the resulting ITR performs better than any of the input ITRs, mitigating uncertainty regarding user choice. Our results indicate that PDX data are a valuable resource for developing individualized treatment strategies in oncology. Supplementary materials for this article are available online.","12":"Analysis of high dimensional data has received considerable and increasing attention in statistics. In practice, we may not be interested in every variable that is observed. Instead, often some of the variables are of particular interest, and the remaining variables are nuisance. To this end, we propose the nuisance penalized regression which does not penalize the parameters of interest. When the coherence between interest parameters and nuisance parameters is negligible, we show that resulting estimator can be directly used for inference without any correction. When the coherence is not negligible, we propose an iteratively procedure to further refine the estimate of interest parameters, based on which we propose a modified profile likelihood based statistic for hypothesis testing. The utilities of our general results are demonstrated in three specific examples. Numerical studies lend further support to our method.","13":"In this paper, we develop a new estimation and valid inference method for single or low-dimensional regression coefficients in high-dimensional generalized linear models. The number of the predictors is allowed to grow exponentially fast with respect to the sample size. The proposed estimator is computed by solving a score function. We recursively conduct model selection to reduce the dimensionality from high to a moderate scale and construct the score equation based on the selected variables. The proposed confidence interval (CI) achieves valid coverage without assuming consistency of the model selection procedure. When the selection consistency is achieved, we show the length of the proposed CI is asymptotically the same as the CI of the \"oracle\" method which works as well as if the support of the control variables were known. In addition, we prove the proposed CI is asymptotically narrower than the CIs constructed based on the de-sparsified Lasso estimator (van de Geer et al., 2014) and the decorrelated score statistic (Ning and Liu, 2017). Simulation studies and real data applications are presented to back up our theoretical findings.","14":"Mediation analysis is critical to understanding the mechanisms underlying exposure-outcome relationships. In this paper, we identify the instrumental variable-direct effect of the exposure on the outcome not through the mediator, using randomization of the instrument. We call this estimand the complier stochastic direct effect (CSDE). To our knowledge, such an estimand has not previously been considered or estimated. We propose and evaluate several estimators for the CSDE: a ratio of inverse-probability of treatment-weighted estimators (IPTW), a ratio of estimating equation estimators (EE), a ratio of targeted minimum loss-based estimators (TMLE), and a TMLE that targets the CSDE directly. These estimators are applicable for a variety of study designs, including randomized encouragement trials, like the Moving to Opportunity housing voucher experiment we consider as an illustrative example, treatment discontinuities, and Mendelian randomization. We found the IPTW estimator to be the most sensitive to finite sample bias, resulting in bias of over 40% even when all models were correctly specified in a sample size of N=100. In contrast, the EE estimator and TMLE that targets the CSDE directly were far less sensitive. The EE and TML estimators also have advantages in terms of efficiency and reduced reliance on correct parametric model specification.","15":"We discuss the results on improving the generalizability of individualized treatment rule following the work in Kallus [1] and Mo et al. [5]. We note that the advocated weights in Kallus [1] are connected to the efficient score of the contrast function. We further propose a likelihood-ratio-based method (LR-ITR) to accommodate covariate shifts, and compare it to the CTE-DR-ITR method proposed in Mo et al. [5]. We provide the upper-bound on the risk function of the target population when both the covariate shift and the contrast function shift are present. Numerical studies show that LR-ITR can outperform CTE-DR-ITR when there is only covariate shift.","16":"Large-scale genome-wide association (GWAS) studies provide opportunities for developing genetic risk prediction models that have the potential to improve disease prevention, intervention or treatment. The key step is to develop polygenic risk score (PRS) models with high predictive performance for a given disease, which typically requires a large training data set for selecting truly associated single nucleotide polymorphisms (SNPs) and estimating effect sizes accurately. Here, we develop a comprehensive penalized regression for fitting l 1 regularized regression models to GWAS summary statistics. We propose incorporating Pleiotropy and ANnotation information into PRS (PANPRS) development through suitable formulation of penalty functions and associated tuning parameters. Extensive simulations show that PANPRS performs equally well or better than existing PRS methods when no functional annotation or pleiotropy is incorporated. When functional annotation data and pleiotropy are informative, PANPRS substantially outperforms existing PRS methods in simulations. Finally, we applied our methods to build PRS for type 2 diabetes and melanoma and found that incorporating relevant functional annotations and GWAS of genetically related traits improved prediction of these two complex diseases.","17":"High-dimensional logistic regression is widely used in analyzing data with binary outcomes. In this paper, global testing and large-scale multiple testing for the regression coefficients are considered in both single- and two-regression settings. A test statistic for testing the global null hypothesis is constructed using a generalized low-dimensional projection for bias correction and its asymptotic null distribution is derived. A lower bound for the global testing is established, which shows that the proposed test is asymptotically minimax optimal over some sparsity range. For testing the individual coefficients simultaneously, multiple testing procedures are proposed and shown to control the false discovery rate (FDR) and falsely discovered variables (FDV) asymptotically. Simulation studies are carried out to examine the numerical performance of the proposed tests and their superiority over existing methods. The testing procedures are also illustrated by analyzing a data set of a metabolomics study that investigates the association between fecal metabolites and pediatric Crohn's disease and the effects of treatment on such associations.","18":"Methods for inferring average causal effects have traditionally relied on two key assumptions: (i) the intervention received by one unit cannot causally influence the outcome of another; and (ii) units can be organized into nonoverlapping groups such that outcomes of units in separate groups are independent. In this article, we develop new statistical methods for causal inference based on a single realization of a network of connected units for which neither assumption (i) nor (ii) holds. The proposed approach allows both for arbitrary forms of interference, whereby the outcome of a unit may depend on interventions received by other units with whom a network path through connected units exists; and long range dependence, whereby outcomes for any two units likewise connected by a path in the network may be dependent. Under network versions of consistency and no unobserved confounding, inference is made tractable by an assumption that the networks outcome, treatment and covariate vectors are a single realization of a certain chain graph model. This assumption allows inferences about various network causal effects via the auto-g-computation algorithm, a network generalization of Robins' well-known g-computation algorithm previously described for causal inference under assumptions (i) and (ii). Supplementary materials for this article are available online.","19":"Scientists often embed cells into a lower-dimensional space when studying single-cell RNA-seq data for improved downstream analyses such as developmental trajectory analyses, but the statistical properties of such nonlinear embedding methods are often not well understood. In this article, we develop the exponential-family SVD (eSVD), a nonlinear embedding method for both cells and genes jointly with respect to a random dot product model using exponential-family distributions. Our estimator uses alternating minimization, which enables us to have a computationally efficient method, prove the identifiability conditions and consistency of our method, and provide statistically principled procedures to tune our method. All these qualities help advance the single-cell embedding literature, and we provide extensive simulations to demonstrate that the eSVD is competitive compared to other embedding methods. We apply the eSVD via Gaussian distributions where the standard deviations are proportional to the means to analyze a single-cell dataset of oligodendrocytes in mouse brains. Using the eSVD estimated embedding, we then investigate the cell developmental trajectories of the oligodendrocytes. While previous results are not able to distinguish the trajectories among the mature oligodendrocyte cell types, our diagnostics and results demonstrate there are two major developmental trajectories that diverge at mature oligodendrocytes. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplementary materials.","20":"Genetics plays a role in age-related macular degeneration (AMD), a common cause of blindness in the elderly. There is a need for powerful methods for carrying out region-based association tests between a dichotomous trait like AMD and genetic variants on family data. Here, we apply our new generalized functional linear mixed models (GFLMM) developed to test for gene-based association in a set of AMD families. Using common and rare variants, we observe significant association with two known AMD genes: CFH and ARMS2. Using rare variants, we find suggestive signals in four genes: ASAH1, CLEC6A, TMEM63C, and SGSM1. Intriguingly, ASAH1 is down-regulated in AMD aqueous humor, and ASAH1 deficiency leads to retinal inflammation and increased vulnerability to oxidative stress. These findings were made possible by our GFLMM which model the effect of a major gene as a fixed mean, the polygenic contributions as a random variation, and the correlation of pedigree members by kinship coefficients. Simulations indicate that the GFLMM likelihood ratio tests (LRTs) accurately control the Type I error rates. The LRTs have similar or higher power than existing retrospective kernel and burden statistics. Our GFLMM-based statistics provide a new tool for conducting family-based genetic studies of complex diseases. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","21":"Phylogenetic tree inference using deep DNA sequencing is reshaping our understanding of rapidly evolving systems, such as the within-host battle between viruses and the immune system. Densely sampled phylogenetic trees can contain special features, including sampled ancestors in which we sequence a genotype along with its direct descendants, and polytomies in which multiple descendants arise simultaneously. These features are apparent after identifying zero-length branches in the tree. However, current maximum-likelihood based approaches are not capable of revealing such zero-length branches. In this paper, we find these zero-length branches by introducing adaptive-LASSO-type regularization estimators for the branch lengths of phylogenetic trees, deriving their properties, and showing regularization to be a practically useful approach for phylogenetics.","22":"Investigating the similarity and changes in brain networks under different mental conditions has become increasingly important in neuroscience research. A standard separate estimation strategy fails to pool information across networks and hence has reduced estimation accuracy and power to detect between-network differences. Motivated by a fMRI Stroop task experiment that involves multiple related tasks, we develop an integrative Bayesian approach for jointly modeling multiple brain networks that provides a systematic inferential framework for network comparisons. The proposed approach explicitly models shared and differential patterns via flexible Dirichlet process-based priors on edge probabilities. Conditional on edges, the connection strengths are modeled via Bayesian spike and slab prior on the precision matrix off-diagonals. Numerical simulations illustrate that the proposed approach has increased power to detect true differential edges while providing adequate control on false positives and achieves greater network estimation accuracy compared to existing methods. The Stroop task data analysis reveals greater connectivity differences between task and fixation that are concentrated in brain regions previously identified as differentially activated in Stroop task, and more nuanced connectivity differences between exertion and relaxed task. In contrast, penalized modeling approaches involving computationally burdensome permutation tests reveal negligible network differences between conditions that seem biologically implausible.","23":"Although increasingly used as a data resource for assembling cohorts, electronic health records (EHRs) pose many analytic challenges. In particular, a patient's health status influences when and what data are recorded, generating sampling bias in the collected data. In this paper, we consider recurrent event analysis using EHR data. Conventional regression methods for event risk analysis usually require the values of covariates to be observed throughout the follow-up period. In EHR databases, time-dependent covariates are intermittently measured during clinical visits, and the timing of these visits is informative in the sense that it depends on the disease course. Simple methods, such as the last-observation-carried-forward approach, can lead to biased estimation. On the other hand, complex joint models require additional assumptions on the covariate process and cannot be easily extended to handle multiple longitudinal predictors. By incorporating sampling weights derived from estimating the observation time process, we develop a novel estimation procedure based on inverse-rate-weighting and kernel-smoothing for the semiparametric proportional rate model of recurrent events. The proposed methods do not require model specifications for the covariate processes and can easily handle multiple time-dependent covariates. Our methods are applied to a kidney transplant study for illustration.","24":"Integrative network modeling of data arising from multiple genomic platforms provides insight into the holistic picture of the interactive system, as well as the flow of information across many disease domains including cancer. The basic data structure consists of a sequence of hierarchically ordered datasets for each individual subject, which facilitates integration of diverse inputs, such as genomic, transcriptomic, and proteomic data. A primary analytical task in such contexts is to model the layered architecture of networks where the vertices can be naturally partitioned into ordered layers, dictated by multiple platforms, and exhibit both undirected and directed relationships. We propose a multi-layered Gaussian graphical model (mlGGM) to investigate conditional independence structures in such multi-level genomic networks in human cancers. We implement a Bayesian node-wise selection (BANS) approach based on variable selection techniques that coherently accounts for the multiple types of dependencies in mlGGM; this flexible strategy exploits edge-specific prior knowledge and selects sparse and interpretable models. Through simulated data generated under various scenarios, we demonstrate that BANS outperforms other existing multivariate regression-based methodologies. Our integrative genomic network analysis for key signaling pathways across multiple cancer types highlights commonalities and differences of p53 integrative networks and epigenetic effects of BRCA2 on p53 and its interaction with T68 phosphorylated CHK2, that may have translational utilities of finding biomarkers and therapeutic targets.","25":"Personalized policy represents a paradigm shift from one-decision-rule-for-all users to an individualized decision rule for each user. Developing personalized policy in mobile health applications imposes challenges. First, for lack of adherence, data from each user are limited. Second, unmeasured contextual factors can potentially impact on decision making. Aiming to optimize immediate rewards, we propose using a generalized linear mixed modeling framework where population features and individual features are modeled as fixed and random effects, respectively, and synthesized to form the personalized policy. The group lasso type penalty is imposed to avoid overfitting of individual deviations from the population model. We examine the conditions under which the proposed method work in the presence of time-varying endogenous covariates, and provide conditional optimality and marginal consistency results of the expected immediate outcome under the estimated policies. We apply our method to develop personalized push (\"prompt\") schedules in 294 app users, with the goal to maximize the prompt response rate given past app usage and other contextual factors. The proposed method compares favorably to existing estimation methods including using the R function \"glmer\" in a simulation study.","26":"We thank the opportunity offered by editors for this discussion and the discussants for their insightful comments and thoughtful contributions. We also want to congratulate Kallus (2020) for his inspiring work in improving the effciency of policy learning by retargeting. Motivated from the discussion in Dukes and Vansteelandt (2020), we first point out interesting connections and distinctions between our work and Kallus (2020) in Section 1. In particular, the assumptions and sources of variation for consideration in these two papers lead to different research problems with different scopes and focuses. In Section 2, following the discussions in Li et al. (2020); Liang and Zhao (2020), we also consider the efficient policy evaluation problem when we have some data from the testing distribution available at the training stage. We show that under the assumption that the sample sizes from training and testing are growing in the same order, efficient value function estimates can deliver competitive performance. We further show some connections of these estimates with existing literature. However, when the growth of testing sample size available for training is in a slower order, efficient value function estimates may not perform well anymore. In contrast, the requirement of the testing sample size for DRITR is not as strong as that of efficient policy evaluation using the combined data. Finally, we highlight the general applicability and usefulness of DRITR in Section 3.","27":"Recent development in the data-driven decision science has seen great advances in individualized decision making. Given data with individual covariates, treatment assignments and outcomes, policy makers best individualized treatment rule (ITR) that maximizes the expected outcome, known as the value function. Many existing methods assume that the training and testing distributions are the same. However, the estimated optimal ITR may have poor generalizability when the training and testing distributions are not identical. In this paper, we consider the problem of finding an optimal ITR from a restricted ITR class where there is some unknown covariate changes between the training and testing distributions. We propose a novel distributionally robust ITR (DR-ITR) framework that maximizes the worst-case value function across the values under a set of underlying distributions that are \"close\" to the training distribution. The resulting DR-ITR can guarantee the performance among all such distributions reasonably well. We further propose a calibrating procedure that tunes the DR-ITR adaptively to a small amount of calibration data from a target population. In this way, the calibrated DR-ITR can be shown to enjoy better generalizability than the standard ITR based on our numerical studies.","28":"This paper is motivated by a regression analysis of electroencephalography (EEG) neuroimaging data with high-dimensional correlated responses with multi-level nested correlations. We develop a divide-and-conquer procedure implemented in a fully distributed and parallelized computational scheme for statistical estimation and inference of regression parameters. Despite significant efforts in the literature, the computational bottleneck associated with high-dimensional likelihoods prevents the scalability of existing methods. The proposed method addresses this challenge by dividing responses into subvectors to be analyzed separately and in parallel on a distributed platform using pairwise composite likelihood. Theoretical challenges related to combining results from dependent data are overcome in a statistically efficient way using a meta-estimator derived from Hansen's generalized method of moments. We provide a rigorous theoretical framework for efficient estimation, inference, and goodness-of-fit tests. We develop an R package for ease of implementation. We illustrate our method's performance with simulations and the analysis of the EEG data, and find that iron deficiency is significantly associated with two auditory recognition memory related potentials in the left parietal-occipital region of the brain.","29":"Q-learning is a regression-based approach that is widely used to formalize the development of an optimal dynamic treatment strategy. Finite dimensional working models are typically used to estimate certain nuisance parameters, and misspecification of these working models can result in residual confounding and\/or efficiency loss. We propose a robust Q-learning approach which allows estimating such nuisance parameters using data-adaptive techniques. We study the asymptotic behavior of our estimators and provide simulation studies that highlight the need for and usefulness of the proposed method in practice. We use the data from the \"Extending Treatment Effectiveness of Naltrexone\" multi-stage randomized trial to illustrate our proposed methods.","30":"This JASA rejoinder concerns the problem of individualized decision making under point, sign, and partial identification. The paper unifies various classical decision making strategies through a lower bound perspective proposed in Cui and Tchetgen Tchetgen (2020b) in the context of optimal treatment regimes under uncertainty due to unmeasured confounding. Building on this unified framework, the paper also provides a novel minimax solution (i.e., a rule that minimizes the maximum regret for so-called opportunists) for individualized decision making\/policy assignment.","31":"Individualized treatment rules (ITRs) recommend treatment according to patient characteristics. There is a growing interest in developing novel and efficient statistical methods in constructing ITRs. We propose an improved doubly robust estimator of the optimal ITRs. The proposed estimator is based on a direct optimization of an augmented inverse-probability weighted estimator (AIPWE) of the expected clinical outcome over a class of ITRs. The method enjoys two key properties. First, it is doubly robust, meaning that the proposed estimator is consistent when either the propensity score or the outcome model is correct. Second, it achieves the smallest variance among the class of doubly robust estimators when the propensity score model is correctly specified, regardless of the specification of the outcome model. Simulation studies show that the estimated ITRs obtained from our method yield better results than those obtained from current popular methods. Data from the Sequenced Treatment Alternatives to Relieve Depression (STAR*D) study is analyzed as an illustrative example.","32":"There is a fast-growing literature on estimating optimal treatment regimes based on randomized trials or observational studies under a key identifying condition of no unmeasured confounding. Because confounding by unmeasured factors cannot generally be ruled out with certainty in observational studies or randomized trials subject to noncompliance, we propose a general instrumental variable approach to learning optimal treatment regimes under endogeneity. Specifically, we establish identification of both value function E [ Y D ( L )   ]   for a given regime D and optimal regimes arg  max D  E [ Y D ( L )   ]   with the aid of a binary instrumental variable, when no unmeasured confounding fails to hold. We also construct novel multiply robust classification-based estimators. Furthermore, we propose to identify and estimate optimal treatment regimes among those who would comply to the assigned treatment under a monotonicity assumption. In this latter case, we establish the somewhat surprising result that complier optimal regimes can be consistently estimated without directly collecting compliance information and therefore without the complier average treatment effect itself being identified. Our approach is illustrated via extensive simulation studies and a data application on the effect of child rearing on labor participation.","33":"Due to the recent advancements in wearables and sensing technology, health scientists are increasingly developing mobile health (mHealth) interventions. In mHealth interventions, mobile devices are used to deliver treatment to individuals as they go about their daily lives. These treatments are generally designed to impact a near time, proximal outcome such as stress or physical activity. The mHealth intervention policies, often called just-in-time adaptive interventions, are decision rules that map a individual's current state (e.g., individual's past behaviors as well as current observations of time, location, social activity, stress and urges to smoke) to a particular treatment at each of many time points. The vast majority of current mHealth interventions deploy expert-derived policies. In this paper, we provide an approach for conducting inference about the performance of one or more such policies using historical data collected under a possibly different policy. Our measure of performance is the average of proximal outcomes over a long time period should the particular mHealth policy be followed. We provide an estimator as well as confidence intervals. This work is motivated by HeartSteps, an mHealth physical activity intervention.","34":"While sample sizes in randomized clinical trials are large enough to estimate the average treatment effect well, they are often insufficient for estimation of treatment-covariate interactions critical to studying data-driven precision medicine. Observational data from real world practice may play an important role in alleviating this problem. One common approach in trials is to predict the outcome of interest with separate regression models in each treatment arm, and estimate the treatment effect based on the contrast of the predictions. Unfortunately, this simple approach may induce spurious treatment-covariate interaction in observational studies when the regression model is misspecified. Motivated by the need of modeling the number of relapses in multiple sclerosis patients, where the ratio of relapse rates is a natural choice of the treatment effect, we propose to estimate the conditional average treatment effect (CATE) as the ratio of expected potential outcomes, and derive a doubly robust estimator of this CATE in a semiparametric model of treatment-covariate interactions. We also provide a validation procedure to check the quality of the estimator on an independent sample. We conduct simulations to demonstrate the finite sample performance of the proposed methods, and illustrate their advantages on real data by examining the treatment effect of dimethyl fumarate compared to teriflunomide in multiple sclerosis patients.","35":null,"36":"There is an extensive literature on the estimation and evaluation of optimal individualized treatment rules in settings where all confounders of the effect of treatment on outcome are observed. We study the development of individualized decision rules in settings where some of these confounders may not have been measured but a valid binary instrument is available for a binary treatment. We first consider individualized treatment rules, which will naturally be most interesting in settings where it is feasible to intervene directly on treatment. We then consider a setting where intervening on treatment is infeasible, but intervening to encourage treatment is feasible. In both of these settings, we also handle the case that the treatment is a limited resource so that optimal interventions focus the available resources on those individuals who will benefit most from treatment. Given a reference rule, we evaluate an optimal individualized rule by its average causal effect relative to a prespecified reference rule. We develop methods to estimate optimal individualized rules and construct asymptotically efficient plug-in estimators of the corresponding average causal effect relative to a prespecified reference rule.","37":"Risk for autism can be influenced by genetic mutations in hundreds of genes. Based on findings showing that genes with highly correlated gene expressions are functionally interrelated, \"guilt by association\" methods such as DAWN have been developed to identify these autism risk genes. Previous research analyze the BrainSpan dataset, which contains gene expression of brain tissues from varying regions and developmental periods. Since the spatiotemporal properties of brain tissue is known to affect the gene expression's covariance, previous research have focused only on a specific subset of samples to avoid the issue of heterogeneity. This analysis leads to a potential loss of power when detecting risk genes. In this article, we develop a new method called COBS (COvariance-Based sample Selection) to find a larger and more homogeneous subset of samples that share the same population covariance matrix for the downstream DAWN analysis. To demonstrate COBS's effectiveness, we use genetic risk scores from two sequential data freezes obtained in 2014 and 2020. We show COBS improves DAWN's ability to predict risk genes detected in the newer data freeze when using the risk scores of the older data freeze as input.","38":"Medical imaging has become an increasingly important tool in screening, diagnosis, prognosis, and treatment of various diseases given its information visualization and quantitative assessment. The aim of this article is to develop a Bayesian scalar-on-image regression model to integrate high-dimensional imaging data and clinical data to predict cognitive, behavioral, or emotional outcomes, while allowing for nonignorable missing outcomes. Such a nonignorable nonresponse consideration is motivated by examining the association between baseline characteristics and cognitive abilities for 802 Alzheimer patients enrolled in the Alzheimer's Disease Neuroimaging Initiative 1 (ADNI1), for which data are partially missing. Ignoring such missing data may distort the accuracy of statistical inference and provoke misleading results. To address this issue, we propose an imaging exponential tilting model to delineate the data missing mechanism and incorporate an instrumental variable to facilitate model identifiability followed by a Bayesian framework with Markov chain Monte Carlo algorithms to conduct statistical inference. This approach is validated in simulation studies where both the finite sample performance and asymptotic properties are evaluated and compared with the model with fully observed data and that with a misspecified ignorable missing mechanism. Our proposed methods are finally carried out on the ADNI1 dataset, which turns out to capture both of those clinical risk factors and imaging regions consistent with the existing literature that exhibits clinical significance. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","39":"Percutaneous coronary interventions (PCIs) are nonsurgical procedures to open blocked blood vessels to the heart, frequently using a catheter to place a stent. The catheter can be inserted into the blood vessels using an artery in the groin or an artery in the wrist. Because clinical trials have indicated that access via the wrist may result in fewer post procedure complications, shortening the length of stay, and ultimately cost less than groin access, adoption of access via the wrist has been encouraged. However, patients treated in usual care are likely to differ from those participating in clinical trials, and there is reason to believe that the effectiveness of wrist access may differ between males and females. Moreover, the choice of artery access strategy is likely to be influenced by patient or physician unmeasured factors. To study the effectiveness of the two artery access site strategies on hospitalization charges, we use data from a state-mandated clinical registry including 7,963 patients undergoing PCI. A hierarchical Bayesian likelihood-based instrumental variable analysis under a latent index modeling framework is introduced to jointly model outcomes and treatment status. Our approach accounts for unobserved heterogeneity via a latent factor structure, and permits nonparametric error distributions with Dirichlet process mixture models. Our results demonstrate that artery access in the wrist reduces hospitalization charges compared to access in the groin, with a higher mean reduction for male patients.","40":"Feature screening plays an important role in the analysis of ultrahigh dimensional data. Due to complicated model structure and high noise level, existing screening methods often suffer from model misspecification and the presence of outliers. To address these issues, we introduce a new metric named cumulative divergence (CD), and develop a CD-based forward screening procedure. This forward screening method is model-free and resistant to the presence of outliers in the response. It also incorporates the joint effects among covariates into the screening process. With a data-driven threshold, the new method can automatically determine the number of features that should be retained after screening. These merits make the CD-based screening very appealing in practice. Under certain regularity conditions, we show that the proposed method possesses sure screening property. The performance of our proposal is illustrated through simulations and a real data example.","41":"We develop a causal inference approach to estimate the number of adverse health events that were prevented due to changes in exposure to multiple pollutants attributable to a large-scale air quality intervention\/regulation, with a focus on the 1990 Clean Air Act Amendments (CAAA). We introduce a causal estimand called the Total Events Avoided (TEA) by the regulation, defined as the difference in the number of health events expected under the no-regulation pollution exposures and the number observed with-regulation. We propose matching and machine learning methods that leverage population-level pollution and health data to estimate the TEA. Our approach improves upon traditional methods for regulation health impact analyses by formalizing causal identifying assumptions, utilizing population-level data, minimizing parametric assumptions, and collectively analyzing multiple pollutants. To reduce model-dependence, our approach estimates cumulative health impacts in the subset of regions with projected no-regulation features lying within the support of the observed with-regulation data, thereby providing a conservative but data-driven assessment to complement traditional parametric approaches. We analyze the health impacts of the CAAA in the US Medicare population in the year 2000, and our estimates suggest that large numbers of cardiovascular and dementia-related hospitalizations were avoided due to CAAA-attributable changes in pollution exposure.","42":"The aim of this paper is to develop a low-rank linear regression model (L2RM) to correlate a high-dimensional response matrix with a high dimensional vector of covariates when coefficient matrices have low-rank structures. We propose a fast and efficient screening procedure based on the spectral norm of each coefficient matrix in order to deal with the case when the number of covariates is extremely large. We develop an efficient estimation procedure based on the trace norm regularization, which explicitly imposes the low rank structure of coefficient matrices. When both the dimension of response matrix and that of covariate vector diverge at the exponential order of the sample size, we investigate the sure independence screening property under some mild conditions. We also systematically investigate some theoretical properties of our estimation procedure including estimation consistency, rank consistency and non-asymptotic error bound under some mild conditions. We further establish a theoretical guarantee for the overall solution of our two-step screening and estimation procedure. We examine the finite-sample performance of our screening and estimation methods using simulations and a large-scale imaging genetic dataset collected by the Philadelphia Neurodevelopmental Cohort (PNC) study.","43":"Cancers arise owing to somatic mutations, and the characteristic combinations of somatic mutations form mutational signatures. Despite many mutational signatures being identified, mutational processes underlying a number of mutational signatures remain unknown, which hinders the identification of interventions that may reduce somatic mutation burdens and prevent the development of cancer. We demonstrate that the unknown cause of a mutational signature can be inferred by the associated signatures with known etiology. However, existing association tests are not statistically powerful due to excess zeros in mutational signatures data. To address this limitation, we propose a semiparametric kernel independence test (SKIT). The SKIT statistic is defined as the integrated squared distance between mixed probability distributions and is decomposed into four disjoint components to pinpoint the source of dependency. We derive the asymptotic null distribution and prove the asymptotic convergence of power. Due to slow convergence to the asymptotic null distribution, a bootstrap method is employed to compute p-values. Simulation studies demonstrate that when zeros are prevalent, SKIT is more resilient to power loss than existing tests and robust to random errors. We applied SKIT to The Cancer Genome Atlas (TCGA) mutational signatures data for over 9,000 tumors across 32 cancer types, and identified a novel association between signature 17 curated in the Catalogue Of Somatic Mutations In Cancer (COSMIC) and apolipoprotein B mRNA editing enzyme (APOBEC) signatures in gastrointestinal cancers. It indicates that APOBEC activity is likely associated with the unknown cause of signature 17.","44":null,"45":"Noncompliance with assigned treatments is a common challenge in analyzing and interpreting randomized clinical trials (RCTs). One way to handle noncompliance is to estimate the complier-average causal effect (CACE), the intervention's efficacy in the subpopulation that complies with assigned treatment. In a two-step meta-analysis, one could first estimate CACE for each study, then combine them to estimate the population-averaged CACE. However, when some trials do not report noncompliance data, the two-step meta-analysis can be less efficient and potentially biased by excluding these trials. This paper proposes a flexible Bayesian hierarchical CACE framework to simultaneously account for heterogeneous and incomplete noncompliance data in a meta-analysis of RCTs. The models are motivated by and used for a meta-analysis estimating the CACE of epidural analgesia on cesarean section, in which only 10 of 27 trials reported complete noncompliance data. The new analysis includes all 27 studies and the results present new insights on the causal effect after accounting for noncompliance. Compared to the estimated risk difference of 0.8% (95% CI: -0.3%, 1.9%) given by the two-step intention-to-treat meta-analysis, the estimated CACE is 4.1% (95% CrI: -0.3%, 10.5%). We also report simulation studies to evaluate the performance of the proposed method.","46":"The target of inference in microbiome analyses is usually relative abundance (RA) because RA in a sample (e.g., stool) can be considered as an approximation of RA in an entire ecosystem (e.g., gut). However, inference on RA suffers from the fact that RA are calculated by dividing absolute abundances (AAs) over the common denominator (CD), the summation of all AA (i.e., library size). Because of that, perturbation in one taxon will result in a change in the CD and thus cause false changes in RA of all other taxa, and those false changes could lead to false positive\/negative findings. We propose a novel analysis approach (IFAA) to make robust inference on AA of an ecosystem that can circumvent the issues induced by the CD problem and compositional structure of RA. IFAA can also address the issues of overdispersion and handle zero-inflated data structures. IFAA identifies microbial taxa associated with the covariates in Phase 1 and estimates the association parameters by employing an independent reference taxon in Phase 2. Two real data applications are presented and extensive simulations show that IFAA outperforms other established existing approaches by a big margin in the presence of unbalanced library size. Supplementary materials for this article are available online.","47":"In contrast to the classical \"one size fits all\" approach, precision medicine proposes the customization of individualized treatment regimes to account for patients' heterogeneity in response to treatments. Most of existing works in the literature focused on estimating optimal individualized treatment regimes. However, there has been less attention devoted to hypothesis testing regarding the existence of overall qualitative treatment effects, especially when there is a large number of prognostic covariates. When covariates don't have qualitative treatment effects, the optimal treatment regime will assign the same treatment to all patients regardless of their covariate values. In this paper, we consider testing the overall qualitative treatment effects of patients' prognostic covariates in a high dimensional setting. We propose a sample splitting method to construct the test statistic, based on a nonparametric estimator of the contrast function. When the dimension of covariates is large, we construct the test based on sparse random projections of covariates into a low-dimensional space. We prove the consistency of our test statistic. In the regular cases, we show the asymptotic power function of our test statistic is asymptotically the same as the \"oracle\" test statistic which is constructed based on the \"optimal\" projection matrix. Simulation studies and real data applications validate our theoretical findings.","48":"A typical approach to the joint analysis of two high-dimensional datasets is to decompose each data matrix into three parts: a low-rank common matrix that captures the shared information across datasets, a low-rank distinctive matrix that characterizes the individual information within a single dataset, and an additive noise matrix. Existing decomposition methods often focus on the orthogonality between the common and distinctive matrices, but inadequately consider the more necessary orthogonal relationship between the two distinctive matrices. The latter guarantees that no more shared information is extractable from the distinctive matrices. We propose decomposition-based canonical correlation analysis (D-CCA), a novel decomposition method that defines the common and distinctive matrices from the  L 2   space of random variables rather than the conventionally used Euclidean space, with a careful construction of the orthogonal relationship between distinctive matrices. D-CCA represents a natural generalization of the traditional canonical correlation analysis. The proposed estimators of common and distinctive matrices are shown to be consistent and have reasonably better performance than some state-of-the-art methods in both simulated data and the real data analysis of breast cancer data obtained from The Cancer Genome Atlas.","49":"Technological advances in science and engineering have led to the routine collection of large and complex data objects, where the dependence structure among those objects is often of great interest. Those complex objects (e.g, different brain subcortical structures) often reside in some Banach spaces, and hence their relationship cannot be well characterized by most of the existing measures of dependence such as correlation coefficients developed in Hilbert spaces. To overcome the limitations of the existing measures, we propose Ball Covariance as a generic measure of dependence between two random objects in two possibly different Banach spaces. Our Ball Covariance possesses the following attractive properties: (i) It is nonparametric and model-free, which make the proposed measure robust to model mis-specification; (ii) It is nonnegative and equal to zero if and only if two random objects in two separable Banach spaces are independent; (iii) Empirical Ball Covariance is easy to compute and can be used as a test statistic of independence. We present both theoretical and numerical results to reveal the potential power of the Ball Covariance in detecting dependence. Also importantly, we analyze two real datasets to demonstrate the usefulness of Ball Covariance in the complex dependence detection.","50":"With the abundance of large data, sparse penalized regression techniques are commonly used in data analysis due to the advantage of simultaneous variable selection and estimation. A number of convex as well as non-convex penalties have been proposed in the literature to achieve sparse estimates. Despite intense work in this area, how to perform valid inference for sparse penalized regression with a general penalty remains to be an active research problem. In this paper, by making use of state-of-the-art optimization tools in stochastic variational inequality theory, we propose a unified framework to construct confidence intervals for sparse penalized regression with a wide range of penalties, including convex and non-convex penalties. We study the inference for parameters under the population version of the penalized regression as well as parameters of the underlying linear model. Theoretical convergence properties of the proposed method are obtained. Several simulated and real data examples are presented to demonstrate the validity and effectiveness of the proposed inference procedure.","51":null,"52":"The era of big data has witnessed an increasing availability of multiple data sources for statistical analyses. We consider estimation of causal effects combining big main data with unmeasured confounders and smaller validation data with supplementary information on these confounders. Under the unconfoundedness assumption with completely observed confounders, the smaller validation data allow for constructing consistent estimators for causal effects, but the big main data can only give error-prone estimators in general. However, by leveraging the information in the big main data in a principled way, we can improve the estimation efficiencies yet preserve the consistencies of the initial estimators based solely on the validation data. Our framework applies to asymptotically normal estimators, including the commonly used regression imputation, weighting, and matching estimators, and does not require a correct specification of the model relating the unmeasured confounders to the observed variables. We also propose appropriate bootstrap procedures, which makes our method straightforward to implement using software routines for existing estimators. Supplementary materials for this article are available online.","53":"Large brain imaging databases contain a wealth of information on brain organization in the populations they target, and on individual variability. While such databases have been used to study group-level features of populations directly, they are currently underutilized as a resource to inform single-subject analysis. Here, we propose leveraging the information contained in large functional magnetic resonance imaging (fMRI) databases by establishing population priors to employ in an empirical Bayesian framework. We focus on estimation of brain networks as source signals in independent component analysis (ICA). We formulate a hierarchical \"template\" ICA model where source signals-including known population brain networks and subject-specific signals-are represented as latent variables. For estimation, we derive an expectation maximization (EM) algorithm having an explicit solution. However, as this solution is computationally intractable, we also consider an approximate subspace algorithm and a faster two-stage approach. Through extensive simulation studies, we assess performance of both methods and compare with dual regression, a popular but ad-hoc method. The two proposed algorithms have similar performance, and both dramatically outperform dual regression. We also conduct a reliability study utilizing the Human Connectome Project and find that template ICA achieves substantially better performance than dual regression, achieving 75-250% higher intra-subject reliability.","54":"Cortical surface fMRI (cs-fMRI) has recently grown in popularity versus traditional volumetric fMRI. In addition to offering better whole-brain visualization, dimension reduction, removal of extraneous tissue types, and improved alignment of cortical areas across subjects, it is also more compatible with common assumptions of Bayesian spatial models. However, as no spatial Bayesian model has been proposed for cs-fMRI data, most analyses continue to employ the classical general linear model (GLM), a \"massive univariate\" approach. Here, we propose a spatial Bayesian GLM for cs-fMRI, which employs a class of sophisticated spatial processes to model latent activation fields. We make several advances compared with existing spatial Bayesian models for volumetric fMRI. First, we use integrated nested Laplacian approximations (INLA), a highly accurate and efficient Bayesian computation technique, rather than variational Bayes (VB). To identify regions of activation, we utilize an excursions set method based on the joint posterior distribution of the latent fields, rather than the marginal distribution at each location. Finally, we propose the first multi-subject spatial Bayesian modeling approach, which addresses a major gap in the existing literature. The methods are very computationally advantageous and are validated through simulation studies and two task fMRI studies from the Human Connectome Project.","55":"Studying the effects of groups of single nucleotide polymorphisms (SNPs), as in a gene, genetic pathway, or network, can provide novel insight into complex diseases like breast cancer, uncovering new genetic associations and augmenting the information that can be gleaned from studying SNPs individually. Common challenges in set-based genetic association testing include weak effect sizes, correlation between SNPs in a SNP-set, and scarcity of signals, with individual SNP effects often ranging from extremely sparse to moderately sparse in number. Motivated by these challenges, we propose the Generalized Berk-Jones (GBJ) test for the association between a SNP-set and outcome. The GBJ extends the Berk-Jones statistic by accounting for correlation among SNPs, and it provides advantages over the Generalized Higher Criticism test when signals in a SNP-set are moderately sparse. We also provide an analytic p-value calculation for SNP-sets of any finite size, and we develop an omnibus statistic that is robust to the degree of signal sparsity. An additional advantage of our work is the ability to conduct inference using individual SNP summary statistics from a genome-wide association study (GWAS). We evaluate the finite sample performance of the GBJ through simulation and apply the method to identify breast cancer risk genes in a GWAS conducted by the Cancer Genetic Markers of Susceptibility Consortium. Our results suggest evidence of association between FGFR2 and breast cancer and also identify other potential susceptibility genes, complementing conventional SNP-level analysis.","56":null,"57":"Current guidelines for treatment decision making largely rely on data from randomized controlled trials (RCTs) studying average treatment effects. They may be inadequate to make individualized treatment decisions in real-world settings. Large-scale electronic health records (EHR) provide opportunities to fulfill the goals of personalized medicine and learn individualized treatment rules (ITRs) depending on patient-specific characteristics from real-world patient data. In this work, we tackle challenges with EHRs and propose a machine learning approach based on matching (M-learning) to estimate optimal ITRs from EHRs. This new learning method performs matching instead of inverse probability weighting as commonly used in many existing methods for estimating ITRs to more accurately assess individuals' treatment responses to alternative treatments and alleviate confounding. Matching-based value functions are proposed to compare matched pairs under a unified framework, where various types of outcomes for measuring treatment response (including continuous, ordinal, and discrete outcomes) can easily be accommodated. We establish the Fisher consistency and convergence rate of M-learning. Through extensive simulation studies, we show that M-learning outperforms existing methods when propensity scores are misspecified or when unmeasured confounders are present in certain scenarios. Lastly, we apply M-learning to estimate optimal personalized second-line treatments for type 2 diabetes patients to achieve better glycemic control or reduce major complications using EHRs from New York Presbyterian Hospital.","58":"The sample frequency spectrum (SFS), or histogram of allele counts, is an important summary statistic in evolutionary biology, and is often used to infer the history of population size changes, migrations, and other demographic events affecting a set of populations. The expected multipopulation SFS under a given demographic model can be efficiently computed when the populations in the model are related by a tree, scaling to hundreds of populations. Admixture, back-migration, and introgression are common natural processes that violate the assumption of a tree-like population history, however, and until now the expected SFS could be computed for only a handful of populations when the demographic history is not a tree. In this article, we present a new method for efficiently computing the expected SFS and linear functionals of it, for demographies described by general directed acyclic graphs. This method can scale to more populations than p reviously possible for complex demographic histories including admixture. We apply our method to an 8-population SFS to estimate the timing and strength of a proposed \"basal Eurasian\" admixture event in human history. We implement and release our method in a new open-source software package momi2.","59":"In the genomic era, the identification of gene signatures associated with disease is of significant interest. Such signatures are often used to predict clinical outcomes in new patients and aid clinical decision-making. However, recent studies have shown that gene signatures are often not replicable. This occurrence has practical implications regarding the generalizability and clinical applicability of such signatures. To improve replicability, we introduce a novel approach to select gene signatures from multiple datasets whose effects are consistently non-zero and account for between-study heterogeneity. We build our model upon some rank-based quantities, facilitating integration over different genomic datasets. A high dimensional penalized Generalized Linear Mixed Model (pGLMM) is used to select gene signatures and address data heterogeneity. We compare our method to some commonly used strategies that select gene signatures ignoring between-study heterogeneity. We provide asymptotic results justifying the performance of our method and demonstrate its advantage in the presence of heterogeneity through thorough simulation studies. Lastly, we motivate our method through a case study subtyping pancreatic cancer patients from four gene expression studies.","60":"Tooth loss from periodontal disease is a major public health burden in the United States. Standard clinical practice is to recommend a dental visit every six months; however, this practice is not evidence-based, and poor dental outcomes and increasing dental insurance premiums indicate room for improvement. We consider a tailored approach that recommends recall time based on patient characteristics and medical history to minimize disease progression without increasing resource expenditures. We formalize this method as a dynamic treatment regime which comprises a sequence of decisions, one per stage of intervention, that follow a decision rule which maps current patient information to a recommendation for their next visit time. The dynamics of periodontal health, visit frequency, and patient compliance are complex, yet the estimated optimal regime must be interpretable to domain experts if it is to be integrated into clinical practice. We combine non-parametric Bayesian dynamics modeling with policy-search algorithms to estimate the optimal dynamic treatment regime within an interpretable class of regimes. Both simulation experiments and application to a rich database of electronic dental records from the HealthPartners HMO shows that our proposed method leads to better dental health without increasing the average recommended recall time relative to competing methods.","61":"Immunotherapies have attracted lots of research interests recently. The need to understand the underlying mechanisms of immunotherapies and to develop precision immunotherapy regimens has spurred great interest in characterizing immune cell composition within the tumor microenvironment. Several methods have been developed to estimate immune cell composition using gene expression data from bulk tumor samples. However, these methods are not flexible enough to handle aberrant patterns of gene expression data, e.g., inconsistent cell type-specific gene expression between purified reference samples and tumor samples. We propose a novel statistical method for expression deconvolution called ICeD-T (Immune Cell Deconvolution in Tumor tissues). ICeD-T automatically identifies aberrant genes whose expression are inconsistent with the deconvolution model and down-weights their contributions to cell type abundance estimates. We evaluated the performance of ICeD-T versus existing methods in simulation studies and several real data analyses. ICeD-T displayed comparable or superior performance to these competing methods. Applying these methods to assess the relationship between immunotherapy response and immune cell composition, ICeD-T is able to identify significant associations that are missed by its competitors.","62":"Combining individual p-values to aggregate multiple small effects has a long-standing interest in statistics, dating back to the classic Fisher's combination test. In modern large-scale data analysis, correlation and sparsity are common features and efficient computation is a necessary requirement for dealing with massive data. To overcome these challenges, we propose a new test that takes advantage of the Cauchy distribution. Our test statistic has a simple form and is defined as a weighted sum of Cauchy transformation of individual p-values. We prove a non-asymptotic result that the tail of the null distribution of our proposed test statistic can be well approximated by a Cauchy distribution under arbitrary dependency structures. Based on this theoretical result, the p-value calculation of our proposed test is not only accurate, but also as simple as the classic z-test or t-test, making our test well suited for analyzing massive data. We further show that the power of the proposed test is asymptotically optimal in a strong sparsity setting. Extensive simulations demonstrate that the proposed test has both strong power against sparse alternatives and a good accuracy with respect to p-value calculations, especially for very small p-values. The proposed test has also been applied to a genome-wide association study of Crohn's disease and compared with several existing tests.","63":"Bronchiolitis (inflammation of the lower respiratory tract) in infants is primarily due to viral infection and is the single most common cause of infant hospitalization in the United States. To increase epidemiological understanding of bronchiolitis (and, subsequently, develop better prevention strategies), this research analyzes data on infant bronchiolitis cases from the U.S. Military Health System between the years 2003-2013 in Norfolk, Virginia, USA. For privacy reasons, child home addresses, birth dates, and diagnosis dates were randomized (jittered) creating spatio-temporal uncertainty in the geographic location and timing of bronchiolitis incidents. Using spatio-temporal point patterns, we created a modeling strategy that accounts for the jittering to estimate and quantify the uncertainty for the incidence proportion (IP) of bronchiolitis. Additionally, we regress the IP onto key covariates including pollution where we adequately account for uncertainty in the pollution levels (i.e., covariate uncertainty) using a land use regression model. Our analysis results indicate that the IP is positively associated with sulfur dioxide and population density. Further, we demonstrate how scientific conclusions may change if various sources of uncertainty (either spatio-temporal or covariate uncertainty) are not accounted for. Code submitted with this article was checked by an Associate Editor for Reproducibility and is available as an online supplement.","64":"Random-effects meta-analyses of observational studies can produce biased estimates if the synthesized studies are subject to unmeasured confounding. We propose sensitivity analyses quantifying the extent to which unmeasured confounding of specified magnitude could reduce to below a certain threshold the proportion of true effect sizes that are scientifically meaningful. We also develop converse methods to estimate the strength of confounding capable of reducing the proportion of scientifically meaningful true effects to below a chosen threshold. These methods apply when a \"bias factor\" is assumed to be normally distributed across studies or is assessed across a range of fixed values. Our estimators are derived using recently proposed sharp bounds on confounding bias within a single study that do not make assumptions regarding the unmeasured confounders themselves or the functional form of their relationships with the exposure and outcome of interest. We provide an R package, EValue, and a free website that compute point estimates and inference and produce plots for conducting such sensitivity analyses. These methods facilitate principled use of random-effects meta-analyses of observational studies to assess the strength of causal evidence for a hypothesis.","65":"Radiomics involves the study of tumor images to identify quantitative markers explaining cancer heterogeneity. The predominant approach is to extract hundreds to thousands of image features, including histogram features comprised of summaries of the marginal distribution of pixel intensities, which leads to multiple testing problems and can miss out on insights not contained in the selected features. In this paper, we present methods to model the entire marginal distribution of pixel intensities via the quantile function as functional data, regressed on a set of demographic, clinical, and genetic predictors to investigate their effects of imaging-based cancer heterogeneity. We call this approach quantile functional regression, regressing subject-specific marginal distributions across repeated measurements on a set of covariates, allowing us to assess which covariates are associated with the distribution in a global sense, as well as to identify distributional features characterizing these differences, including mean, variance, skewness, heavy-tailedness, and various upper and lower quantiles. To account for smoothness in the quantile functions, account for intrafunctional correlation, and gain statistical power, we introduce custom basis functions we call quantlets that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given data set and containing a Gaussian subspace so non-Gaussianness can be assessed. We fit this model using a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and provides fully Bayesian inference after fitting a Markov chain Monte Carlo. We demonstrate the benefit of the basis space modeling through simulation studies, and apply the method to Magnetic resonance imaging (MRI) based radiomic dataset from Glioblastoma Multiforme to relate imaging-based quantile functions to various demographic, clinical, and genetic predictors, finding specific differences in tumor pixel intensity distribution between males and females and between tumors with and without DDIT3 mutations.","66":"Inference of directional pairwise relations between interacting units in a directed acyclic graph (DAG), such as a regulatory gene network, is common in practice, imposing challenges because of lack of inferential tools. For example, inferring a specific gene pathway of a regulatory gene network is biologically important. Yet, frequentist inference of directionality of connections remains largely unexplored for regulatory models. In this article, we propose constrained likelihood ratio tests for inference of the connectivity as well as directionality subject to nonconvex acyclicity constraints in a Gaussian directed graphical model. Particularly, we derive the asymptotic distributions of the constrained likelihood ratios in a high-dimensional situation. For testing of connectivity, the asymptotic distribution is either chi-squared or normal depending on if the number of testable links in a DAG model is small. For testing of directionality, the asymptotic distribution is the minimum of d independent chi-squared variables with one-degree of freedom or a generalized Gamma distribution depending on if d is small, where d is number of breakpoints in a hypothesized pathway. Moreover, we develop a computational method to perform the proposed tests, which integrates an alternating direction method of multipliers and difference convex programming. Finally, the power analysis and simulations suggest that the tests achieve the desired objectives of inference. An analysis of an Alzheimer's disease gene expression dataset illustrates the utility of the proposed method to infer a directed pathway in a gene network.","67":"Doubly truncated data are found in astronomy, econometrics and survival analysis literature. They arise when each observation is confined to an interval, i.e., only those which fall within their respective intervals are observed along with the intervals. Unlike the one-sided truncation that can be handled by counting process-based approach, doubly truncated data are much more difficult to handle. In their analysis of an astronomical data set, Efron and Petrosian (1999) proposed some nonparametric methods for doubly truncated data. Motivated by their approach, as well as by the work of Bhattacharya et al. (1983) for right truncated data, we propose a general method for estimating the regression parameter when the dependent variable is subject to the double truncation. It extends the Mann-Whitney-type rank estimator and can be computed easily by existing software packages. Weighted rank estimation are also considered for improving estimation efficiency. We show that the resulting estimators are consistent and asymptotically normal. Resampling schemes are proposed with large sample justification for approximating the limiting distributions. The quasar data in Efron and Petrosian (1999) and an AIDS incubation data are analyzed by the new method. Simulation results show that the proposed method works well.","68":"The vision for precision medicine is to use individual patient characteristics to inform a personalized treatment plan that leads to the best possible health-care for each patient. Mobile technologies have an important role to play in this vision as they offer a means to monitor a patient's health status in real-time and subsequently to deliver interventions if, when, and in the dose that they are needed. Dynamic treatment regimes formalize individualized treatment plans as sequences of decision rules, one per stage of clinical intervention, that map current patient information to a recommended treatment. However, most existing methods for estimating optimal dynamic treatment regimes are designed for a small number of fixed decision points occurring on a coarse time-scale. We propose a new reinforcement learning method for estimating an optimal treatment regime that is applicable to data collected using mobile technologies in an out-patient setting. The proposed method accommodates an indefinite time horizon and minute-by-minute decision making that are common in mobile health applications. We show that the proposed estimators are consistent and asymptotically normal under mild conditions. The proposed methods are applied to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes.","69":"The National Birth Defects Prevention Study (NBDPS) is a case-control study of birth defects conducted across 10 U.S. states. Researchers are interested in characterizing the etiologic role of maternal diet, collected using a food frequency questionnaire. Because diet is multi-dimensional, dimension reduction methods such as cluster analysis are often used to summarize dietary patterns. In a large, heterogeneous population, traditional clustering methods, such as latent class analysis, used to estimate dietary patterns can produce a large number of clusters due to a variety of factors, including study size and regional diversity. These factors result in a loss of interpretability of patterns that may differ due to minor consumption changes. Based on adaptation of the local partition process, we propose a new method, Robust Profile Clustering, to handle these data complexities. Here, participants may be clustered at two levels: (1) globally, where women are assigned to an overall population-level cluster via an overfitted finite mixture model, and (2) locally, where regional variations in diet are accommodated via a beta-Bernoulli process dependent on subpopulation differences. We use our method to analyze the NBDPS data, deriving pre-pregnancy dietary patterns for women in the NBDPS while accounting for regional variability.","70":"Inference in a high-dimensional situation may involve regularization of a certain form to treat overparameterization, imposing challenges to inference. The common practice of inference uses either a regularized model, as in inference after model selection, or bias-reduction known as \"debias.\" While the first ignores statistical uncertainty inherent in regularization, the second reduces the bias inbred in regularization at the expense of increased variance. In this article, we propose a constrained maximum likelihood method for hypothesis testing involving unspecific nuisance parameters, with a focus of alleviating the impact of regularization on inference. Particularly, for general composite hypotheses, we unregularize hypothesized parameters whereas regularizing nuisance parameters through a L 0-constraint controlling the degree of sparseness. This approach is analogous to semiparametric likelihood inference in a high-dimensional situation. On this ground, for the Gaussian graphical model and linear regression, we derive conditions under which the asymptotic distribution of the constrained likelihood ratio is established, permitting parameter dimension increasing with the sample size. Interestingly, the corresponding limiting distribution is the chi-square or normal, depending on if the co-dimension of a test is finite or increases with the sample size, leading to asymptotic similar tests. This goes beyond the classical Wilks phenomenon. Numerically, we demonstrate that the proposed method performs well against it competitors in various scenarios. Finally, we apply the proposed method to infer linkages in brain network analysis based on MRI data, to contrast Alzheimer's disease patients against healthy subjects. Supplementary materials for this article are available online.","71":"The study of gene expression quantitative trait loci (eQTL) is an effective approach to illuminate the functional roles of genetic variants. Computational methods have been developed for eQTL mapping using gene expression data from microarray or RNA-seq technology. Application of these methods for eQTL mapping in tumor tissues is problematic because tumor tissues are composed of both tumor and infiltrating normal cells (e.g. immune cells) and eQTL effects may vary between tumor and infiltrating normal cells. To address this challenge, we have developed a new method for eQTL mapping using RNA-seq data from tumor samples. Our method separately estimates the eQTL effects in tumor and infiltrating normal cells using both total expression and allele-specific expression (ASE). We demonstrate that our method controls type I error rate and has higher power than some alternative approaches. We applied our method to study RNA-seq data from The Cancer Genome Atlas and illustrated the similarities and differences of eQTL effects in tumor and normal cells.","72":null,"73":"The simultaneous estimation and variable selection for Cox model has been discussed by several authors (Fan and Li, 2002; Huang and Ma, 2010; Tibshirani, 1997) when one observes right-censored failure time data. However, there does not seem to exist an established procedure for interval-censored data, a more general and complex type of failure time data, except two parametric procedures given in Scolas et al. (2016) and Wu and Cook (2015). To address this, we propose a broken adaptive ridge (BAR) regression procedure that combines the strengths of the quadratic regularization and the adaptive weighted bridge shrinkage. In particular, the method allows for the number of covariates to be diverging with the sample size. Under some weak regularity conditions, unlike most of the existing variable selection methods, we establish both the oracle property and the grouping effect of the proposed BAR procedure. An extensive simulation study is conducted and indicates that the proposed approach works well in practical situations and deals with the collinearity problem better than the other oracle-like methods. An application is also provided.","74":"Suppose we have a Bayesian model that combines evidence from several different sources. We want to know which model parameters most affect the estimate or decision from the model, or which of the parameter uncertainties drive the decision uncertainty. Furthermore, we want to prioritize what further data should be collected. These questions can be addressed by Value of Information (VoI) analysis, in which we estimate expected reductions in loss from learning specific parameters or collecting data of a given design. We describe the theory and practice of VoI for Bayesian evidence synthesis, using and extending ideas from health economics, computer modeling and Bayesian design. The methods are general to a range of decision problems including point estimation and choices between discrete actions. We apply them to a model for estimating prevalence of HIV infection, combining indirect information from surveys, registers, and expert beliefs. This analysis shows which parameters contribute most of the uncertainty about each prevalence estimate, and the expected improvements in precision from specific amounts of additional data. These benefits can be traded with the costs of sampling to determine an optimal sample size. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","75":"The standard approach to Bayesian inference is based on the assumption that the distribution of the data belongs to the chosen model class. However, even a small violation of this assumption can have a large impact on the outcome of a Bayesian procedure. We introduce a novel approach to Bayesian inference that improves robustness to small departures from the model: rather than conditioning on the event that the observed data are generated by the model, one conditions on the event that the model generates data close to the observed data, in a distributional sense. When closeness is defined in terms of relative entropy, the resulting \"coarsened\" posterior can be approximated by simply tempering the likelihood-that is, by raising the likelihood to a fractional power-thus, inference can usually be implemented via standard algorithms, and one can even obtain analytical solutions when using conjugate priors. Some theoretical properties are derived, and we illustrate the approach with real and simulated data using mixture models and autoregressive models of unknown order.","76":"Many surveillance systems of infectious diseases are syndrome-based, capturing patients by clinical manifestation. Only a fraction of patients, mostly severe cases, undergo laboratory validation to identify the underlying pathogen. Motivated by the need to understand transmission dynamics and associate risk factors of enteroviruses causing the hand, foot and mouth disease (HFMD) in China, we developed a Bayesian spatio-temporal modeling framework for surveillance data of infectious diseases with small validation sets. A novel approach was proposed to sample unobserved pathogen-specific patient counts over space and time and was compared to an existing sampling approach. The practical utility of this framework in identifying key parameters was assessed in simulations for a range of realistic sizes of the validation set. Several designs of sampling patients for laboratory validation were compared with and without aggregation of sparse validation data. The methodology was applied to the 2009 HFMD epidemic in southern China to evaluate transmissibility and the effects of climatic conditions for the leading pathogens of the disease, enterovirus 71 and Coxsackie A16.","77":"This paper addresses the challenge of efficiently capturing a high proportion of true signals for subsequent data analyses when sample sizes are relatively limited with respect to data dimension. We propose the signal missing rate as a new measure for false negative control to account for the variability of false negative proportion. Novel data-adaptive procedures are developed to control signal missing rate without incurring many unnecessary false positives under dependence. We justify the efficiency and adaptivity of the proposed methods via theory and simulation. The proposed methods are applied to GWAS on human height to effectively remove irrelevant SNPs while retaining a high proportion of relevant SNPs for subsequent polygenic analysis.","78":"Dynamic treatment regimes are a set of decision rules and each treatment decision is tailored over time according to patients' responses to previous treatments as well as covariate history. There is a growing interest in development of correct statistical inference for optimal dynamic treatment regimes to handle the challenges of non-regularity problems in the presence of non-respondents who have zero-treatment effects, especially when the dimension of the tailoring variables is high. In this paper, we propose a high-dimensional Q-learning (HQ-learning) to facilitate the inference of optimal values and parameters. The proposed method allows us to simultaneously estimate the optimal dynamic treatment regimes and select the important variables that truly contribute to the individual reward. At the same time, hard thresholding is introduced in the method to eliminate the effects of the non-respondents. The asymptotic properties for the parameter estimators as well as the estimated optimal value function are then established by adjusting the bias due to thresholding. Both simulation studies and real data analysis demonstrate satisfactory performance for obtaining the proper inference for the value function for the optimal dynamic treatment regimes.","79":"Analysis of genomic data is often complicated by the presence of missing values, which may arise due to cost or other reasons. The prevailing approach of single imputation is generally invalid if the imputation model is misspecified. In this paper, we propose a robust score statistic based on imputed data for testing the association between a phenotype and a genomic variable with (partially) missing values. We fit a semiparametric regression model for the genomic variable against an arbitrary function of the linear predictor in the phenotype model and impute each missing value by its estimated posterior expectation. We show that the score statistic with such imputed values is asymptotically unbiased under general missing-data mechanisms, even when the imputation model is misspecified. We develop a spline-based method to estimate the semiparametric imputation model and derive the asymptotic distribution of the corresponding score statistic with a consistent variance estimator using sieve approximation theory and empirical process theory. The proposed test is computationally feasible regardless of the number of independent variables in the imputation model. We demonstrate the advantages of the proposed method over existing methods through extensive simulation studies and provide an application to a major cancer genomics study.","80":"In modern scientific research, data are often collected from multiple modalities. Since different modalities could provide complementary information, statistical prediction methods using multi-modality data could deliver better prediction performance than using single modality data. However, one special challenge for using multi-modality data is related to block-missing data. In practice, due to dropouts or the high cost of measures, the observations of a certain modality can be missing completely for some subjects. In this paper, we propose a new DIrect Sparse regression procedure using COvariance from Multi-modality data (DISCOM). Our proposed DISCOM method includes two steps to find the optimal linear prediction of a continuous response variable using block-missing multi-modality predictors. In the first step, rather than deleting or imputing missing data, we make use of all available information to estimate the covariance matrix of the predictors and the cross-covariance vector between the predictors and the response variable. The proposed new estimate of the covariance matrix is a linear combination of the identity matrix, the estimates of the intra-modality covariance matrix and the cross-modality covariance matrix. Flexible estimates for both the sub-Gaussian and heavy-tailed cases are considered. In the second step, based on the estimated covariance matrix and the estimated cross-covariance vector, an extended Lasso-type estimator is used to deliver a sparse estimate of the coefficients in the optimal linear prediction. The number of samples that are effectively used by DISCOM is the minimum number of samples with available observations from two modalities, which can be much larger than the number of samples with complete observations from all modalities. The effectiveness of the proposed method is demonstrated by theoretical studies, simulated examples, and a real application from the Alzheimer's Disease Neuroimaging Initiative. The comparison between DISCOM and some existing methods also indicates the advantages of our proposed method.","81":null,"82":"Time-varying networks are fast emerging in a wide range of scientific and business applications. Most existing dynamic network models are limited to a single-subject and discrete-time setting. In this article, we propose a mixed-effect network model that characterizes the continuous time-varying behavior of the network at the population level, meanwhile taking into account both the individual subject variability as well as the prior module information. We develop a multistep optimization procedure for a constrained likelihood estimation and derive the associated asymptotic properties. We demonstrate the effectiveness of our method through both simulations and an application to a study of brain development in youth. Supplementary materials for this article are available online.","83":"Estimating an optimal individualized treatment rule (ITR) based on patients' information is an important problem in precision medicine. An optimal ITR is a decision function that optimizes patients' expected clinical outcomes. Many existing methods in the literature are designed for binary treatment settings with the interest of a continuous outcome. Much less work has been done on estimating optimal ITRs in multiple treatment settings with good interpretations. In this article, we propose angle-based direct learning (AD-learning) to efficiently estimate optimal ITRs with multiple treatments. Our proposed method can be applied to various types of outcomes, such as continuous, survival, or binary outcomes. Moreover, it has an interesting geometric interpretation on the effect of different treatments for each individual patient, which can help doctors and patients make better decisions. Finite sample error bounds have been established to provide a theoretical guarantee for AD-learning. Finally, we demonstrate the superior performance of our method via an extensive simulation study and real data applications. Supplementary materials for this article are available online.","84":"Model averaging generally provides better predictions than model selection, but the existing model averaging methods cannot lead to parsimonious models. Parsimony is an especially important property when the number of parameters is large. To achieve a parsimonious model averaging coefficient estimator, we suggest a novel criterion for choosing weights. Asymptotic properties are derived in two practical scenarios: (i) one or more correct models exist in the candidate model set and (ii) all candidate models are misspecified. Under the former scenario, it is proved that our method can put the weight one to the smallest correct model and the resulting model averaging estimators of coefficients have many zeros and thus lead to a parsimonious model. The asymptotic distribution of the estimators is also provided. Under the latter scenario, prediction is mainly focused on and we prove that the proposed procedure is asymptotically optimal in the sense that its squared prediction loss and risk are asymptotically identical to those of the best-but infeasible-model averaging estimator. Numerical analysis shows the promise of the proposed procedure over existing model averaging and selection methods.","85":"Kidney obstruction, if untreated in a timely manner, can lead to irreversible loss of renal function. A widely used technology for evaluations of kidneys with suspected obstruction is diuresis renography. However, it is generally very challenging for radiologists who typically interpret renography data in practice to build high level of competency due to the low volume of renography studies and insufficient training. Another challenge is that there is currently no gold standard for detection of kidney obstruction. Seeking to develop a computer-aided diagnostic (CAD) tool that can assist practicing radiologists to reduce errors in the interpretation of kidney obstruction, a recent study collected data from diuresis renography, interpretations on the renography data from highly experienced nuclear medicine experts as well as clinical data. To achieve the objective, we develop a statistical model that can be used as a CAD tool for assisting radiologists in kidney interpretation. We use a Bayesian latent class modeling approach for predicting kidney obstruction through the integrative analysis of time-series renogram data, expert ratings, and clinical variables. A nonparametric Bayesian latent factor regression approach is adopted for modeling renogram curves in which the coefficients of the basis functions are parameterized via the factor loadings dependent on the latent disease status and the extended latent factors that can also adjust for clinical variables. A hierarchical probit model is used for expert ratings, allowing for training with rating data from multiple experts while predicting with at most one expert, which makes the proposed model operable in practice. An efficient MCMC algorithm is developed to train the model and predict kidney obstruction with associated uncertainty. We demonstrate the superiority of the proposed method over several existing methods through extensive simulations. Analysis of the renal study also lends support to the usefulness of our model as a CAD tool to assist less experienced radiologists in the field.","86":"The time-varying power spectrum of a time series process is a bivariate function that quantifies the magnitude of oscillations at different frequencies and times. To obtain low-dimensional, parsimonious measures from this functional parameter, applied researchers consider collapsed measures of power within local bands that partition the frequency space. Frequency bands commonly used in the scientific literature were historically derived, but they are not guaranteed to be optimal or justified for adequately summarizing information from a given time series process under current study. There is a dearth of methods for empirically constructing statistically optimal bands for a given signal. The goal of this article is to provide a standardized, unifying approach for deriving and analyzing customized frequency bands. A consistent, frequency-domain, iterative cumulative sum based scanning procedure is formulated to identify frequency bands that best preserve nonstationary information. A formal hypothesis testing procedure is also developed to test which, if any, frequency bands remain stationary. The proposed method is used to analyze heart rate variability of a patient during sleep and uncovers a refined partition of frequency bands that best summarize the time-varying power spectrum.","87":"Nowadays, events are spread rapidly along social networks. We are interested in whether people's responses to an event are affected by their friends' characteristics. For example, how soon will a person start playing a game given that his\/her friends like it? Studying social network dependence is an emerging research area. In this work, we propose a novel latent spatial autocorrelation Cox model to study social network dependence with time-to-event data. The proposed model introduces a latent indicator to characterize whether a person's survival time might be affected by his or her friends' features. We first propose a score-type test for detecting the existence of social network dependence. If it exists, we further develop an EM-type algorithm to estimate the model parameters. The performance of the proposed test and estimators are illustrated by simulation studies and an application to a time-to-event dataset about playing a popular mobile game from one of the largest online social network platforms. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","88":"Multimodal integrative analysis fuses different types of data collected on the same set of experimental subjects. It is becoming a norm in many branches of scientific research, such as multi-omics and multimodal neuroimaging analysis. In this article, we address the problem of simultaneous covariance inference of associations between multiple modalities, which is of a vital interest in multimodal integrative analysis. Recognizing that there are few readily available solutions in the literature for this type of problem, we develop a new simultaneous testing procedure. It provides an explicit quantification of statistical significance, a much improved detection power, as well as a rigid false discovery control. Our proposal makes novel and useful contributions from both the scientific perspective and the statistical methodological perspective. We demonstrate the efficacy of the new method through both simulations and a multimodal positron emission tomography study of associations between two hallmark pathological proteins of Alzheimer's disease.","89":"The two-phase design is a cost-effective sampling strategy to evaluate the effects of covariates on an outcome when certain covariates are too expensive to be measured on all study subjects. Under such a design, the outcome and inexpensive covariates are measured on all subjects in the first phase and the first-phase information is used to select subjects for measurements of expensive covariates in the second phase. Previous research on two-phase studies has focused largely on the inference procedures rather than the design aspects. We investigate the design efficiency of the two-phase study, as measured by the semiparametric efficiency bound for estimating the regression coefficients of expensive covariates. We consider general two-phase studies, where the outcome variable can be continuous, discrete, or censored, and the second-phase sampling can depend on the first-phase data in any manner. We develop optimal or approximately optimal two-phase designs, which can be substantially more efficient than the existing designs. We demonstrate the improvements of the new designs over the existing ones through extensive simulation studies and two large medical studies.","90":"When predicting an outcome is the scientific goal, one must decide on a metric by which to evaluate the quality of predictions. We consider the problem of measuring the performance of a prediction algorithm with the same data that were used to train the algorithm. Typical approaches involve bootstrapping or cross-validation. However, we demonstrate that bootstrap-based approaches often fail and standard cross-validation estimators may perform poorly. We provide a general study of cross-validation-based estimators that highlights the source of this poor performance, and propose an alternative framework for estimation using techniques from the efficiency theory literature. We provide a theorem establishing the weak convergence of our estimators. The general theorem is applied in detail to two specific examples and we discuss possible extensions to other parameters of interest. For the two explicit examples that we consider, our estimators demonstrate remarkable finite-sample improvements over standard approaches.","91":null,"92":"We introduce a new shrinkage prior on function spaces, called the functional horseshoe prior (fHS), that encourages shrinkage towards parametric classes of functions. Unlike other shrinkage priors for parametric models, the fHS shrinkage acts on the shape of the function rather than inducing sparsity on model parameters. We study the efficacy of the proposed approach by showing an adaptive posterior concentration property on the function. We also demonstrate consistency of the model selection procedure that thresholds the shrinkage parameter of the functional horseshoe prior. We apply the fHS prior to nonparametric additive models and compare its performance with procedures based on the standard horseshoe prior and several penalized likelihood approaches. We find that the new procedure achieves smaller estimation error and more accurate model selection than other procedures in several simulated and real examples. The supplementary material for this article, which contains additional simulated and real data examples, MCMC diagnostics, and proofs of the theoretical results, is available online.","93":"We propose a new approach for assigning weights to models using a divergence-based method (D-probabilities), relying on evaluating parametric models relative to a nonparametric Bayesian reference using Kullback-Leibler divergence. D-probabilities are useful in goodness-of-fit assessments, in comparing imperfect models, and in providing model weights to be used in model aggregation. D-probabilities avoid some of the disadvantages of Bayesian model probabilities, such as large sensitivity to prior choice, and tend to place higher weight on a greater diversity of models. In an application to linear model selection against a Gaussian process reference, we provide simple analytic forms for routine implementation and show that D-probabilities automatically penalize model complexity. Some asymptotic properties are described, and we provide interesting probabilistic interpretations of the proposed model weights. The framework is illustrated through simulation examples and an ozone data application.","94":"People are increasingly concerned with understanding their personal environment, including possible exposure to harmful air pollutants. In order to make informed decisions on their day-to-day activities, they are interested in real-time information on a localized scale. Publicly available, fine-scale, high-quality air pollution measurements acquired using mobile monitors represent a paradigm shift in measurement technologies. A methodological framework utilizing these increasingly fine-scale measurements to provide real-time air pollution maps and short-term air quality forecasts on a fine-resolution spatial scale could prove to be instrumental in increasing public awareness and understanding. The Google Street View study provides a unique source of data with spatial and temporal complexities, with the potential to provide information about commuter exposure and hot spots within city streets with high traffic. We develop a computationally efficient spatiotemporal model for these data and use the model to make short-term forecasts and high-resolution maps of current air pollution levels. We also show via an experiment that mobile networks can provide more nuanced information than an equally-sized fixed-location network. This modeling framework has important real-world implications in understanding citizens' personal environments, as data production and real-time availability continue to be driven by the ongoing development and improvement of mobile measurement technologies.","95":"In studies evaluating the accuracy of diagnostic tests, three designs are commonly used, crossover, randomized, and non-comparative. Existing methods for meta-analysis of diagnostic tests mainly consider the simple cases in which the reference test in all or none of the studies can be considered a gold standard test, and in which all studies use either a randomized or non-comparative design. The proliferation of diagnostic instruments and the diversity of study designs create a need for more general methods to combine studies that include or do not include a gold standard test and that use various designs. This paper extends the Bayesian hierarchical summary receiver operating characteristic model to network meta-analysis of diagnostic tests to simultaneously compare multiple tests within a missing data framework. The method accounts for correlations between multiple tests and for heterogeneity between studies. It also allows different studies to include different subsets of diagnostic tests and provides flexibility in the choice of summary statistics. The model is evaluated using simulations and illustrated using real data on tests for deep vein thrombosis, with sensitivity analyses.","96":null,"97":"Extracting important features from ultra-high dimensional data is one of the primary tasks in statistical learning, information theory, precision medicine and biological discovery. Many of the sure independent screening methods developed to meet these needs are suitable for special models under some assumptions. With the availability of more data types and possible models, a model-free generic screening procedure with fewer and less restrictive assumptions is desirable. In this paper, we propose a generic nonparametric sure independence screening procedure, called BCor-SIS, on the basis of a recently developed universal dependence measure: Ball correlation. We show that the proposed procedure has strong screening consistency even when the dimensionality is an exponential order of the sample size without imposing sub-exponential moment assumptions on the data. We investigate the flexibility of this procedure by considering three commonly encountered challenging settings in biological discovery or precision medicine: iterative BCor-SIS, interaction pursuit, and survival outcomes. We use simulation studies and real data analyses to illustrate the versatility and practicability of our BCor-SIS method.","98":"Diagnosing glaucoma progression is critical for limiting irreversible vision loss. A common method for assessing glaucoma progression uses a longitudinal series of visual fields (VF) acquired at regular intervals. VF data are characterized by a complex spatiotemporal structure due to the data generating process and ocular anatomy. Thus, advanced statistical methods are needed to make clinical determinations regarding progression status. We introduce a spatiotemporal boundary detection model that allows the underlying anatomy of the optic disc to dictate the spatial structure of the VF data across time. We show that our new method provides novel insight into vision loss that improves diagnosis of glaucoma progression using data from the Vein Pulsation Study Trial in Glaucoma and the Lions Eye Institute trial registry. Simulations are presented, showing the proposed methodology is preferred over existing spatial methods for VF data. Supplementary materials for this article are available online and the method is implemented in the R package womblR.","99":"When available, vaccines are an effective means of disease prevention. Unfortunately, efficacious vaccines have not yet been developed for several major infectious diseases, including HIV and malaria. Vaccine sieve analysis studies whether and how the efficacy of a vaccine varies with the genetics of the pathogen of interest, which can guide subsequent vaccine development and deployment. In sieve analyses, the effect of the vaccine on the cumulative incidence corresponding to each of several possible genotypes is often assessed within a competing risks framework. In the context of clinical trials, the estimators employed in these analyses generally do not account for covariates, even though the latter may be predictive of the study endpoint or censoring. Motivated by two recent preventive vaccine efficacy trials for HIV and malaria, we develop new methodology for vaccine sieve analysis. Our approach offers improved validity and efficiency relative to existing approaches by allowing covariate adjustment through ensemble machine learning. We derive results that indicate how to perform statistical inference using our estimators. Our analysis of the HIV and malaria trials shows markedly increased precision -- up to doubled efficiency in both trials -- under more plausible assumptions compared with standard methodology. Our findings provide greater evidence for vaccine sieve effects in both trials.","100":"A population quantity of interest in statistical shape analysis is the location of landmarks, which are points that aid in reconstructing and representing shapes of objects. We provide an automated, model-based approach to inferring landmarks given a sample of shape data. The model is formulated based on a linear reconstruction of the shape, passing through the specified points, and a Bayesian inferential approach is described for estimating unknown landmark locations. The question of how many landmarks to select is addressed in two different ways: (1) by defining a criterion-based approach, and (2) joint estimation of the number of landmarks along with their locations. Efficient methods for posterior sampling are also discussed. We motivate our approach using several simulated examples, as well as data obtained from applications in computer vision, biology and medical imaging.","101":"Health sciences research often involves both right- and interval-censored events because the occurrence of a symptomatic disease can only be observed up to the end of follow-up, while the occurrence of an asymptomatic disease can only be detected through periodic examinations. We formulate the effects of potentially time-dependent covariates on the joint distribution of multiple right- and interval-censored events through semiparametric proportional hazards models with random effects that capture the dependence both within and between the two types of events. We consider nonparametric maximum likelihood estimation and develop a simple and stable EM algorithm for computation. We show that the resulting estimators are consistent and the parametric components are asymptotically normal and efficient with a covariance matrix that can be consistently estimated by profile likelihood or nonparametric bootstrap. In addition, we leverage the joint modelling to provide dynamic prediction of disease incidence based on the evolving event history. Furthermore, we assess the performance of the proposed methods through extensive simulation studies. Finally, we provide an application to a major epidemiological cohort study. Supplementary materials for this article are available online.","102":null,"103":"Researchers are often interested in using observational data to estimate the effect on a health outcome of maintaining a continuous treatment within a pre-specified range over time; e.g. \"always exercise at least 30 minutes per day\". There may be many precise interventions that could achieve this range. In this paper we consider representative interventions. These are special cases of random dynamic interventions; interventions under which treatment at each time is assigned according to a random draw from a distribution that may depend on a subject's measured past. Estimators of risk under representative interventions on a time-varying treatment have previously been described based on g-estimation of structural nested cumulative failure time models. In this paper, we consider an alternative approach based on inverse probability weighting (IPW) of marginal structural models. In particular, we show that the risk under a representative intervention on a time-varying continuous treatment can be consistently estimated via computationally simple IPW methods traditionally used for deterministic static (i.e. \"nonrandom\" and \"nondynamic\") interventions for binary treatments. We present an application of IPW in this setting to estimate the 28-year risk of coronary heart disease under various representative interventions on lifestyle behaviors in the Nurses Health Study.","104":"Penetrance, which plays a key role in genetic research, is defined as the proportion of individuals with the genetic variants (i.e., genotype) that cause a particular trait and who have clinical symptoms of the trait (i.e., phenotype). We propose a Bayesian semiparametric approach to estimate the cancer-specific age-at-onset penetrance in the presence of the competing risk of multiple cancers. We employ a Bayesian semiparametric competing risk model to model the duration until individuals in a high-risk group develop different cancers, and accommodate family data using family-wise likelihoods. We tackle the ascertainment bias arising when family data are collected through probands in a high-risk population in which disease cases are more likely to be observed. We apply the proposed method to a cohort of 186 families with Li-Fraumeni syndrome identified through probands with sarcoma treated at MD Anderson Cancer Center from 1944 to 1982.","105":"Vector autoregressive (VAR) models aim to capture linear temporal interdependencies amongst multiple time series. They have been widely used in macroeconomics and financial econometrics and more recently have found novel applications in functional genomics and neuroscience. These applications have also accentuated the need to investigate the behavior of the VAR model in a high-dimensional regime, which provides novel insights into the role of temporal dependence for regularized estimates of the model's parameters. However, hardly anything is known regarding properties of the posterior distribution for Bayesian VAR models in such regimes. In this work, we consider a VAR model with two prior choices for the autoregressive coefficient matrix: a non-hierarchical matrix-normal prior and a hierarchical prior, which corresponds to an arbitrary scale mixture of normals. We establish posterior consistency for both these priors under standard regularity assumptions, when the dimension p of the VAR model grows with the sample size n (but still remains smaller than n). A special case corresponds to a shrinkage prior that introduces (group) sparsity in the columns of the model coefficient matrices. The performance of the model estimates are illustrated on synthetic and real macroeconomic data sets.","106":"It is a common interest in medicine to determine whether a hospital meets a benchmark created from an aggregate reference population, after accounting for differences in distributions of multiple covariates. Due to the difficulties of collecting individual-level data, however, it is often the case that only marginal distributions of the covariates are available, making covariate-adjusted comparison challenging. We propose and evaluate a novel approach for conducting indirect standardization when only marginal covariate distributions of the studied hospital are known, but complete information is available for the reference hospitals. We do this with the aid of two existing methods: iterative proportional fit, which estimates the cells of a contingency table when only marginal sums are known, and synthetic control methods, which create a counterfactual control group using a weighted combination of potential control groups. The proper application of these existing methods for indirect standardization would require accounting for the statistical uncertainties induced by a situation where no individual-level data is collected from the studied population. We address this need with a novel method which uses a random Dirichlet parametrization of the synthetic control weights to estimate uncertainty intervals for the standard incidence ratio. We demonstrate our novel methods by estimating hospital-level standardized incidence ratios for comparing the adjusted probability of computed tomography examinations with high radiations doses, relative to a reference standard and we evalauate out methods in a simulation study.","107":"One application of positron emission tomography (PET), a nuclear imaging technique, in neuroscience involves in vivo estimation of the density of various proteins (often, neuroreceptors) in the brain. PET scanning begins with the injection of a radiolabeled tracer that binds preferentially to the target protein; tracer molecules are then continuously delivered to the brain via the bloodstream. By detecting the radioactive decay of the tracer over time, dynamic PET data are constructed to reflect the concentration of the target protein in the brain at each time. The fundamental problem in the analysis of dynamic PET data involves estimating the impulse response function (IRF), which is necessary for describing the binding behavior of the injected radiotracer. Virtually all existing methods have three common aspects: summarizing the entire IRF with a single scalar measure; modeling each subject separately; and the imposition of parametric restrictions on the IRF. In contrast, we propose a functional data analytic approach that regards each subject's IRF as the basic analysis unit, models multiple subjects simultaneously, and estimates the IRF nonparametrically. We pose our model as a linear mixed effect model in which population level fixed effects and subject-specific random effects are expanded using a B-spline basis. Shrinkage and roughness penalties are incorporated in the model to enforce identifiability and smoothness of the estimated curves, respectively, while monotonicity and non-negativity constraints impose biological information on estimates. We illustrate this approach by applying it to clinical PET data with subjects belonging to three diagnosic groups. We explore differences among groups by means of pointwise confidence intervals of the estimated mean curves based on bootstrap samples.","108":"We consider the problem of learning a conditional Gaussian graphical model in the presence of latent variables. Building on recent advances in this field, we suggest a method that decomposes the parameters of a conditional Markov random field into the sum of a sparse and a low-rank matrix. We derive convergence bounds for this estimator and show that it is well-behaved in the high-dimensional regime as well as \"sparsistent\" (i.e., capable of recovering the graph structure). We then show how proximal gradient algorithms and semi-definite programming techniques can be employed to fit the model to thousands of variables. Through extensive simulations, we illustrate the conditions required for identifiability and show that there is a wide range of situations in which this model performs significantly better than its counterparts, for example, by accommodating more latent variables. Finally, the suggested method is applied to two datasets comprising individual level data on genetic variants and metabolites levels. We show our results replicate better than alternative approaches and show enriched biological signal. Supplementary materials for this article are available online.","109":"Heterosis, or hybrid vigor, is the enhancement of the phenotype of hybrid progeny relative to their inbred parents. Heterosis is extensively used in agriculture, and the underlying mechanisms are unclear. To investigate the molecular basis of phenotypic heterosis, researchers search tens of thousands of genes for heterosis with respect to expression in the transcriptome. Difficulty arises in the assessment of heterosis due to composite null hypotheses and non-uniform distributions for p-values under these null hypotheses. Thus, we develop a general hierarchical model for count data and a fully Bayesian analysis in which an efficient parallelized Markov chain Monte Carlo algorithm ameliorates the computational burden. We use our method to detect gene expression heterosis in a two-hybrid plant-breeding scenario, both in a real RNA-seq maize dataset and in simulation studies. In the simulation studies, we show our method has well-calibrated posterior probabilities and credible intervals when the model assumed in analysis matches the model used to simulate the data. Although model misspecification can adversely affect calibration, the methodology is still able to accurately rank genes. Finally, we show that hyperparameter posteriors are extremely narrow and an empirical Bayes (eBayes) approach based on posterior means from the fully Bayesian analysis provides virtually equivalent posterior probabilities, credible intervals, and gene rankings relative to the fully Bayesian solution. This evidence of equivalence provides support for the use of eBayes procedures in RNA-seq data analysis if accurate hyperparameter estimates can be obtained.","110":"We propose a novel Bayesian methodology for analyzing nonstationary time series that exhibit oscillatory behavior. We approximate the time series using a piecewise oscillatory model with unknown periodicities, where our goal is to estimate the change-points while simultaneously identifying the potentially changing periodicities in the data. Our proposed methodology is based on a trans-dimensional Markov chain Monte Carlo algorithm that simultaneously updates the change-points and the periodicities relevant to any segment between them. We show that the proposed methodology successfully identifies time changing oscillatory behavior in two applications which are relevant to e-Health and sleep research, namely the occurrence of ultradian oscillations in human skin temperature during the time of night rest, and the detection of instances of sleep apnea in plethysmographic respiratory traces. Supplementary materials for this article are available online.","111":"Glaucoma, a leading cause of blindness, is characterized by optic nerve damage related to intraocular pressure (IOP), but its full etiology is unknown. Researchers at UAB have devised a custom device to measure scleral strain continuously around the eye under fixed levels of IOP, which here is used to assess how strain varies around the posterior pole, with IOP, and across glaucoma risk factors such as age. The hypothesis is that scleral strain decreases with age, which could alter biomechanics of the optic nerve head and cause damage that could eventually lead to glaucoma. To evaluate this hypothesis, we adapted Bayesian Functional Mixed Models to model these complex data consisting of correlated functions on spherical scleral surface, with nonparametric age effects allowed to vary in magnitude and smoothness across the scleral surface, multi-level random effect functions to capture within-subject correlation, and functional growth curve terms to capture serial correlation across IOPs that can vary around the scleral surface. Our method yields fully Bayesian inference on the scleral surface or any aggregation or transformation thereof, and reveals interesting insights into the biomechanical etiology of glaucoma. The general modeling framework described is very flexible and applicable to many complex, high-dimensional functional data.","112":"Sorted L-One Penalized Estimation (SLOPE, Bogdan et al., 2013, 2015) is a relatively new convex optimization procedure which allows for adaptive selection of regressors under sparse high dimensional designs. Here we extend the idea of SLOPE to deal with the situation when one aims at selecting whole groups of explanatory variables instead of single regressors. Such groups can be formed by clustering strongly correlated predictors or groups of dummy variables corresponding to different levels of the same qualitative predictor. We formulate the respective convex optimization problem, gSLOPE (group SLOPE), and propose an efficient algorithm for its solution. We also define a notion of the group false discovery rate (gFDR) and provide a choice of the sequence of tuning parameters for gSLOPE so that gFDR is provably controlled at a prespecified level if the groups of variables are orthogonal to each other. Moreover, we prove that the resulting procedure adapts to unknown sparsity and is asymptotically minimax with respect to the estimation of the proportions of variance of the response variable explained by regressors from different groups. We also provide a method for the choice of the regularizing sequence when variables in different groups are not orthogonal but statistically independent and illustrate its good properties with computer simulations. Finally, we illustrate the advantages of gSLOPE in the context of Genome Wide Association Studies. R package grpSLOPE with an implementation of our method is available on CRAN.","113":"This paper proposes a novel paradigm for building regression trees and ensemble learning in survival analysis. Generalizations of the CART and Random Forests algorithms for general loss functions, and in the latter case more general bootstrap procedures, are both introduced. These results, in combination with an extension of the theory of censoring unbiased transformations applicable to loss functions, underpin the development of two new classes of algorithms for constructing survival trees and survival forests: Censoring Unbiased Regression Trees and Censoring Unbiased Regression Ensembles. For a certain \"doubly robust\" censoring unbiased transformation of squared error loss, we further show how these new algorithms can be implemented using existing software (e.g., CART, random forests). Comparisons of these methods to existing ensemble procedures for predicting survival probabilities are provided in both simulated settings and through applications to four datasets. It is shown that these new methods either improve upon, or remain competitive with, existing implementations of random survival forests, conditional inference forests, and recursively imputed survival trees.","114":"Identifying patient-specific prognostic biomarkers is of critical importance in developing personalized treatment for clinically and molecularly heterogeneous diseases such as cancer. In this article, we propose a novel regression framework, Bayesian hierarchical varying-sparsity regression (BEHAVIOR) models to select clinically relevant disease markers by integrating proteogenomic (proteomic+genomic) and clinical data. Our methods allow flexible modeling of protein-gene relationships as well as induces sparsity in both protein-gene and protein-survival relationships, to select ge-nomically driven prognostic protein markers at the patient-level. Simulation studies demonstrate the superior performance of BEHAVIOR against competing method in terms of both protein marker selection and survival prediction. We apply BEHAV-IOR to The Cancer Genome Atlas (TCGA) proteogenomic pan-cancer data and find several interesting prognostic proteins and pathways that are shared across multiple cancers and some that exclusively pertain to specific cancers.","115":"Panel data, also known as longitudinal data, consist of a collection of time series. Each time series, which could itself be multivariate, comprises a sequence of measurements taken on a distinct unit. Mechanistic modeling involves writing down scientifically motivated equations describing the collection of dynamic systems giving rise to the observations on each unit. A defining characteristic of panel systems is that the dynamic interaction between units should be negligible. Panel models therefore consist of a collection of independent stochastic processes, generally linked through shared parameters while also having unit-specific parameters. To give the scientist flexibility in model specification, we are motivated to develop a framework for inference on panel data permitting the consideration of arbitrary nonlinear, partially observed panel models. We build on iterated filtering techniques that provide likelihood-based inference on nonlinear partially observed Markov process models for time series data. Our methodology depends on the latent Markov process only through simulation; this plug-and-play property ensures applicability to a large class of models. We demonstrate our methodology on a toy example and two epidemiological case studies. We address inferential and computational issues arising due to the combination of model complexity and dataset size. Supplementary materials for this article are available online.","116":null,"117":"It is of fundamental interest in statistics to test the significance of a set of covariates. For example, in genome-wide association studies, a joint null hypothesis of no genetic effect is tested for a set of multiple genetic variants. The minimum p-value method, higher criticism, and Berk-Jones tests are particularly effective when the covariates with nonzero effects are sparse. However, the correlations among covariates and the non-Gaussian distribution of the response pose a great challenge towards the p-value calculation of the three tests. In practice, permutation is commonly used to obtain accurate p-values, but it is computationally very intensive, especially when we need to conduct a large amount of hypothesis testing. In this paper, we propose a Gaussian approximation method based on a Monte Carlo scheme, which is computationally more efficient than permutation while still achieving similar accuracy. We derive non-asymptotic approximation error bounds that could vanish in the limit even if the number of covariates is much larger than the sample size. Through real-genotype-based simulations and data analysis of a genome-wide association study of Crohn's disease, we compare the accuracy and computation cost of our proposed method, of permutation, and of the method based on asymptotic distribution.","118":"Direct regression modeling of the subdistribution has become popular for analyzing data with multiple, competing event types. All general approaches so far are based on non-likelihood based procedures and target covariate effects on the subdistribution. We introduce a novel weighted likelihood function that allows for a direct extension of the Fine-Gray model to a broad class of semiparametric regression models. The model accommodates time-dependent covariate effects on the subdistribution hazard. To motivate the proposed likelihood method, we derive standard nonparametric estimators and discuss a new interpretation based on pseudo risk sets. We establish consistency and asymptotic normality of the estimators and propose a sandwich estimator of the variance. In comprehensive simulation studies we demonstrate the solid performance of the weighted NPMLE in the presence of independent right censoring. We provide an application to a very large bone marrow transplant dataset, thereby illustrating its practical utility.","119":"The aim of this paper is to develop a novel class of functional structural equation models (FSEMs) for dissecting functional genetic and environmental effects on twin functional data, while characterizing the varying association between functional data and covariates of interest. We propose a three-stage estimation procedure to estimate varying coefficient functions for various covariates (e.g., gender) as well as three covariance operators for the genetic and environmental effects. We develop an inference procedure based on weighted likelihood ratio statistics to test the genetic\/environmental effect at either a fixed location or a compact region. We also systematically carry out the theoretical analysis of the estimated varying functions, the weighted likelihood ratio statistics, and the estimated covariance operators. We conduct extensive Monte Carlo simulations to examine the finite-sample performance of the estimation and inference procedures. We apply the proposed FSEM to quantify the degree of genetic and environmental effects on twin white-matter tracts obtained from the UNC early brain development study.","120":"Nonlinear kernel regression models are often used in statistics and machine learning because they are more accurate than linear models. Variable selection for kernel regression models is a challenge partly because, unlike the linear regression setting, there is no clear concept of an effect size for regression coefficients. In this paper, we propose a novel framework that provides an effect size analog for each explanatory variable in Bayesian kernel regression models when the kernel is shift-invariant - for example, the Gaussian kernel. We use function analytic properties of shift-invariant reproducing kernel Hilbert spaces (RKHS) to define a linear vector space that: (i) captures nonlinear structure, and (ii) can be projected onto the original explanatory variables. This projection onto the original explanatory variables serves as an analog of effect sizes. The specific function analytic property we use is that shift-invariant kernel functions can be approximated via random Fourier bases. Based on the random Fourier expansion, we propose a computationally efficient class of Bayesian approximate kernel regression (BAKR) models for both nonlinear regression and binary classification for which one can compute an analog of effect sizes. We illustrate the utility of BAKR by examining two important problems in statistical genetics: genomic selection (i.e. phenotypic prediction) and association mapping (i.e. inference of significant variants or loci). State-of-the-art methods for genomic selection and association mapping are based on kernel regression and linear models, respectively. BAKR is the first method that is competitive in both settings.","121":"Many scientific studies collect data where the response and predictor variables are both functions of time, location, or some other covariate. Understanding the relationship between these functional variables is a common goal in these studies. Motivated from two real-life examples, we present in this paper a function-on-function regression model that can be used to analyze such kind of functional data. Our estimator of the 2D coefficient function is the optimizer of a form of penalized least squares where the penalty enforces a certain level of smoothness on the estimator. Our first result is the Representer Theorem which states that the exact optimizer of the penalized least squares actually resides in a data-adaptive finite dimensional subspace although the optimization problem is defined on a function space of infinite dimensions. This theorem then allows us an easy incorporation of the Gaussian quadrature into the optimization of the penalized least squares, which can be carried out through standard numerical procedures. We also show that our estimator achieves the minimax convergence rate in mean prediction under the framework of function-on-function regression. Extensive simulation studies demonstrate the numerical advantages of our method over the existing ones, where a sparse functional data extension is also introduced. The proposed method is then applied to our motivating examples of the benchmark Canadian weather data and a histone regulation study.","122":"Precision medicine is currently a topic of great interest in clinical and intervention science. A key component of precision medicine is that it is evidence-based, i.e., data-driven, and consequently there has been tremendous interest in estimation of precision medicine strategies using observational or randomized study data. One way to formalize precision medicine is through a treatment regime, which is a sequence of decision rules, one per stage of clinical intervention, that map up-to-date patient information to a recommended treatment. An optimal treatment regime is defined as maximizing the mean of some cumulative clinical outcome if applied to a population of interest. It is well-known that even under simple generative models an optimal treatment regime can be a highly nonlinear function of patient information. Consequently, a focal point of recent methodological research has been the development of flexible models for estimating optimal treatment regimes. However, in many settings, estimation of an optimal treatment regime is an exploratory analysis intended to generate new hypotheses for subsequent research and not to directly dictate treatment to new patients. In such settings, an estimated treatment regime that is interpretable in a domain context may be of greater value than an unintelligible treatment regime built using 'black-box' estimation methods. We propose an estimator of an optimal treatment regime composed of a sequence of decision rules, each expressible as a list of \"if-then\" statements that can be presented as either a paragraph or as a simple flowchart that is immediately interpretable to domain experts. The discreteness of these lists precludes smooth, i.e., gradient-based, methods of estimation and leads to non-standard asymptotics. Nevertheless, we provide a computationally efficient estimation algorithm, prove consistency of the proposed estimator, and derive rates of convergence. We illustrate the proposed methods using a series of simulation examples and application to data from a sequential clinical trial on bipolar disorder.","123":null,"124":"The divide and conquer method is a common strategy for handling massive data. In this article, we study the divide and conquer method for cubic-rate estimators under the massive data framework. We develop a general theory for establishing the asymptotic distribution of the aggregated M-estimators using a weighted average with weights depending on the subgroup sample sizes. Under certain condition on the growing rate of the number of subgroups, the resulting aggregated estimators are shown to have faster convergence rate and asymptotic normal distribution, which are more tractable in both computation and inference than the original M-estimators based on pooled data. Our theory applies to a wide class of M-estimators with cube root convergence rate, including the location estimator, maximum score estimator and value search estimator. Empirical performance via simulations and a real data application also validate our theoretical findings.","125":"Medical therapy often consists of multiple stages, with a treatment chosen by the physician at each stage based on the patient's history of treatments and clinical outcomes. These decisions can be formalized as a dynamic treatment regime. This paper describes a new approach for optimizing dynamic treatment regimes that bridges the gap between Bayesian inference and existing approaches, like Q-learning. The proposed approach fits a series of Bayesian regression models, one for each stage, in reverse sequential order. Each model uses as a response variable the remaining payoff assuming optimal actions are taken at subsequent stages, and as covariates the current history and relevant actions at that stage. The key difficulty is that the optimal decision rules at subsequent stages are unknown, and even if these decision rules were known the relevant response variables may be counterfactual. However, posterior distributions can be derived from the previously fitted regression models for the optimal decision rules and the counterfactual response variables under a particular set of rules. The proposed approach averages over these posterior distributions when fitting each regression model. An efficient sampling algorithm for estimation is presented, along with simulation studies that compare the proposed approach with Q-learning.","126":"Motivated by gene set enrichment analysis, we investigate the problem of combined hypothesis testing on a graph. A general framework is introduced to make effective use of the structural information of the underlying graph when testing multivariate means. A new testing procedure is proposed within this framework, and shown to be optimal in that it can consistently detect departures from the collective null at a rate that no other test could improve, for almost all graphs. We also provide general performance bounds for the proposed test under any specific graph, and illustrate their utility through several common types of graphs. Numerical experiments are presented to further demonstrate the merits of our approach.","127":"Ordinary differential equations (ODEs) are widely used to model the dynamic behavior of a complex system. Parameter estimation and variable selection for a \"Big System\" with linear ODEs are very challenging due to the need of nonlinear optimization in an ultra-high dimensional parameter space. In this article, we develop a parameter estimation and variable selection method based on the ideas of similarity transformation and separable least squares (SLS). Simulation studies demonstrate that the proposed matrix-based SLS method could be used to estimate the coefficient matrix more accurately and perform variable selection for a linear ODE system with thousands of dimensions and millions of parameters much better than the direct least squares (LS) method and the vector-based two-stage method that are currently available. We applied this new method to two real data sets: a yeast cell cycle gene expression data set with 30 dimensions and 930 unknown parameters and the Standard &amp; Poor 1500 index stock price data with 1250 dimensions and 1,563,750 unknown parameters, to illustrate the utility and numerical performance of the proposed parameter estimation and variable selection method for big systems in practice.","128":"","129":"In this article, we consider the sparse tensor singular value decomposition, which aims for dimension reduction on high-dimensional high-order data with certain sparsity structure. A method named sparse tensor alternating thresholding for singular value decomposition (STAT-SVD) is proposed. The proposed procedure features a novel double projection &amp; thresholding scheme, which provides a sharp criterion for thresholding in each iteration. Compared with regular tensor SVD model, STAT-SVD permits more robust estimation under weaker assumptions. Both the upper and lower bounds for estimation accuracy are developed. The proposed procedure is shown to be minimax rate-optimal in a general class of situations. Simulation studies show that STAT-SVD performs well under a variety of configurations. We also illustrate the merits of the proposed procedure on a longitudinal tensor dataset on European country mortality rates. Supplementary materials for this article are available online.","130":"The United States Environmental Protection Agency considers nutrient pollution in stream ecosystems one of the U.S.' most pressing environmental challenges. But limited independent replicates, lack of experimental randomization, and space- and time-varying confounding handicap causal inference on effects of nutrient pollution. In this paper the causal g-methods are extended to allow for exposures to vary in time and space in order to assess the effects of nutrient pollution on chlorophyll a - a proxy for algal production. Publicly available data from North Carolina's Cape Fear River and a simulation study are used to show how causal effects of upstream nutrient concentrations on downstream chlorophyll a levels may be estimated from typical water quality monitoring data. Estimates obtained from the parametric g-formula, a marginal structural model, and a structural nested model indicate that chlorophyll a concentrations at Lock and Dam 1 were influenced by nitrate concentrations measured 86 to 109 km upstream, an area where four major industrial and municipal point sources discharge wastewater.","131":null,"132":"Inferring patterns of synchronous brain activity from a heterogeneous sample of electroencephalograms (EEG) is scientifically and methodologically challenging. While it is intuitively and statistically appealing to rely on readings from more than one individual in order to highlight recurrent patterns of brain activation, pooling information across subjects presents non-trivial methodological problems. We discuss some of the scientific issues associated with the understanding of synchronized neuronal activity and propose a methodological framework for statistical inference from a sample of EEG readings. Our work builds on classical contributions in time-series, clustering and functional data analysis, in an effort to reframe a challenging inferential problem in the context of familiar analytical techniques. Some attention is paid to computational issues, with a proposal based on the combination of machine learning and Bayesian techniques.","133":"Large-scale multiple testing with correlated and heavy-tailed data arises in a wide range of research areas from genomics, medical imaging to finance. Conventional methods for estimating the false discovery proportion (FDP) often ignore the effect of heavy-tailedness and the dependence structure among test statistics, and thus may lead to inefficient or even inconsistent estimation. Also, the commonly imposed joint normality assumption is arguably too stringent for many applications. To address these challenges, in this paper we propose a Factor-Adjusted Robust Multiple Testing (FarmTest) procedure for large-scale simultaneous inference with control of the false discovery proportion. We demonstrate that robust factor adjustments are extremely important in both controlling the FDP and improving the power. We identify general conditions under which the proposed method produces consistent estimate of the FDP. As a byproduct that is of independent interest, we establish an exponential-type deviation inequality for a robust U-type covariance estimator under the spectral norm. Extensive numerical experiments demonstrate the advantage of the proposed method over several state-of-the-art methods especially when the data are generated from heavy-tailed distributions. The proposed procedures are implemented in the R-package FarmTest.","134":null,"135":"This article develops a pair of new prediction summary measures for a nonlinear prediction function with right-censored time-to-event data. The first measure, defined as the proportion of explained variance by a linearly corrected prediction function, quantifies the potential predictive power of the nonlinear prediction function. The second measure, defined as the proportion of explained prediction error by its corrected prediction function, gauges the closeness of the prediction function to its corrected version and serves as a supplementary measure to indicate (by a value less than 1) whether the correction is needed to fulfill its potential predictive power and quantify how much prediction error reduction can be realized with the correction. The two measures together provide a complete summary of the predictive accuracy of the nonlinear prediction function. We motivate these measures by first establishing a variance decomposition and a prediction error decomposition at the population level and then deriving uncensored and censored sample versions of these decompositions. We note that for the least square prediction function under the linear model with no censoring, the first measure reduces to the classical coefficient of determination and the second measure degenerates to 1. We show that the sample measures are consistent estimators of their population counterparts and conduct extensive simulations to investigate their finite sample properties. A real data illustration is provided using the PBC data. Supplementary materials for this article are available online. An R package PAmeasures has been developed and made available via the CRAN R library. Supplementary materials for this article are available online.","136":"Under the logistic regression framework, we propose a forward-backward method, SODA, for variable selection with both main and quadratic interaction terms. In the forward stage, SODA adds in predictors that have significant overall effects, whereas in the backward stage SODA removes unimportant terms to optimize the extended Bayesian Information Criterion (EBIC). Compared with existing methods for variable selection in quadratic discriminant analysis, SODA can deal with high-dimensional data in which the number of predictors is much larger than the sample size and does not require the joint normality assumption on predictors, leading to much enhanced robustness. We further extend SODA to conduct variable selection and model fitting for general index models. Compared with existing variable selection methods based on the Sliced Inverse Regression (SIR) (Li, 1991), SODA requires neither linearity nor constant variance condition and is thus more robust. Our theoretical analysis establishes the variable-selection consistency of SODA under high-dimensional settings, and our simulation studies as well as real-data applications demonstrate superior performances of SODA in dealing with non-Gaussian design matrices in both logistic and general index models.","137":"Despite the risk of misspecification they are tied to, parametric models continue to be used in statistical practice because they are simple and convenient to use. In particular, efficient estimation procedures in parametric models are easy to describe and implement. Unfortunately, the same cannot be said of semiparametric and nonparametric models. While the latter often reflect the level of available scientific knowledge more appropriately, performing efficient inference in these models is generally challenging. The efficient influence function is a key analytic object from which the construction of asymptotically efficient estimators can potentially be streamlined. However, the theoretical derivation of the efficient influence function requires specialized knowledge and is often a difficult task, even for experts. In this paper, we present a novel representation of the efficient influence function and describe a numerical procedure for approximating its evaluation. The approach generalizes the nonparametric procedures of Frangakis et al. (2015) and Luedtke et al. (2015) to arbitrary models. We present theoretical results to support our proposal, and illustrate the method in the context of several semiparametric problems. The proposed approach is an important step toward automating efficient estimation in general statistical models, thereby rendering more accessible the use of realistic models in statistical analyses.","138":"In studying structural inter-connections in the human brain, it is common to first estimate fiber bundles connecting different regions relying on diffusion MRI. These fiber bundles act as highways for neural activity. Current statistical methods reduce the rich information into an adjacency matrix, with the elements containing a count of fibers or a mean diffusion feature along the fibers. The goal of this article is to avoid discarding the rich geometric information of fibers, developing flexible models for characterizing the population distribution of fibers between brain regions of interest within and across different individuals. We start by decomposing each fiber into a rotation matrix, shape and translation from a global reference curve. These components are viewed as data lying on a product space composed of different Euclidean spaces and manifolds. To nonparametrically model the distribution within and across individuals, we rely on a hierarchical mixture of product kernels specific to the component spaces. Taking a Bayesian approach to inference, we develop efficient methods for posterior sampling. The approach automatically produces clusters of fibers within and across individuals. Applying the method to Human Connectome Project data, we find interesting relationships between brain fiber geometry and reading ability. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.","139":"In various real-world problems, we are presented with classification problems with positive and unlabeled data, referred to as presence-only responses. In this article we study variable selection in the context of presence only responses where the number of features or covariates p is large. The combination of presence-only responses and high dimensionality presents both statistical and computational challenges. In this article, we develop the PUlasso algorithm for variable selection and classification with positive and unlabeled responses. Our algorithm involves using the majorization-minimization framework which is a generalization of the well-known expectation-maximization (EM) algorithm. In particular to make our algorithm scalable, we provide two computational speed-ups to the standard EM algorithm. We provide a theoretical guarantee where we first show that our algorithm converges to a stationary point, and then prove that any stationary point within a local neighborhood of the true parameter achieves the minimax optimal mean-squared error under both strict sparsity and group sparsity assumptions. We also demonstrate through simulations that our algorithm outperforms state-of-the-art algorithms in the moderate p settings in terms of classification performance. Finally, we demonstrate that our PUlasso algorithm performs well on a biochemistry example. Supplementary materials for this article are available online.","140":null,"141":"We consider the problem of controlling the FDR among discoveries from searching an incomplete database. This problem differs from the classical multiple testing setting because there are two different types of false discoveries: those arising from objects that have no match in the database and those that are incorrectly matched. We show that commonly used FDR controlling procedures are inadequate for this setup, a special case of which is tandem mass spectrum identification. We then derive a novel FDR controlling approach which extensive simulations suggest is unbiased. We also compare its performance with problem-specific as well as general FDR controlling procedures using both simulated and real mass spectrometry data.","142":"Many modern network datasets arise from processes of interactions in a population, such as phone calls, email exchanges, co-authorships, and professional collaborations. In such interaction networks, the edges comprise the fundamental statistical units, making a framework for edge-labeled networks more appropriate for statistical analysis. In this context we initiate the study of edge exchangeable network models and explore its basic statistical properties. Several theoretical and practical features make edge exchangeable models better suited to many applications in network analysis than more common vertex-centric approaches. In particular, edge exchangeable models allow for sparse structure and power law degree distributions, both of which are widely observed empirical properties that cannot be handled naturally by more conventional approaches. Our discussion culminates in the Hollywood model, which we identify here as the canonical family of edge exchangeable distributions. The Hollywood model is computationally tractable, admits a clear interpretation, exhibits good theoretical properties, and performs reasonably well in estimation and prediction as we demonstrate on real network datasets. As a generalization of the Hollywood model, we further identify the vertex components model as a nonparametric subclass of models with a convenient stick breaking construction.","143":"In mobile health interventions aimed at behavior change and maintenance, treatments are provided in real time to manage current or impending high risk situations or promote healthy behaviors in near real time. Currently there is great scientific interest in developing data analysis approaches to guide the development of mobile interventions. In particular data from mobile health studies might be used to examine effect moderators-individual characteristics, time-varying context or past treatment response that moderate the effect of current treatment on a subsequent response. This paper introduces a formal definition for moderated effects in terms of potential outcomes, a definition that is particularly suited to mobile interventions, where treatment occasions are numerous, individuals are not always available for treatment, and potential moderators might be influenced by past treatment. Methods for estimating moderated effects are developed and compared. The proposed approach is illustrated using BASICS-Mobile, a smartphone-based intervention designed to curb heavy drinking and smoking among college students.","144":"Finding the optimal treatment regime (or a series of sequential treatment regimes) based on individual characteristics has important applications in areas such as precision medicine, government policies and active labor market interventions. In the current literature, the optimal treatment regime is usually defined as the one that maximizes the average benefit in the potential population. This paper studies a general framework for estimating the quantile-optimal treatment regime, which is of importance in many real-world applications. Given a collection of treatment regimes, we consider robust estimation of the quantile-optimal treatment regime, which does not require the analyst to specify an outcome regression model. We propose an alternative formulation of the estimator as a solution of an optimization problem with an estimated nuisance parameter. This novel representation allows us to investigate the asymptotic theory of the estimated optimal treatment regime using empirical process techniques. We derive theory involving a nonstandard convergence rate and a non-normal limiting distribution. The same nonstandard convergence rate would also occur if the mean optimality criterion is applied, but this has not been studied. Thus, our results fill an important theoretical gap for a general class of policy search methods in the literature. The paper investigates both static and dynamic treatment regimes. In addition, doubly robust estimation and alternative optimality criterion such as that based on Gini's mean difference or weighted quantiles are investigated. Numerical simulations demonstrate the performance of the proposed estimator. A data example from a trial in HIV+ patients is used to illustrate the application.","145":"We propose a novel method for estimating population-level and subject-specific effects of covariates on the variability of functional data. We extend the functional principal components analysis framework by modeling the variance of principal component scores as a function of covariates and subject-specific random effects. In a setting where principal components are largely invariant across subjects and covariate values, modeling the variance of these scores provides a flexible and interpretable way to explore factors that affect the variability of functional data. Our work is motivated by a novel dataset from an experiment assessing upper extremity motor control, and quantifies the reduction in motion variance associated with skill learning.","146":"We show that under the null, the 2 log(Bayes factor) is asymptotically distributed as a weighted sum of chi-squared random variables with a shifted mean. This claim holds for Bayesian multi-linear regression with a family of conjugate priors, namely, the normal-inverse-gamma prior, the g-prior, and the normal prior. Our results have three immediate impacts. First, we can compute analytically a p-value associated with a Bayes factor without the need of permutation. We provide a software package that can evaluate the p-value associated with Bayes factor efficiently and accurately. Second, the null distribution is illuminating to some intrinsic properties of Bayes factor, namely, how Bayes factor quantitatively depends on prior and the genesis of Bartlett's paradox. Third, enlightened by the null distribution of Bayes factor, we formulate a novel scaled Bayes factor that depends less on the prior and is immune to Bartlett's paradox. When two tests have an identical p-value, the test with a larger power tends to have a larger scaled Bayes factor, a desirable property that is missing for the (unscaled) Bayes factor.","147":"We introduce a modeling approach for characterizing heterogeneity in healthcare utilization using massive medical claims data. We first translate the medical claims observed for a large study population and across five years into individual-level discrete events of care called utilization sequences. We model the utilization sequences using an exponential proportional hazards mixture model to capture heterogeneous behaviors in patients' healthcare utilization. The objective is to cluster patients according to their longitudinal utilization behaviors and to determine the main drivers of variation in healthcare utilization while controlling for the demographic, geographic, and health characteristics of the patients. Due to the computational infeasibility of fitting a parametric proportional hazards model for high-dimensional, large sample size data we use an iterative one-step procedure to estimate the model parameters and impute the cluster membership. The approach is used to draw inferences on utilization behaviors of children in the Medicaid system with persistent asthma across six states. We conclude with policy implications for targeted interventions to improve adherence to recommended care practices for pediatric asthma.","148":"Ordinal outcomes are common in scientific research and everyday practice, and we often rely on regression models to make inference. A long-standing problem with such regression analyses is the lack of effective diagnostic tools for validating model assumptions. The difficulty arises from the fact that an ordinal variable has discrete values that are labeled with, but not, numerical values. The values merely represent ordered categories. In this paper, we propose a surrogate approach to defining residuals for an ordinal outcome Y. The idea is to define a continuous variable S as a \"surrogate\" of Y and then obtain residuals based on S. For the general class of cumulative link regression models, we study the residual's theoretical and graphical properties. We show that the residual has null properties similar to those of the common residuals for continuous outcomes. Our numerical studies demonstrate that the residual has power to detect misspecification with respect to 1) mean structures; 2) link functions; 3) heteroscedasticity; 4) proportionality; and 5) mixed populations. The proposed residual also enables us to develop numeric measures for goodness-of-fit using classical distance notions. Our results suggest that compared to a previously defined residual, our residual can reveal deeper insights into model diagnostics. We stress that this work focuses on residual analysis, rather than hypothesis testing. The latter has limited utility as it only provides a single p-value, whereas our residual can reveal what components of the model are misspecified and advise how to make improvements.","149":"Necrotic enteritis (NE) is a serious disease of poultry caused by the bacterium C. perfringens. To identify proteins of C. perfringens that confer virulence with respect to NE, the protein secretions of four NE disease-producing strains and one baseline non-disease-producing strain of C. perfringens were examined. The problem then becomes a clustering task, for the identification of two extreme groups of proteins that were produced at either concordantly higher or concordantly lower levels across all four disease-producing strains compared to the baseline, when most of the proteins do not exhibit significant change across all strains. However, the existence of some nuisance proteins of discordant change may severely distort any biologically meaningful cluster pattern. We develop a tailored multivariate clustering approach to robustly identify the proteins of concordant change. Using a three-component normal mixture model as the skeleton, our approach incorporates several constraints to account for biological expectations and data characteristics. More importantly, we adopt a sparse mean-shift parameterization in the reference distribution, coupled with a regularized estimation approach, to flexibly accommodate proteins of discordant change. We explore the connections and differences between our approach and other robust clustering methods, and resolve the issue of unbounded likelihood under an eigenvalue-ratio condition. Simulation studies demonstrate the superior performance of our method compared with a number of alternative approaches. Our protein analysis along with further biological investigations may shed light on the discovery of the complete set of virulence factors in NE.","150":"","151":"Mismeasured time to event data used as a predictor in risk prediction models will lead to inaccurate predictions. This arises in the context of self-reported family history, a time to event predictor often measured with error, used in Mendelian risk prediction models. Using validation data, we propose a method to adjust for this type of error. We estimate the measurement error process using a nonparametric smoothed Kaplan-Meier estimator, and use Monte Carlo integration to implement the adjustment. We apply our method to simulated data in the context of both Mendelian and multivariate survival prediction models. Simulations are evaluated using measures of mean squared error of prediction (MSEP), area under the response operating characteristics curve (ROC-AUC), and the ratio of observed to expected number of events. These results show that our method mitigates the effects of measurement error mainly by improving calibration and total accuracy. We illustrate our method in the context of Mendelian risk prediction models focusing on misreporting of breast cancer, fitting the measurement error model on data from the University of California at Irvine, and applying our method to counselees from the Cancer Genetics Network. We show that our method improves overall calibration, especially in low risk deciles.","152":"Structural equation modeling is commonly used to capture complex structures of relationships among multiple variables, both latent and observed. We propose a general class of structural equation models with a semiparametric component for potentially censored survival times. We consider nonparametric maximum likelihood estimation and devise a combined Expectation-Maximization and Newton-Raphson algorithm for its implementation. We establish conditions for model identifiability and prove the consistency, asymptotic normality, and semiparametric efficiency of the estimators. Finally, we demonstrate the satisfactory performance of the proposed methods through simulation studies and provide an application to a motivating cancer study that contains a variety of genomic variables. Supplementary materials for this article are available online.","153":"For massive data, the family of subsampling algorithms is popular to downsize the data volume and reduce computational burden. Existing studies focus on approximating the ordinary least squares estimate in linear regression, where statistical leverage scores are often used to define subsampling probabilities. In this paper, we propose fast subsampling algorithms to efficiently approximate the maximum likelihood estimate in logistic regression. We first establish consistency and asymptotic normality of the estimator from a general subsampling algorithm, and then derive optimal subsampling probabilities that minimize the asymptotic mean squared error of the resultant estimator. An alternative minimization criterion is also proposed to further reduce the computational cost. The optimal subsampling probabilities depend on the full data estimate, so we develop a two-step algorithm to approximate the optimal subsampling procedure. This algorithm is computationally efficient and has a significant reduction in computing time compared to the full data approach. Consistency and asymptotic normality of the estimator from a two-step algorithm are also established. Synthetic and real data sets are used to evaluate the practical performance of the proposed method.","154":"Suppose one has a collection of parameters indexed by a (possibly infinite dimensional) set. Given data generated from some distribution, the objective is to estimate the maximal parameter in this collection evaluated at the distribution that generated the data. This estimation problem is typically non-regular when the maximizing parameter is non-unique, and as a result standard asymptotic techniques generally fail in this case. We present a technique for developing parametric-rate confidence intervals for the quantity of interest in these non-regular settings. We show that our estimator is asymptotically efficient when the maximizing parameter is unique so that regular estimation is possible. We apply our technique to a recent example from the literature in which one wishes to report the maximal absolute correlation between a prespecified outcome and one of p predictors. The simplicity of our technique enables an analysis of the previously open case where p grows with sample size. Specifically, we only require that log p grows slower than n , where n is the sample size. We show that, unlike earlier approaches, our method scales to massive data sets: the point estimate and confidence intervals can be constructed in O(np) time.","155":"We consider the problem of multivariate density deconvolution when interest lies in estimating the distribution of a vector valued random variable X but precise measurements on X are not available, observations being contaminated by measurement errors U. The existing sparse literature on the problem assumes the density of the measurement errors to be completely known. We propose robust Bayesian semiparametric multivariate deconvolution approaches when the measurement error density of U is not known but replicated proxies are available for at least some individuals. Additionally, we allow the variability of U to depend on the associated unobserved values of X through unknown relationships, which also automatically includes the case of multivariate multiplicative measurement errors. Basic properties of finite mixture models, multivariate normal kernels and exchangeable priors are exploited in novel ways to meet modeling and computational challenges. Theoretical results showing the flexibility of the proposed methods in capturing a wide variety of data generating processes are provided. We illustrate the efficiency of the proposed methods in recovering the density of X through simulation experiments. The methodology is applied to estimate the joint consumption pattern of different dietary components from contaminated 24 hour recalls. Supplementary Material presents substantive additional details.","156":"The mid-p-value is a proposed improvement on the ordinary p-value for the case where the test statistic is partially or completely discrete. In this case, the ordinary p-value is conservative, meaning that its null distribution is larger than a uniform distribution on the unit interval, in the usual stochastic order. The mid-p-value is not conservative. However, its null distribution is dominated by the uniform distribution in a different stochastic order, called the convex order. The property leads us to discover some new finite-sample and asymptotic bounds on functions of mid-p-values, which can be used to combine results from different hypothesis tests conservatively, yet more powerfully, using mid-p-values rather than p-values. Our methodology is demonstrated on real data from a cyber-security application.","157":"The development of coherent missing data models to account for nonmonotone missing at random (MAR) data by inverse probability weighting (IPW) remains to date largely unresolved. As a consequence, IPW has essentially been restricted for use only in monotone missing data settings. We propose a class of models for nonmonotone missing data mechanisms that spans the MAR model, while allowing the underlying full data law to remain unrestricted. For parametric specifications within the proposed class, we introduce an unconstrained maximum likelihood estimator for estimating the missing data probabilities which can be easily implemented using existing software. To circumvent potential convergence issues with this procedure, we also introduce a Bayesian constrained approach to estimate the missing data process which is guaranteed to yield inferences that respect all model restrictions. The efficiency of the standard IPW estimator is improved by incorporating information from incomplete cases through an augmented estimating equation which is optimal within a large class of estimating equations. We investigate the finite-sample properties of the proposed estimators in a simulation study and illustrate the new methodology in an application evaluating key correlates of preterm delivery for infants born to HIV infected mothers in Botswana, Africa.","158":"Error variance estimation plays an important role in statistical inference for high dimensional regression models. This paper concerns with error variance estimation in high dimensional sparse additive model. We study the asymptotic behavior of the traditional mean squared errors, the naive estimate of error variance, and show that it may significantly underestimate the error variance due to spurious correlations which are even higher in nonparametric models than linear models. We further propose an accurate estimate for error variance in ultrahigh dimensional sparse additive model by effectively integrating sure independence screening and refitted cross-validation techniques (Fan, Guo and Hao, 2012). The root n consistency and the asymptotic normality of the resulting estimate are established. We conduct Monte Carlo simulation study to examine the finite sample performance of the newly proposed estimate. A real data example is used to illustrate the proposed methodology.","159":"Individualized medical decision making is often complex due to patient treatment response heterogeneity. Pharmacotherapy may exhibit distinct efficacy and safety profiles for different patient populations. An \"optimal\" treatment that maximizes clinical benefit for a patient may also lead to concern of safety due to a high risk of adverse events. Thus, to guide individualized clinical decision making and deliver optimal tailored treatments, maximizing clinical benefit should be considered in the context of controlling for potential risk. In this work, we propose two approaches to identify personalized optimal treatment strategy that maximizes clinical benefit under a constraint on the average risk. We derive the theoretical optimal treatment rule under the risk constraint and draw an analogy to the Neyman-Pearson lemma to prove the theorem. We present algorithms that can be easily implemented by any off-the-shelf quadratic programming package. We conduct extensive simulation studies to show satisfactory risk control when maximizing the clinical benefit. Lastly, we apply our method to a randomized trial of type 2 diabetes patients to guide optimal utilization of the first line insulin treatments based on individual patient characteristics while controlling for the rate of hypoglycemia events. We identify baseline glycated hemoglobin level, body mass index, and fasting blood glucose as three key factors among 18 biomarkers to differentiate treatment assignments, and demonstrate a successful control of the risk of hypoglycemia in both the training and testing data set.","160":"A natural Bayesian approach for mixture models with an unknown number of components is to take the usual finite mixture model with symmetric Dirichlet weights, and put a prior on the number of components-that is, to use a mixture of finite mixtures (MFM). The most commonly-used method of inference for MFMs is reversible jump Markov chain Monte Carlo, but it can be nontrivial to design good reversible jump moves, especially in high-dimensional spaces. Meanwhile, there are samplers for Dirichlet process mixture (DPM) models that are relatively simple and are easily adapted to new applications. It turns out that, in fact, many of the essential properties of DPMs are also exhibited by MFMs-an exchangeable partition distribution, restaurant process, random measure representation, and stick-breaking representation-and crucially, the MFM analogues are simple enough that they can be used much like the corresponding DPM properties. Consequently, many of the powerful methods developed for inference in DPMs can be directly applied to MFMs as well; this simplifies the implementation of MFMs and can substantially improve mixing. We illustrate with real and simulated data, including high-dimensional gene expression data used to discriminate cancer subtypes.","161":"Factor modeling is an essential tool for exploring intrinsic dependence structures among high-dimensional random variables. Much progress has been made for estimating the covariance matrix from a high-dimensional factor model. However, the blessing of dimensionality has not yet been fully embraced in the literature: much of the available data are often ignored in constructing covariance matrix estimates. If our goal is to accurately estimate a covariance matrix of a set of targeted variables, shall we employ additional data, which are beyond the variables of interest, in the estimation? In this article, we provide sufficient conditions for an affirmative answer, and further quantify its gain in terms of Fisher information and convergence rate. In fact, even an oracle-like result (as if all the factors were known) can be achieved when a sufficiently large number of variables is used. The idea of using data as much as possible brings computational challenges. A divide-and-conquer algorithm is thus proposed to alleviate the computational burden, and also shown not to sacrifice any statistical accuracy in comparison with a pooled analysis. Simulation studies further confirm our advocacy for the use of full data, and demonstrate the effectiveness of the above algorithm. Our proposal is applied to a microarray data example that shows empirical benefits of using more data. Supplementary materials for this article are available online.","162":"Making accurate inference for gene regulatory networks, including inferring about pathway by pathway interactions, is an important and difficult task. Motivated by such genomic applications, we consider multiple testing for conditional dependence between subgroups of variables. Under a Gaussian graphical model framework, the problem is translated into simultaneous testing for a collection of submatrices of a high-dimensional precision matrix with each submatrix summarizing the dependence structure between two subgroups of variables. A novel multiple testing procedure is proposed and both theoretical and numerical properties of the procedure are investigated. Asymptotic null distribution of the test statistic for an individual hypothesis is established and the proposed multiple testing procedure is shown to asymptotically control the false discovery rate (FDR) and false discovery proportion (FDP) at the pre-specified level under regularity conditions. Simulations show that the procedure works well in controlling the FDR and has good power in detecting the true interactions. The procedure is applied to a breast cancer gene expression study to identify between pathway interactions.","163":"In modern epidemiological and clinical studies, the covariates of interest may involve genome sequencing, biomarker assay, or medical imaging and thus are prohibitively expensive to measure on a large number of subjects. A cost-effective solution is the two-phase design, under which the outcome and inexpensive covariates are observed for all subjects during the first phase and that information is used to select subjects for measurements of expensive covariates during the second phase. For example, subjects with extreme values of quantitative traits were selected for whole-exome sequencing in the National Heart, Lung, and Blood Institute (NHLBI) Exome Sequencing Project (ESP). Herein, we consider general two-phase designs, where the outcome can be continuous or discrete, and inexpensive covariates can be continuous and correlated with expensive covariates. We propose a semiparametric approach to regression analysis by approximating the conditional density functions of expensive covariates given inexpensive covariates with B-spline sieves. We devise a computationally efficient and numerically stable EM-algorithm to maximize the sieve likelihood. In addition, we establish the consistency, asymptotic normality, and asymptotic efficiency of the estimators. Furthermore, we demonstrate the superiority of the proposed methods over existing ones through extensive simulation studies. Finally, we present applications to the aforementioned NHLBI ESP.","164":"","165":"We introduce the Hamming ball sampler, a novel Markov chain Monte Carlo algorithm, for efficient inference in statistical models involving high-dimensional discrete state spaces. The sampling scheme uses an auxiliary variable construction that adaptively truncates the model space allowing iterative exploration of the full model space. The approach generalizes conventional Gibbs sampling schemes for discrete spaces and provides an intuitive means for user-controlled balance between statistical efficiency and computational tractability. We illustrate the generic utility of our sampling algorithm through application to a range of statistical models. Supplementary materials for this article are available online.","166":"Human microbiome studies use sequencing technologies to measure the abundance of bacterial species or Operational Taxonomic Units (OTUs) in samples of biological material. Typically the data are organized in contingency tables with OTU counts across heterogeneous biological samples. In the microbial ecology community, ordination methods are frequently used to investigate latent factors or clusters that capture and describe variations of OTU counts across biological samples. It remains important to evaluate how uncertainty in estimates of each biological sample's microbial distribution propagates to ordination analyses, including visualization of clusters and projections of biological samples on low dimensional spaces. We propose a Bayesian analysis for dependent distributions to endow frequently used ordinations with estimates of uncertainty. A Bayesian nonparametric prior for dependent normalized random measures is constructed, which is marginally equivalent to the normalized generalized Gamma process, a well-known prior for nonparametric analyses. In our prior, the dependence and similarity between microbial distributions is represented by latent factors that concentrate in a low dimensional space. We use a shrinkage prior to tune the dimensionality of the latent factors. The resulting posterior samples of model parameters can be used to evaluate uncertainty in analyses routinely applied in microbiome studies. Specifically, by combining them with multivariate data analysis techniques we can visualize credible regions in ecological ordination plots. The characteristics of the proposed model are illustrated through a simulation study and applications in two microbiome datasets.","167":"This article considers the problem of analyzing associations between power spectra of multiple time series and cross-sectional outcomes when data are observed from multiple subjects. The motivating application comes from sleep medicine, where researchers are able to non-invasively record physiological time series signals during sleep. The frequency patterns of these signals, which can be quantified through the power spectrum, contain interpretable information about biological processes. An important problem in sleep research is drawing connections between power spectra of time series signals and clinical characteristics; these connections are key to understanding biological pathways through which sleep affects, and can be treated to improve, health. Such analyses are challenging as they must overcome the complicated structure of a power spectrum from multiple time series as a complex positive-definite matrix-valued function. This article proposes a new approach to such analyses based on a tensor-product spline model of Cholesky components of outcome-dependent power spectra. The approach exibly models power spectra as nonparametric functions of frequency and outcome while preserving geometric constraints. Formulated in a fully Bayesian framework, a Whittle likelihood based Markov chain Monte Carlo (MCMC) algorithm is developed for automated model fitting and for conducting inference on associations between outcomes and spectral measures. The method is used to analyze data from a study of sleep in older adults and uncovers new insights into how stress and arousal are connected to the amount of time one spends in bed.","168":"Dynamic regression models, including the quantile regression model and Aalen's additive hazards model, are widely adopted to investigate evolving covariate effects. Yet lack of monotonicity respecting with standard estimation procedures remains an outstanding issue. Advances have recently been made, but none provides a complete resolution. In this article, we propose a novel adaptive interpolation method to restore monotonicity respecting, by successively identifying and then interpolating nearest monotonicity-respecting points of an original estimator. Under mild regularity conditions, the resulting regression coefficient estimator is shown to be asymptotically equivalent to the original. Our numerical studies have demonstrated that the proposed estimator is much more smooth and may have better finite-sample efficiency than the original as well as, when available as only in special cases, other competing monotonicity-respecting estimators. Illustration with a clinical study is provided.","169":"Prediction precision is arguably the most relevant criterion of a model in practice and is often a sought after property. A common difficulty with covariates measured with errors is the impossibility of performing prediction evaluation on the data even if a model is completely given without any unknown parameters. We bypass this inherent difficulty by using special properties on moment relations in linear regression models with measurement errors. The end product is a model selection procedure that achieves the same optimality properties that are achieved in classical linear regression models without covariate measurement error. Asymptotically, the procedure selects the model with the minimum prediction error in general, and selects the smallest correct model if the regression relation is indeed linear. Our model selection procedure is useful in prediction when future covariates without measurement error become available, e.g., due to improved technology or better management and design of data collection procedures.","170":"Motivated by analyses of DNA methylation data, we propose a semiparametric mixture model, namely the generalized exponential tilt mixture model, to account for heterogeneity between differentially methylated and non-differentially methylated subjects in the cancer group, and capture the differences in higher order moments (e.g. mean and variance) between subjects in cancer and normal groups. A pairwise pseudolikelihood is constructed to eliminate the unknown nuisance function. To circumvent boundary and non-identifiability problems as in parametric mixture models, we modify the pseudolikelihood by adding a penalty function. In addition, the test with simple asymptotic distribution has computational advantages compared with permutation-based test for high-dimensional genetic or epigenetic data. We propose a pseudolikelihood based expectation-maximization test, and show the proposed test follows a simple chi-squared limiting distribution. Simulation studies show that the proposed test controls Type I errors well and has better power compared to several current tests. In particular, the proposed test outperforms the commonly used tests under all simulation settings considered, especially when there are variance differences between two groups. The proposed test is applied to a real data set to identify differentially methylated sites between ovarian cancer subjects and normal subjects.","171":"Genome-wide association studies (GWAS) and differential expression analyses have had limited success in finding genes that cause complex diseases such as heart failure (HF), a leading cause of death in the United States. This paper proposes a new statistical approach that integrates GWAS and expression quantitative trait loci (eQTL) data to identify important HF genes. For such genes, genetic variations that perturb its expression are also likely to influence disease risk. The proposed method thus tests for the presence of simultaneous signals: SNPs that are associated with the gene's expression as well as with disease. An analytic expression for the p-value is obtained, and the method is shown to be asymptotically adaptively optimal under certain conditions. It also allows the GWAS and eQTL data to be collected from different groups of subjects, enabling investigators to integrate public resources with their own data. Simulation experiments show that it can be more powerful than standard approaches and also robust to linkage disequilibrium between variants. The method is applied to an extensive analysis of HF genomics and identifies several genes with biological evidence for being functionally relevant in the etiology of HF. It is implemented in the R package ssa.","172":"","173":"There are many applications in which a statistic follows, at least asymptotically, a normal distribution with a singular or nearly singular variance matrix. A classic example occurs in linear regression models under multicollinearity but there are many more such examples. There is well-developed theory for testing linear equality constraints when the alternative is two-sided and the variance matrix is either singular or non-singular. In recent years there is considerable, and growing, interest in developing methods for situations in which the estimated variance matrix is nearly singular. However, there is no corresponding methodology for addressing one-sided, i.e., constrained or ordered alternatives. In this paper we develop a unified framework for analyzing such problems. Our approach may be viewed as the trimming or winsorizing of the eigenvalues of the corresponding variance matrix. The proposed methodology is applicable to a wide range of scientific problems and to a variety of statistical models in which inequality constraints arise. We illustrate the methodology using data from a gene expression microarray experiment obtained from the NIEHS' Fibroid Growth Study.","174":"Immunotherapy is an innovative treatment approach that stimulates a patient's immune system to fight cancer. It demonstrates characteristics distinct from conventional chemotherapy and stands to revolutionize cancer treatment. We propose a Bayesian phase I\/II dosefinding design that incorporates the unique features of immunotherapy by simultaneously considering three outcomes: immune response, toxicity and efficacy. The objective is to identify the biologically optimal dose, defined as the dose with the highest desirability in the risk-benefit tradeoff. An Emax model is utilized to describe the marginal distribution of the immune response. Conditional on the immune response, we jointly model toxicity and efficacy using a latent variable approach. Using the accumulating data, we adaptively randomize patients to experimental doses based on the continuously updated model estimates. A simulation study shows that our proposed design has good operating characteristics in terms of selecting the target dose and allocating patients to the target dose.","175":"In the fields of neuroimaging and genetics, a key goal is testing the association of a single outcome with a very high-dimensional imaging or genetic variable. Often, summary measures of the high-dimensional variable are created to sequentially test and localize the association with the outcome. In some cases, the associations between the outcome and summary measures are significant, but subsequent tests used to localize differences are underpowered and do not identify regions associated with the outcome. Here, we propose a generalization of Rao's score test based on projecting the score statistic onto a linear subspace of a high-dimensional parameter space. The approach provides a way to localize signal in the high-dimensional space by projecting the scores to the subspace where the score test was performed. This allows for inference in the high-dimensional space to be performed on the same degrees of freedom as the score test, effectively reducing the number of comparisons. Simulation results demonstrate the test has competitive power relative to others commonly used. We illustrate the method by analyzing a subset of the Alzheimer's Disease Neuroimaging Initiative dataset. Results suggest cortical thinning of the frontal and temporal lobes may be a useful biological marker of Alzheimer's disease risk.","176":"Several methods have been proposed for partially or point identifying the average treatment effect (ATE) using instrumental variable (IV) type assumptions. The descriptions of these methods are widespread across the statistical, economic, epidemiologic, and computer science literature, and the connections between the methods have not been readily apparent. In the setting of a binary instrument, treatment, and outcome, we review proposed methods for partial and point identification of the ATE under IV assumptions, express the identification results in a common notation and terminology, and propose a taxonomy that is based on sets of identifying assumptions. We further demonstrate and provide software for the application of these methods to estimate bounds. Supplementary materials for this article are available online.","177":"Motivated by data gathered in an oral health study, we propose a Bayesian nonparametric approach for population-averaged modeling of correlated time-to-event data, when the responses can only be determined to lie in an interval obtained from a sequence of examination times and the determination of the occurrence of the event is subject to misclassification. The joint model for the true, unobserved time-to-event data is defined semiparametrically; proportional hazards, proportional odds, and accelerated failure time (proportional quantiles) are all fit and compared. The baseline distribution is modeled as a flexible tailfree prior. The joint model is completed by considering a parametric copula function. A general misclassification model is discussed in detail, considering the possibility that different examiners were involved in the assessment of the occurrence of the events for a given subject across time. We provide empirical evidence that the model can be used to estimate the underlying time-to-event distribution and the misclassification parameters without any external information about the latter parameters. We also illustrate the effect on the statistical inferences of neglecting the presence of misclassification.","178":"The goal of this paper is to give confidence regions for the excursion set of a spatial function above a given threshold from repeated noisy observations on a fine grid of fixed locations. Given an asymptotically Gaussian estimator of the target function, a pair of data-dependent nested excursion sets are constructed that are sub- and super-sets of the true excursion set, respectively, with a desired confidence. Asymptotic coverage probabilities are determined via a multiplier bootstrap method, not requiring Gaussianity of the original data nor stationarity or smoothness of the limiting Gaussian field. The method is used to determine regions in North America where the mean summer and winter temperatures are expected to increase by mid 21st century by more than 2 degrees Celsius.","179":"Recent advances in high-throughput biotechnologies have provided an unprecedented opportunity for biomarker discovery, which, from a statistical point of view, can be cast as a variable selection problem. This problem is challenging due to the high-dimensional and non-linear nature of omics data and, in general, it suffers three difficulties: (i) an unknown functional form of the nonlinear system, (ii) variable selection consistency, and (iii) high-demanding computation. To circumvent the first difficulty, we employ a feed-forward neural network to approximate the unknown nonlinear function motivated by its universal approximation ability. To circumvent the second difficulty, we conduct structure selection for the neural network, which induces variable selection, by choosing appropriate prior distributions that lead to the consistency of variable selection. To circumvent the third difficulty, we implement the population stochastic approximation Monte Carlo algorithm, a parallel adaptive Markov Chain Monte Carlo (MCMC) algorithm, on the OpenMP platform which provides a linear speedup for the simulation with the number of cores of the computer. The numerical results indicate that the proposed method can work very well for identification of relevant variables for high-dimensional nonlinear systems. The proposed method is successfully applied to identification of the genes that are associated with anticancerdrug sensitivities based on the data collected in the cancer cell line encyclopedia (CCLE) study.","180":"The identification of reproducible signals from the results of replicate high-throughput experiments is an important part of modern biological research. Often little is known about the dependence structure and the marginal distribution of the data, motivating the development of a nonparametric approach to assess reproducibility. The procedure, which we call the maximum rank reproducibility (MaRR) procedure, uses a maximum rank statistic to parse reproducible signals from noise without making assumptions about the distribution of reproducible signals. Because it uses the rank scale this procedure can be easily applied to a variety of data types. One application is to assess the reproducibility of RNA-seq technology using data produced by the sequencing quality control (SEQC) consortium, which coordinated a multi-laboratory effort to assess reproducibility across three RNA-seq platforms. Our results on simulations and SEQC data show that the MaRR procedure effectively controls false discovery rates, has desirable power properties, and compares well to existing methods. Supplementary materials for this article are available online.","181":"In sleep research, applying finite mixture models to sleep characteristics captured 8 through multiple data types, including self-reported sleep diary, a wrist monitor capturing movement (actigraphy), and brain waves (polysomnography), may suggest new phenotypes that reflect underlying disease mechanisms. However, a direct mixture model application is challenging because there are many sleep variables from which to choose, and sleep variables are often highly skewed even in homogenous samples. Moreover, previous sleep research findings indicate that some of the most clinically interesting solutions will be those that incorporate all three data types. Thus, we present two novel skewed variable selection algorithms based on the multivariate skew normal (MSN) distribution: one that selects the best set of variables ignoring data type and another that embraces the exploratory nature of clustering and suggests multiple statistically plausible sets of variables that each incorporate all data types. Through a simulation study we empirically compare our approach with other asymmetric and normal dimension reduction strategies for clustering. Finally, we demonstrate our methods using a sample of older adults with and without insomnia. The proposed MSN-based variable selection algorithm appears to be suitable for both MSN and multivariate normal cluster distributions, especially with moderate to large sample sizes.","182":"The use of weights provides an effective strategy to incorporate prior domain knowledge in large-scale inference. This paper studies weighted multiple testing in a decision-theoretic framework. We develop oracle and data-driven procedures that aim to maximize the expected number of true positives subject to a constraint on the weighted false discovery rate. The asymptotic validity and optimality of the proposed methods are established. The results demonstrate that incorporating informative domain knowledge enhances the interpretability of results and precision of inference. Simulation studies show that the proposed method controls the error rate at the nominal level, and the gain in power over existing methods is substantial in many settings. An application to a genome-wide association study is discussed.","183":null,"184":"Many researchers in biology and medicine have focused on trying to understand biological rhythms and their potential impact on disease. A common biological rhythm is circadian, where the cycle repeats itself every 24 hours. However, a disturbance of the circadian pattern may be indicative of future disease. In this article, we develop new statistical methodology for assessing the degree of disturbance or irregularity in a circadian pattern for count sequences that are observed over time in a population of individuals. We develop a latent variable Poisson modeling approach with both circadian and stochastic short-term trend (autoregressive latent process) components that allow for individual variation in the degree of each component. A parameterization is proposed for modeling covariate dependence on the proportion of these two model components across individuals. In addition, we incorporate covariate dependence in the overall mean, the magnitude of the trend, and the phase-shift of the circadian pattern. Innovative Markov chain Monte Carlo sampling is used to carry out Bayesian posterior computation. Several variations of the proposed models are considered and compared using the deviance information criterion. We illustrate this methodology with longitudinal physical activity count data measured in a longitudinal cohort of adolescents.","185":null,"186":"Bayesian variable selection often assumes normality, but the effects of model misspecification are not sufficiently understood. There are sound reasons behind this assumption, particularly for large p: ease of interpretation, analytical and computational convenience. More flexible frameworks exist, including semi- or non-parametric models, often at the cost of some tractability. We propose a simple extension that allows for skewness and thicker-than-normal tails but preserves tractability. It leads to easy interpretation and a log-concave likelihood that facilitates optimization and integration. We characterize asymptotically parameter estimation and Bayes factor rates, under certain model misspecification. Under suitable conditions misspecified Bayes factors induce sparsity at the same rates than under the correct model. However, the rates to detect signal change by an exponential factor, often reducing sensitivity. These deficiencies can be ameliorated by inferring the error distribution, a simple strategy that can improve inference substantially. Our work focuses on the likelihood and can be combined with any likelihood penalty or prior, but here we focus on non-local priors to induce extra sparsity and ameliorate finite-sample effects caused by misspecification. We show the importance of considering the likelihood rather than solely the prior, for Bayesian variable selection. The methodology is in R package 'mombf'.","187":"Three dimensional confocal scanning laser microscope images offer dramatic visualizations of the action of living biofilms before and after interventions. Here we use confocal microscopy to study the effect of a treatment over time that causes a biofilm to swell and contract due to osmotic pressure changes. From these data, our goal is to reconstruct biofilm surfaces, to estimate the effect of the treatment on the biofilm's volume, and to quantify the related uncertainties. We formulate the associated massive linear Bayesian inverse problem and then solve it using iterative samplers from large multivariate Gaussians that exploit well-established polynomial acceleration techniques from numerical linear algebra. Because of a general equivalence with linear solvers, these polynomial accelerated iterative samplers have known convergence rates, stopping criteria, and perform well in finite precision. An explicit algorithm is provided, for the first time, for an iterative sampler that is accelerated by the synergistic implementation of preconditioned conjugate gradient and Chebyshev polynomials.","188":"We present a nonparametric framework to model a short sequence of probability distributions that vary both due to underlying effects of sequential progression and confounding noise. To distinguish between these two types of variation and estimate the sequential-progression effects, our approach leverages an assumption that these effects follow a persistent trend. This work is motivated by the recent rise of single-cell RNA-sequencing experiments over a brief time course, which aim to identify genes relevant to the progression of a particular biological process across diverse cell populations. While classical statistical tools focus on scalar-response regression or order-agnostic differences between distributions, it is desirable in this setting to consider both the full distributions as well as the structure imposed by their ordering. We introduce a new regression model for ordinal covariates where responses are univariate distributions and the underlying relationship reflects consistent changes in the distributions over increasing levels of the covariate. This concept is formalized as a trend in distributions, which we define as an evolution that is linear under the Wasserstein metric. Implemented via a fast alternating projections algorithm, our method exhibits numerous strengths in simulations and analyses of single-cell gene expression data.","189":null,"190":"The population-based case-control study design has been widely used for studying the etiology of chronic diseases. It is well established that the Cox proportional hazards model can be adapted to the case-control study and hazard ratios can be estimated by (conditional) logistic regression model with time as either a matched set or a covariate (Prentice and Breslow, 1978). However, the baseline hazard function, a critical component in absolute risk assessment, is unidentifiable, because the ratio of cases and controls is controlled by the investigators and does not reflect the true disease incidence rate in the population. In this paper we propose a simple and innovative approach, which makes use of routinely collected family history information, to estimate the baseline hazard function for any logistic regression model that is fit to the risk factor data collected on cases and controls. We establish that the proposed baseline hazard function estimator is consistent and asymptotically normal and show via simulation that it performs well in finite samples. We illustrate the proposed method by a population-based case-control study of prostate cancer where the association of various risk factors is assessed and the family history information is used to estimate the baseline hazard function.","191":null,"192":"We consider a random effects model for longitudinal data with the occurrence of an informative terminal event that is subject to right censoring. Existing methods for analyzing such data include the joint modeling approach using latent frailty and the marginal estimating equation approach using inverse probability weighting; in both cases the effect of the terminal event on the response variable is not explicit and thus not easily interpreted. In contrast, we treat the terminal event time as a covariate in a conditional model for the longitudinal data, which provides a straight-forward interpretation while keeping the usual relationship of interest between the longitudinally measured response variable and covariates for times that are far from the terminal event. A two-stage semiparametric likelihood-based approach is proposed for estimating the regression parameters; first, the conditional distribution of the right-censored terminal event time given other covariates is estimated and then the likelihood function for the longitudinal event given the terminal event and other regression parameters is maximized. The method is illustrated by numerical simulations and by analyzing medical cost data for patients with end-stage renal disease. Desirable asymptotic properties are provided.","193":"Dynamic functional connectivity, i.e., the study of how interactions among brain regions change dynamically over the course of an fMRI experiment, has recently received wide interest in the neuroimaging literature. Current approaches for studying dynamic connectivity often rely on ad-hoc approaches for inference, with the fMRI time courses segmented by a sequence of sliding windows. We propose a principled Bayesian approach to dynamic functional connectivity, which is based on the estimation of time varying networks. Our method utilizes a hidden Markov model for classification of latent cognitive states, achieving estimation of the networks in an integrated framework that borrows strength over the entire time course of the experiment. Furthermore, we assume that the graph structures, which define the connectivity states at each time point, are related within a super-graph, to encourage the selection of the same edges among related graphs. We apply our method to simulated task-based fMRI data, where we show how our approach allows the decoupling of the task-related activations and the functional connectivity states. We also analyze data from an fMRI sensorimotor task experiment on an individual healthy subject and obtain results that support the role of particular anatomical regions in modulating interaction between executive control and attention networks.","194":"Estimating the size of stigmatized, hidden, or hard-to-reach populations is a major problem in epidemiology, demography, and public health research. Capture-recapture and multiplier methods are standard tools for inference of hidden population sizes, but they require random sampling of target population members, which is rarely possible. Respondent-driven sampling (RDS) is a survey method for hidden populations that relies on social link tracing. The RDS recruitment process is designed to spread through the social network connecting members of the target population. In this paper, we show how to use network data revealed by RDS to estimate hidden population size. The key insight is that the recruitment chain, timing of recruitments, and network degrees of recruited subjects provide information about the number of individuals belonging to the target population who are not yet in the sample. We use a computationally efficient Bayesian method to integrate over the missing edges in the subgraph of recruited individuals. We validate the method using simulated data and apply the technique to estimate the number of people who inject drugs in St. Petersburg, Russia.","195":"We propose a random partition distribution indexed by pairwise similarity information such that partitions compatible with the similarities are given more probability. The use of pairwise similarities, in the form of distances, is common in some clustering algorithms (e.g., hierarchical clustering), but we show how to use this type of information to define a prior partition distribution for flexible Bayesian modeling. A defining feature of the distribution is that it allocates probability among partitions within a given number of subsets, but it does not shift probability among sets of partitions with different numbers of subsets. Our distribution places more probability on partitions that group similar items yet keeps the total probability of partitions with a given number of subsets constant. The distribution of the number of subsets (and its moments) is available in closed-form and is not a function of the similarities. Our formulation has an explicit probability mass function (with a tractable normalizing constant) so the full suite of MCMC methods may be used for posterior inference. We compare our distribution with several existing partition distributions, showing that our formulation has attractive properties. We provide three demonstrations to highlight the features and relative performance of our distribution.","196":"We propose an extrinsic regression framework for modeling data with manifold valued responses and Euclidean predictors. Regression with manifold responses has wide applications in shape analysis, neuroscience, medical imaging and many other areas. Our approach embeds the manifold where the responses lie onto a higher dimensional Euclidean space, obtains a local regression estimate in that space, and then projects this estimate back onto the image of the manifold. Outside the regression setting both intrinsic and extrinsic approaches have been proposed for modeling i.i.d manifold-valued data. However, to our knowledge our work is the first to take an extrinsic approach to the regression problem. The proposed extrinsic regression framework is general, computationally efficient and theoretically appealing. Asymptotic distributions and convergence rates of the extrinsic regression estimates are derived and a large class of examples are considered indicating the wide applicability of our approach.","197":"Motivated by the analysis of imaging data, we propose a novel functional varying-coefficient single index model (FVCSIM) to carry out the regression analysis of functional response data on a set of covariates of interest. FVCSIM represents a new extension of varying-coefficient single index models for scalar responses collected from cross-sectional and longitudinal studies. An efficient estimation procedure is developed to iteratively estimate varying coefficient functions, link functions, index parameter vectors, and the covariance function of individual functions. We systematically examine the asymptotic properties of all estimators including the weak convergence of the estimated varying coefficient functions, the asymptotic distribution of the estimated index parameter vectors, and the uniform convergence rate of the estimated covariance function and their spectrum. Simulation studies are carried out to assess the finite-sample performance of the proposed procedure. We apply FVCSIM to investigating the development of white matter diffusivities along the corpus callosum skeleton obtained from Alzheimer's Disease Neuroimaging Initiative (ADNI) study.","198":"The use of imaging markers to predict clinical outcomes can have a great impact in public health. The aim of this paper is to develop a class of generalized scalar-on-image regression models via total variation (GSIRM-TV), in the sense of generalized linear models, for scalar response and imaging predictor with the presence of scalar covariates. A key novelty of GSIRM-TV is that it is assumed that the slope function (or image) of GSIRM-TV belongs to the space of bounded total variation in order to explicitly account for the piecewise smooth nature of most imaging data. We develop an efficient penalized total variation optimization to estimate the unknown slope function and other parameters. We also establish nonasymptotic error bounds on the excess risk. These bounds are explicitly specified in terms of sample size, image size, and image smoothness. Our simulations demonstrate a superior performance of GSIRM-TV against many existing approaches. We apply GSIRM-TV to the analysis of hippocampus data obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset.","199":"We propose a multiscale weighted principal component regression (MWPCR) framework for the use of high dimensional features with strong spatial features (e.g., smoothness and correlation) to predict an outcome variable, such as disease status. This development is motivated by identifying imaging biomarkers that could potentially aid detection, diagnosis, assessment of prognosis, prediction of response to treatment, and monitoring of disease status, among many others. The MWPCR can be regarded as a novel integration of principal components analysis (PCA), kernel methods, and regression models. In MWPCR, we introduce various weight matrices to prewhitten high dimensional feature vectors, perform matrix decomposition for both dimension reduction and feature extraction, and build a prediction model by using the extracted features. Examples of such weight matrices include an importance score weight matrix for the selection of individual features at each location and a spatial weight matrix for the incorporation of the spatial pattern of feature vectors. We integrate the importance score weights with the spatial weights in order to recover the low dimensional structure of high dimensional features. We demonstrate the utility of our methods through extensive simulations and real data analyses of the Alzheimer's disease neuroimaging initiative (ADNI) data set.","200":"In follow-up studies, utility marker measurements are usually collected upon the occurrence of recurrent events until a terminal event such as death takes place. In this article, we define the recurrent marker process to characterize utility accumulation over time. For example, with medical cost and repeated hospitalizations being treated as marker and recurrent events respectively, the recurrent marker process is the trajectory of cumulative cost, which stops to increase after death. In many applications, competing risks arise as subjects are at risk of more than one mutually exclusive terminal event, such as death from different causes, and modeling the recurrent marker process for each failure type is often of interest. However, censoring creates challenges in the methodological development, because for censored subjects, both failure type and recurrent marker process after censoring are unobserved. To circumvent this problem, we propose a nonparametric framework for recurrent marker process with competing terminal events. In the presence of competing risks, we start with an estimator by using marker information from uncensored subjects. As a result, the estimator can be inefficient under heavy censoring. To improve efficiency, we propose a second estimator by combining the first estimator with auxiliary information from the estimate under non-competing risks model. The large sample properties and optimality of the second estimator is established. Simulation studies and an application to the SEER-Medicare linked data are presented to illustrate the proposed methods. Supplemental materials are available online.","201":"Rates of kidney cancer have been increasing, with small incidental tumors experiencing the fastest growth rates. Much of the increase could be due to increased use of CT scans, MRIs, and ultrasounds for unrelated conditions. Many tumors might never have been detected or become symptomatic in the past. This suggests that many patients might benefit from less aggressive therapy, such as active surveillance by which tumors are surgically removed only if they become sufficiently large. However, it has been difficult for clinicians to identify subgroups of patients for whom treatment might be especially beneficial or harmful. In this work, we use a principal stratification framework to estimate the proportion and characteristics of individuals who have large or small hazard rates of death in two treatment arms. This allows us to assess who might be helped or harmed by aggressive treatment. We also use Weibull mixture models. This work differs from much previous work in that the survival classes upon which principal stratification is based are latent variables. That is, survival class is not an observed variable. We apply this work using Surveillance Epidemiology and End Results-Medicare claims data. Clinicians can use our methods for investigating treatments with heterogeneous effects.","202":"Recurrent event data arise frequently in various fields such as biomedical sciences, public health, engineering, and social sciences. In many instances, the observation of the recurrent event process can be stopped by the occurrence of a correlated failure event, such as treatment failure and death. In this article, we propose a joint scale-change model for the recurrent event process and the failure time, where a shared frailty variable is used to model the association between the two types of outcomes. In contrast to the popular Cox-type joint modeling approaches, the regression parameters in the proposed joint scale-change model have marginal interpretations. The proposed approach is robust in the sense that no parametric assumption is imposed on the distribution of the unobserved frailty and that we do not need the strong Poisson-type assumption for the recurrent event process. We establish consistency and asymptotic normality of the proposed semiparametric estimators under suitable regularity conditions. To estimate the corresponding variances of the estimators, we develop a computationally efficient resampling-based procedure. Simulation studies and an analysis of hospitalization data from the Danish Psychiatric Central Register illustrate the performance of the proposed method.","203":"In quantile linear regression with ultra-high dimensional data, we propose an algorithm for screening all candidate variables and subsequently selecting relevant predictors. Specifically, we first employ quantile partial correlation for screening, and then we apply the extended Bayesian information criterion (EBIC) for best subset selection. Our proposed method can successfully select predictors when the variables are highly correlated, and it can also identify variables that make a contribution to the conditional quantiles but are marginally uncorrelated or weakly correlated with the response. Theoretical results show that the proposed algorithm can yield the sure screening set. By controlling the false selection rate, model selection consistency can be achieved theoretically. In practice, we proposed using EBIC for best subset selection so that the resulting model is screening consistent. Simulation studies demonstrate that the proposed algorithm performs well, and an empirical example is presented.","204":"Personalized medicine has received increasing attention among statisticians, computer scientists, and clinical practitioners. A major component of personalized medicine is the estimation of individualized treatment rules (ITRs). Recently, Zhao et al. (2012) proposed outcome weighted learning (OWL) to construct ITRs that directly optimize the clinical outcome. Although OWL opens the door to introducing machine learning techniques to optimal treatment regimes, it still has some problems in performance. (1) The estimated ITR of OWL is affected by a simple shift of the outcome. (2) The rule from OWL tries to keep treatment assignments that subjects actually received. (3) There is no variable selection mechanism with OWL. All of them weaken the finite sample performance of OWL. In this article, we propose a general framework, called Residual Weighted Learning (RWL), to alleviate these problems, and hence to improve finite sample performance. Unlike OWL which weights misclassification errors by clinical outcomes, RWL weights these errors by residuals of the outcome from a regression fit on clinical covariates excluding treatment assignment. We utilize the smoothed ramp loss function in RWL, and provide a difference of convex (d.c.) algorithm to solve the corresponding non-convex optimization problem. By estimating residuals with linear models or generalized linear models, RWL can effectively deal with different types of outcomes, such as continuous, binary and count outcomes. We also propose variable selection methods for linear and nonlinear rules, respectively, to further improve the performance. We show that the resulting estimator of the treatment rule is consistent. We further obtain a rate of convergence for the difference between the expected outcome using the estimated ITR and that of the optimal treatment rule. The performance of the proposed RWL methods is illustrated in simulation studies and in an analysis of cystic fibrosis clinical trial data.","205":"A design is presented for a randomized clinical trial comparing two second-line treatments, chemotherapy versus chemotherapy plus reirradiation, for treatment of recurrent non-small-cell lung cancer. The central research question is whether the potential efficacy benefit that adding reirradiation to chemotherapy may provide justifies its potential for increasing the risk of toxicity. The design uses two co-primary outcomes: time to disease progression or death, and time to severe toxicity. Because patients may be given an active third-line treatment at disease progression that confounds second-line treatment effects on toxicity and survival following disease progression, for the purpose of this comparative study follow-up ends at disease progression or death. In contrast, follow-up for disease progression or death continues after severe toxicity, so these are semi-competing risks. A conditionally conjugate Bayesian model that is robust to misspecification is formulated using piecewise exponential distributions. A numerical utility function is elicited from the physicians that characterizes desirabilities of the possible co-primary outcome realizations. A comparative test based on posterior mean utilities is proposed. A simulation study is presented to evaluate test performance for a variety of treatment differences, and a sensitivity assessment to the elicited utility function is performed. General guidelines are given for constructing a design in similar settings, and a computer program for simulation and trial conduct is provided.","206":"","207":"A dynamic treatment regime is a sequence of decision rules, each of which recommends treatment based on features of patient medical history such as past treatments and outcomes. Existing methods for estimating optimal dynamic treatment regimes from data optimize the mean of a response variable. However, the mean may not always be the most appropriate summary of performance. We derive estimators of decision rules for optimizing probabilities and quantiles computed with respect to the response distribution for two-stage, binary treatment settings. This enables estimation of dynamic treatment regimes that optimize the cumulative distribution function of the response at a prespecified point or a prespecified quantile of the response distribution such as the median. The proposed methods perform favorably in simulation experiments. We illustrate our approach with data from a sequentially randomized trial where the primary outcome is remission of depression symptoms.","208":"We propose a systematic method for testing and identifying a subgroup with an enhanced treatment effect. We adopts a change-plane technique to first test the existence of a subgroup, and then identify the subgroup if the null hypothesis on non-existence of such a subgroup is rejected. A semiparametric model is considered for the response with an unspecified baseline function and an interaction between a subgroup indicator and treatment. A doubly-robust test statistic is constructed based on this model, and asymptotic distributions of the test statistic under both null and local alternative hypotheses are derived. Moreover, a sample size calculation method for subgroup detection is developed based on the proposed statistic. The finite sample performance of the proposed test is evaluated via simulations. Finally, the proposed methods for subgroup identification and sample size calculation are applied to a data from an AIDS study.","209":"It is of substantial interest to study the effects of genes, genetic pathways, and networks on the risk of complex diseases. These genetic constructs each contain multiple SNPs, which are often correlated and function jointly, and might be large in number. However, only a sparse subset of SNPs in a genetic construct is generally associated with the disease of interest. In this article, we propose the generalized higher criticism (GHC) to test for the association between an SNP set and a disease outcome. The higher criticism is a test traditionally used in high-dimensional signal detection settings when marginal test statistics are independent and the number of parameters is very large. However, these assumptions do not always hold in genetic association studies, due to linkage disequilibrium among SNPs and the finite number of SNPs in an SNP set in each genetic construct. The proposed GHC overcomes the limitations of the higher criticism by allowing for arbitrary correlation structures among the SNPs in an SNP-set, while performing accurate analytic p-value calculations for any finite number of SNPs in the SNP-set. We obtain the detection boundary of the GHC test. We compared empirically using simulations the power of the GHC method with existing SNP-set tests over a range of genetic regions with varied correlation structures and signal sparsity. We apply the proposed methods to analyze the CGEM breast cancer genome-wide association study. Supplementary materials for this article are available online.","210":"Recurrent event processes with marker measurements are mostly and largely studied with forward time models starting from an initial event. Interestingly, the processes could exhibit important terminal behavior during a time period before occurrence of the failure event. A natural and direct way to study recurrent events prior to a failure event is to align the processes using the failure event as the time origin and to examine the terminal behavior by a backward time model. This paper studies regression models for backward recurrent marker processes by counting time backward from the failure event. A three-level semiparametric regression model is proposed for jointly modeling the time to a failure event, the backward recurrent event process, and the marker observed at the time of each backward recurrent event. The first level is a proportional hazards model for the failure time, the second level is a proportional rate model for the recurrent events occurring before the failure event, and the third level is a proportional mean model for the marker given the occurrence of a recurrent event backward in time. By jointly modeling the three components, estimating equations can be constructed for marked counting processes to estimate the target parameters in the three-level regression models. Large sample properties of the proposed estimators are studied and established. The proposed models and methods are illustrated by a community-based AIDS clinical trial to examine the terminal behavior of frequencies and severities of opportunistic infections among HIV infected individuals in the last six months of life.","211":"We showcase a novel analytic strategy to identify sub-types of cancer that possess distinctive causal factors, i.e. sub-types that are \"etiologically\" distinct. The method involves the integrated analysis of two types of study design: an incident series of cases with double primary cancers with detailed information on tumor characteristics that can be used to define the sub-types; a case-series of incident cases with information on known risk factors that can be used to investigate the specific risk factors that distinguish the sub-types. The methods are applied to a rich melanoma dataset with detailed information on pathologic tumor factors, and comprehensive information on known genetic and environmental risk factors for melanoma. Identification of the optimal sub-typing solution is accomplished using a novel clustering analysis that seeks to maximize a measure that characterizes the distinctiveness of the distributions of risk factors across the sub-types and that is a function of the correlations of tumor factors in the case-specific tumor pairs. This analysis is challenged by the presence of extensive missing data. If successful, studies of this nature offer the opportunity for efficient study design to identify unknown risk factors whose effects are concentrated in defined sub-types.","212":"Graphical models are a popular approach to find dependence and conditional independence relationships between gene expressions. Directed acyclic graphs (DAGs) are a special class of directed graphical models, where all the edges are directed edges and contain no directed cycles. The DAGs are well known models for discovering causal relationships between genes in gene regulatory networks. However, estimating DAGs without assuming known ordering is challenging due to high dimensionality, the acyclic constraints, and the presence of equivalence class from observational data. To overcome these challenges, we propose a two-stage adaptive Lasso approach, called NS-DIST, which performs neighborhood selection (NS) in stage 1, and then estimates DAGs by the Discrete Improving Search with Tabu (DIST) algorithm within the selected neighborhood. Simulation studies are presented to demonstrate the effectiveness of the method and its computational efficiency. Two real data examples are used to demonstrate the practical usage of our method for gene regulatory network inference.","213":"Stroke is a disturbance in blood supply to the brain resulting in the loss of brain functions, particularly motor function. A study was conducted by the UCI Neurorehabilitation Lab to investigate the impact of stroke on motor-related brain regions. Functional MRI (fMRI) data were collected from stroke patients and healthy controls while the subjects performed a simple motor task. In addition to affecting local neuronal activation strength, stroke might also alter communications (i.e., connectivity) between brain regions. We develop a hierarchical Bayesian modeling approach for the analysis of multi-subject fMRI data that allows us to explore brain changes due to stroke. Our approach simultaneously estimates activation and condition-specific connectivity at the group level, and provides estimates for region\/subject-specific hemodynamic response functions. Moreover, our model uses spike and slab priors to allow for direct posterior inference on the connectivity network. Our results indicate that motor-control regions show greater activation in the unaffected hemisphere and the midline surface in stroke patients than those same regions in healthy controls during the simple motor task. We also note increased connectivity within secondary motor regions in stroke subjects. These findings provide insight into altered neural correlates of movement in subjects who suffered a stroke.","214":"This paper is concerned with the problem of feature screening for multi-class linear discriminant analysis under ultrahigh dimensional setting. We allow the number of classes to be relatively large. As a result, the total number of relevant features is larger than usual. This makes the related classification problem much more challenging than the conventional one, where the number of classes is small (very often two). To solve the problem, we propose a novel pairwise sure independence screening method for linear discriminant analysis with an ultrahigh dimensional predictor. The proposed procedure is directly applicable to the situation with many classes. We further prove that the proposed method is screening consistent. Simulation studies are conducted to assess the finite sample performance of the new procedure. We also demonstrate the proposed methodology via an empirical analysis of a real life example on handwritten Chinese character recognition.","215":"In this manuscript, we are concerned with data generated from a diffusion tensor imaging (DTI) experiment. The goal is to parameterize manifold-like white matter tracts, such as the corpus callosum, using principal surfaces. The problem is approached by finding a geometrically motivated surface-based representation of the corpus callosum and visualized fractional anisotropy (FA) values projected onto the surface. The method also applies to any other diffusion summary. An algorithm is proposed that 1) constructs the principal surface of a corpus callosum; 2) flattens the surface into a parametric 2D map; 3) projects associated FA values on the map. The algorithm is applied to a longitudinal study containing 466 diffusion tensor images of 176 multiple sclerosis (MS) patients observed at multiple visits. For each subject and visit the study contains a registered DTI scan of the corpus callosum at roughly 20,000 voxels. Extensive simulation studies demonstrate fast convergence and robust performance of the algorithm under a variety of challenging scenarios.","216":"We introduce a new sparse estimator of the covariance matrix for high-dimensional models in which the variables have a known ordering. Our estimator, which is the solution to a convex optimization problem, is equivalently expressed as an estimator which tapers the sample covariance matrix by a Toeplitz, sparsely-banded, data-adaptive matrix. As a result of this adaptivity, the convex banding estimator enjoys theoretical optimality properties not attained by previous banding or tapered estimators. In particular, our convex banding estimator is minimax rate adaptive in Frobenius and operator norms, up to log factors, over commonly-studied classes of covariance matrices, and over more general classes. Furthermore, it correctly recovers the bandwidth when the true covariance is exactly banded. Our convex formulation admits a simple and efficient algorithm. Empirical studies demonstrate its practical effectiveness and illustrate that our exactly-banded estimator works well even when the true covariance matrix is only close to a banded matrix, confirming our theoretical results. Our method compares favorably with all existing methods, in terms of accuracy and speed. We illustrate the practical merits of the convex banding estimator by showing that it can be used to improve the performance of discriminant analysis for classifying sound recordings.","217":"Matrix completion has attracted significant recent attention in many fields including statistics, applied mathematics and electrical engineering. Current literature on matrix completion focuses primarily on independent sampling models under which the individual observed entries are sampled independently. Motivated by applications in genomic data integration, we propose a new framework of structured matrix completion (SMC) to treat structured missingness by design. Specifically, our proposed method aims at efficient matrix recovery when a subset of the rows and columns of an approximately low-rank matrix are observed. We provide theoretical justification for the proposed SMC method and derive lower bound for the estimation errors, which together establish the optimal rate of recovery over certain classes of approximately low-rank matrices. Simulation studies show that the method performs well in finite sample under a variety of configurations. The method is applied to integrate several ovarian cancer genomic studies with different extent of genomic measurements, which enables us to construct more accurate prediction rules for ovarian cancer survival.","218":"This paper considers linear regression with missing covariates and a right censored outcome. We first consider a general two-phase outcome sampling design, where full covariate information is only ascertained for subjects in phase two and sampling occurs under an independent Bernoulli sampling scheme with known subject-specific sampling probabilities that depend on phase one information (e.g., survival time, failure status and covariates). The semiparametric information bound is derived for estimating the regression parameter in this setting. We also introduce a more practical class of augmented estimators that is shown to improve asymptotic efficiency over simple but inefficient inverse probability of sampling weighted estimators. Estimation for known sampling weights and extensions to the case of estimated sampling weights are both considered. The allowance for estimated sampling weights permits covariates to be missing at random according to a monotone but unknown mechanism. The asymptotic properties of the augmented estimators are derived and simulation results demonstrate substantial efficiency improvements over simpler inverse probability of sampling weighted estimators in the indicated settings. With suitable modification, the proposed methodology can also be used to improve augmented estimators previously used for missing covariates in a Cox regression model.","219":"The optimal dose for treating patients with a molecularly targeted agent may differ according to the patient's individual characteristics, such as biomarker status. In this article, we propose a Bayesian phase I\/II dose-finding design to find the optimal dose that is personalized for each patient according to his\/her biomarker status. To overcome the curse of dimensionality caused by the relatively large number of biomarkers and their interactions with the dose, we employ canonical partial least squares (CPLS) to extract a small number of components from the covariate matrix containing the dose, biomarkers, and dose-by-biomarker interactions. Using these components as the covariates, we model the ordinal toxicity and efficacy using the latent-variable approach. Our model accounts for important features of molecularly targeted agents. We quantify the desirability of the dose using a utility function and propose a two-stage dose-finding algorithm to find the personalized optimal dose according to each patient's individual biomarker profile. Simulation studies show that our proposed design has good operating characteristics, with a high probability of identifying the personalized optimal dose.","220":"Since the early 2000s, evidence has accumulated for a significant differential effect of first-line antiretroviral therapy (ART) regimens on human immunodeficiency virus (HIV) viral load suppression. This finding was replicated in our data from the Harvard President's Emergency Plan for AIDS Relief (PEPFAR) program in Nigeria. Investigators were interested in finding the source of these differences, i.e., understanding the mechanisms through which one regimen outperforms another, particularly via adherence. This question can be naturally formulated via mediation analysis with adherence playing the role of a mediator. Existing mediation analysis results, however, have relied on an assumption of no exposure-induced confounding of the intermediate variable, and generally require an assumption of no unmeasured confounding for nonparametric identification. Both assumptions are violated by the presence of drug toxicity. In this paper, we relax these assumptions and show that certain path-specific effects remain identified under weaker conditions. We focus on the path-specific effect solely mediated by adherence and not by toxicity and propose an estimator for this effect. We illustrate with simulations and present results from a study applying the methodology to the Harvard PEPFAR data. Supplementary materials are available online.","221":"","222":"Evidence-based personalized medicine formalizes treatment selection as an individualized treatment regime that maps up-to-date patient information into the space of possible treatments. Available patient information may include static features such race, gender, family history, genetic and genomic information, as well as longitudinal information including the emergence of comorbidities, waxing and waning of symptoms, side-effect burden, and adherence. Dynamic information measured at multiple time points before treatment assignment should be included as input to the treatment regime. However, subject longitudinal measurements are typically sparse, irregularly spaced, noisy, and vary in number across subjects. Existing estimators for treatment regimes require equal information be measured on each subject and thus standard practice is to summarize longitudinal subject information into a scalar, ad hoc summary during data pre-processing. This reduction of the longitudinal information to a scalar feature precedes estimation of a treatment regime and is therefore not informed by subject outcomes, treatments, or covariates. Furthermore, we show that this reduction requires more stringent causal assumptions for consistent estimation than are necessary. We propose a data-driven method for constructing maximally prescriptive yet interpretable features that can be used with standard methods for estimating optimal treatment regimes. In our proposed framework, we treat the subject longitudinal information as a realization of a stochastic process observed with error at discrete time points. Functionals of this latent process are then combined with outcome models to estimate an optimal treatment regime. The proposed methodology requires weaker causal assumptions than Q-learning with an ad hoc scalar summary and is consistent for the optimal treatment regime.","223":"For data with high-dimensional covariates but small sample sizes, the analysis of single datasets often generates unsatisfactory results. The integrative analysis of multiple independent datasets provides an effective way of pooling information and outperforms single-dataset and several alternative multi-datasets methods. Under many scenarios, multiple datasets are expected to share common important covariates, that is, the corresponding models have similarity in their sparsity structures. However, the existing methods do not have a mechanism to promote the similarity in sparsity structures in integrative analysis. In this study, we consider penalized variable selection and estimation in integrative analysis. We develop an L0-penalty based method, which explicitly promotes the similarity in sparsity structures. Computationally it is realized using a coordinate descent algorithm. Theoretically it has the selection and estimation consistency properties. Under a wide spectrum of simulation scenarios, it has identification and estimation performance comparable to or better than the alternatives. In the analysis of three lung cancer datasets with gene expression measurements, it identifies genes with sound biological implications and satisfactory prediction performance.","224":"Biased sampling occurs frequently in economics, epidemiology, and medical studies either by design or due to data collecting mechanism. Failing to take into account the sampling bias usually leads to incorrect inference. We propose a unified estimation procedure and a computationally fast resampling method to make statistical inference for quantile regression with survival data under general biased sampling schemes, including but not limited to the length-biased sampling, the case-cohort design, and variants thereof. We establish the uniform consistency and weak convergence of the proposed estimator as a process of the quantile level. We also investigate more efficient estimation using the generalized method of moments and derive the asymptotic normality. We further propose a new resampling method for inference, which differs from alternative procedures in that it does not require to repeatedly solve estimating equations. It is proved that the resampling method consistently estimates the asymptotic covariance matrix. The unified framework proposed in this article provides researchers and practitioners a convenient tool for analyzing data collected from various designs. Simulation studies and applications to real datasets are presented for illustration. Supplementary materials for this article are available online.","225":null,"226":"Investigators from a large consortium of scientists recently performed a multi-year study in which they replicated 100 psychology experiments. Although statistically significant results were reported in 97% of the original studies, statistical significance was achieved in only 36% of the replicated studies. This article presents a reanalysis of these data based on a formal statistical model that accounts for publication bias by treating outcomes from unpublished studies as missing data, while simultaneously estimating the distribution of effect sizes for those studies that tested nonnull effects. The resulting model suggests that more than 90% of tests performed in eligible psychology experiments tested negligible effects, and that publication biases based on p-values caused the observed rates of nonreproducibility. The results of this reanalysis provide a compelling argument for both increasing the threshold required for declaring scientific discoveries and for adopting statistical summaries of evidence that account for the high proportion of tested hypotheses that are false. Supplementary materials for this article are available online.","227":"We propose a generalized score type test for set-based inference for gene-environment interaction with longitudinally measured quantitative traits. The test is robust to misspecification of within subject correlation structure and has enhanced power compared to existing alternatives. Unlike tests for marginal genetic association, set-based tests for gene-environment interaction face the challenges of a potentially misspecified and high-dimensional main effect model under the null hypothesis. We show that our proposed test is robust to main effect misspecification of environmental exposure and genetic factors under the gene-environment independence condition. When genetic and environmental factors are dependent, the method of sieves is further proposed to eliminate potential bias due to a misspecified main effect of a continuous environmental exposure. A weighted principal component analysis approach is developed to perform dimension reduction when the number of genetic variants in the set is large relative to the sample size. The methods are motivated by an example from the Multi-Ethnic Study of Atherosclerosis (MESA), investigating interaction between measures of neighborhood environment and genetic regions on longitudinal measures of blood pressure over a study period of about seven years with 4 exams.","228":"Benefit-risk assessment is a crucial step in medical decision process. In many biomedical studies, both longitudinal marker measurements and time to a terminal event serve as important endpoints for benefit-risk assessment. The effect of an intervention or a treatment on the longitudinal marker process, however, can be in conflict with its effect on the time to the terminal event. Thus, questions arise on how to evaluate treatment effects based on the two endpoints, for the purpose of deciding on which treatment is most likely to benefit the patients. In this article, we present a unified framework for benefit-risk assessment using the observed longitudinal markers and time to event data. We propose a cumulative weighted marker process to synthesize information from the two endpoints, and use its mean function at a prespecified time point as a benefit-risk summary measure. We consider nonparametric estimation of the summary measure under two scenarios: (i) the longitudinal marker is measured intermittently during the study period, and (ii) the value of the longitudinal marker is observed throughout the entire follow-up period. The large-sample properties of the estimators are derived and compared. Simulation studies and data examples exhibit that the proposed methods are easy to implement and reliable for practical use. Supplemental materials for this article are available online.","229":"This paper develops a nonparametric shrinkage and selection estimator via the measurement error selection likelihood approach recently proposed by Stefanski, Wu, and White. The Measurement Error Kernel Regression Operator (MEKRO) has the same form as the Nadaraya-Watson kernel estimator, but optimizes a measurement error model selection likelihood to estimate the kernel bandwidths. Much like LASSO or COSSO solution paths, MEKRO results in solution paths depending on a tuning parameter that controls shrinkage and selection via a bound on the harmonic mean of the pseudo-measurement error standard deviations. We use small-sample-corrected AIC to select the tuning parameter. Large-sample properties of MEKRO are studied and small-sample properties are explored via Monte Carlo experiments and applications to data.","230":"We consider the task of learning a dynamical system from high-dimensional time-course data. For instance, we might wish to estimate a gene regulatory network from gene expression data measured at discrete time points. We model the dynamical system nonparametrically as a system of additive ordinary differential equations. Most existing methods for parameter estimation in ordinary differential equations estimate the derivatives from noisy observations. This is known to be challenging and inefficient. We propose a novel approach that does not involve derivative estimation. We show that the proposed method can consistently recover the true network structure even in high dimensions, and we demonstrate empirical improvement over competing approaches. Supplementary materials for this article are available online.","231":null,"232":"We consider a problem motivated by issues in nutritional epidemiology, across diseases and populations. In this area, it is becoming increasingly common for diseases to be modeled by a single diet score, such as the Healthy Eating Index, the Mediterranean Diet Score, etc. For each disease and for each population, a partially linear single-index model is fit. The partially linear aspect of the problem is allowed to differ in each population and disease. However, and crucially, the single-index itself, having to do with the diet score, is common to all diseases and populations, and the nonparametrically estimated functions of the single-index are the same up to a scale parameter. Using B-splines with an increasing number of knots, we develop a method to solve the problem, and display its asymptotic theory. An application to the NIH-AARP Study of Diet and Health is described, where we show the advantages of using multiple diseases and populations simultaneously rather than one at a time in understanding the effect of increased Milk consumption. Simulations illustrate the properties of the methods.","233":null,"234":"Individualized treatment rules (ITRs) tailor treatments according to individual patient characteristics. They can significantly improve patient care and are thus becoming increasingly popular. The data collected during randomized clinical trials are often used to estimate the optimal ITRs. However, these trials are generally expensive to run, and, moreover, they are not designed to efficiently estimate ITRs. In this article, we propose a cost-effective estimation method from an active learning perspective. In particular, our method recruits only the \"most informative\" patients (in terms of learning the optimal ITRs) from an ongoing clinical trial. Simulation studies and real-data examples show that our active clinical trial method significantly improves on competing methods. We derive risk bounds and show that they support these observed empirical advantages. Supplementary materials for this article are available online.","235":"We develop a functional conditional autoregressive (CAR) model for spatially correlated data for which functions are collected on areal units of a lattice. Our model performs functional response regression while accounting for spatial correlations with potentially nonseparable and nonstationary covariance structure, in both the space and functional domains. We show theoretically that our construction leads to a CAR model at each functional location, with spatial covariance parameters varying and borrowing strength across the functional domain. Using basis transformation strategies, the nonseparable spatial-functional model is computationally scalable to enormous functional datasets, generalizable to different basis functions, and can be used on functions defined on higher dimensional domains such as images. Through simulation studies, we demonstrate that accounting for the spatial correlation in our modeling leads to improved functional regression performance. Applied to a high-throughput spatially correlated copy number dataset, the model identifies genetic markers not identified by comparable methods that ignore spatial correlations.","236":"Exploration of causal mechanisms is often important for researchers and policymakers to understand how an intervention works and how it can be improved. This task can be crucial in clustered encouragement designs (CED). Encouragement design studies arise frequently when the treatment cannot be enforced because of ethical or practical constrains and an encouragement intervention (information campaigns, incentives, etc) is conceived with the purpose of increasing the uptake of the treatment of interest. By design, encouragements always entail the complication of non-compliance. Encouragements can also give rise to a variety of mechanisms, particularly when encouragement is assigned at cluster level. Social interactions among units within the same cluster can result in spillover effects. Disentangling the effect of encouragement through spillover effects from that through the enhancement of the treatment would give better insight into the intervention and it could be compelling for planning the scaling-up phase of the program. Building on previous works on CEDs and non-compliance, we use the principal stratification framework to define stratum-specific causal effects, that is, effects for specific latent subpopulations, defined by the joint potential compliance statuses under both encouragement conditions. We show how the latter stratum-specific causal effects are related to the decomposition commonly used in the literature and provide flexible homogeneity assumptions under which an extrapolation across principal strata allows one to disentangle the effects. Estimation of causal estimands can be performed with Bayesian inferential methods using hierarchical models to account for clustering. We illustrate the proposed methodology by analyzing a cluster randomized experiment implemented in Zambia and designed to evaluate the impact on malaria prevalence of an agricultural loan program intended to increase the bed net coverage. Farmer households assigned to the program could take advantage of a deferred payment and a discount in the purchase of new bed nets. Our analysis shows a lack of evidence of an effect of the offering of the program to a cluster of households through spillover effects, that is through a greater bed net coverage in the neighborhood.","237":null,"238":"","239":"In regions without complete-coverage civil registration and vital statistics systems there is uncertainty about even the most basic demographic indicators. In such regions the majority of deaths occur outside hospitals and are not recorded. Worldwide, fewer than one-third of deaths are assigned a cause, with the least information available from the most impoverished nations. In populations like this, verbal autopsy (VA) is a commonly used tool to assess cause of death and estimate cause-specific mortality rates and the distribution of deaths by cause. VA uses an interview with caregivers of the decedent to elicit data describing the signs and symptoms leading up to the death. This paper develops a new statistical tool known as InSilicoVA to classify cause of death using information acquired through VA. InSilicoVA shares uncertainty between cause of death assignments for specific individuals and the distribution of deaths by cause across the population. Using side-by-side comparisons with both observed and simulated data, we demonstrate that InSilicoVA has distinct advantages compared to currently available methods.","240":"With the rapidly increasing availability of data in the public domain, combining information from different sources to infer about associations or differences of interest has become an emerging challenge to researchers. This paper presents a novel approach to improve efficiency in estimating the survival time distribution by synthesizing information from the individual-level data with t-year survival probabilities from external sources such as disease registries. While disease registries provide accurate and reliable overall survival statistics for the disease population, critical pieces of information that influence both choice of treatment and clinical outcomes usually are not available in the registry database. To combine with the published information, we propose to summarize the external survival information via a system of nonlinear population moments and estimate the survival time model using empirical likelihood methods. The proposed approach is more flexible than the conventional meta-analysis in the sense that it can automatically combine survival information for different subgroups and the information may be derived from different studies. Moreover, an extended estimator that allows for a different baseline risk in the aggregate data is also studied. Empirical likelihood ratio tests are proposed to examine whether the auxiliary survival information is consistent with the individual-level data. Simulation studies show that the proposed estimators yield a substantial gain in efficiency over the conventional partial likelihood approach. Two sets of data analysis are conducted to illustrate the methods and theory.","241":"The goal of this paper is to develop a novel statistical model for studying cross-neuronal spike train interactions during decision making. For an individual to successfully complete the task of decision-making, a number of temporally-organized events must occur: stimuli must be detected, potential outcomes must be evaluated, behaviors must be executed or inhibited, and outcomes (such as reward or no-reward) must be experienced. Due to the complexity of this process, it is likely the case that decision-making is encoded by the temporally-precise interactions between large populations of neurons. Most existing statistical models, however, are inadequate for analyzing such a phenomenon because they provide only an aggregated measure of interactions over time. To address this considerable limitation, we propose a dynamic Bayesian model which captures the time-varying nature of neuronal activity (such as the time-varying strength of the interactions between neurons). The proposed method yielded results that reveal new insight into the dynamic nature of population coding in the prefrontal cortex during decision making. In our analysis, we note that while some neurons in the prefrontal cortex do not synchronize their firing activity until the presence of a reward, a different set of neurons synchronize their activity shortly after stimulus onset. These differentially synchronizing sub-populations of neurons suggests a continuum of population representation of the reward-seeking task. Secondly, our analyses also suggest that the degree of synchronization differs between the rewarded and non-rewarded conditions. Moreover, the proposed model is scalable to handle data on many simultaneously-recorded neurons and is applicable to analyzing other types of multivariate time series data with latent structure. Supplementary materials (including computer codes) for our paper are available online.","242":"","243":"Cellular response to a perturbation is the result of a dynamic system of biological variables linked in a complex network. A major challenge in drug and disease studies is identifying the key factors of a biological network that are essential in determining the cell's fate. Here our goal is the identification of perturbed pathways from high-throughput gene expression data. We develop a three-level hierarchical model, where (i) the first level captures the relationship between gene expression and biological pathways using confirmatory factor analysis, (ii) the second level models the behavior within an underlying network of pathways induced by an unknown perturbation using a conditional autoregressive model, and (iii) the third level is a spike-and-slab prior on the perturbations. We then identify perturbations through posterior-based variable selection. We illustrate our approach using gene transcription drug perturbation profiles from the DREAM7 drug sensitivity predication challenge data set. Our proposed method identified regulatory pathways that are known to play a causative role and that were not readily resolved using gene set enrichment analysis or exploratory factor models. Simulation results are presented assessing the performance of this model relative to a network-free variant and its robustness to inaccuracies in biological databases.","244":null,"245":"Information from various public and private data sources of extremely large sample sizes are now increasingly available for research purposes. Statistical methods are needed for utilizing information from such big data sources while analyzing data from individual studies that may collect more detailed information required for addressing specific hypotheses of interest. In this article, we consider the problem of building regression models based on individual-level data from an \"internal\" study while utilizing summary-level information, such as information on parameters for reduced models, from an \"external\" big data source. We identify a set of very general constraints that link internal and external models. These constraints are used to develop a framework for semiparametric maximum likelihood inference that allows the distribution of covariates to be estimated using either the internal sample or an external reference sample. We develop extensions for handling complex stratified sampling designs, such as case-control sampling, for the internal study. Asymptotic theory and variance estimators are developed for each case. We use simulation studies and a real data application to assess the performance of the proposed methods in contrast to the generalized regression (GR) calibration methodology that is popular in the sample survey literature.","246":"A tuning-free procedure is proposed to estimate the covariate-adjusted Gaussian graphical model. For each finite subgraph, this estimator is asymptotically normal and efficient. As a consequence, a confidence interval can be obtained for each edge. The procedure enjoys easy implementation and efficient computation through parallel estimation on subgraphs or edges. We further apply the asymptotic normality result to perform support recovery through edge-wise adaptive thresholding. This support recovery procedure is called ANTAC, standing for Asymptotically Normal estimation with Thresholding after Adjusting Covariates. ANTAC outperforms other methodologies in the literature in a range of simulation studies. We apply ANTAC to identify gene-gene interactions using an eQTL dataset. Our result achieves better interpretability and accuracy in comparison with CAMPE.","247":"Disease phenotyping by omics data has become a popular approach that potentially can lead to better personalized treatment. Identifying disease subtypes via unsupervised machine learning is the first step towards this goal. In this paper, we extend a sparse K-means method towards a meta-analytic framework to identify novel disease subtypes when expression profiles of multiple cohorts are available. The lasso regularization and meta-analysis identify a unique set of gene features for subtype characterization. An additional pattern matching reward function guarantees consistent subtype signatures across studies. The method was evaluated by simulations and leukemia and breast cancer data sets. The identified disease subtypes from meta-analysis were characterized with improved accuracy and stability compared to single study analysis. The breast cancer model was applied to an independent METABRIC dataset and generated improved survival difference between subtypes. These results provide a basis for diagnosis and development of targeted treatments for disease subgroups.","248":"Multiple testing of correlations arises in many applications including gene coexpression network analysis and brain connectivity analysis. In this paper, we consider large scale simultaneous testing for correlations in both the one-sample and two-sample settings. New multiple testing procedures are proposed and a bootstrap method is introduced for estimating the proportion of the nulls falsely rejected among all the true nulls. The properties of the proposed procedures are investigated both theoretically and numerically. It is shown that the procedures asymptotically control the overall false discovery rate and false discovery proportion at the nominal level. Simulation results show that the methods perform well numerically in terms of both the size and power of the test and it significantly outperforms two alternative methods. The two-sample procedure is also illustrated by an analysis of a prostate cancer dataset for the detection of changes in coexpression patterns between gene expression levels.","249":"Hidden Markov models (HMMs) are one of the most widely used statistical methods for analyzing sequence data. However, the reporting of output from HMMs has largely been restricted to the presentation of the most-probable (MAP) hidden state sequence, found via the Viterbi algorithm, or the sequence of most probable marginals using the forward-backward algorithm. In this article, we expand the amount of information we could obtain from the posterior distribution of an HMM by introducing linear-time dynamic programming recursions that, conditional on a user-specified constraint in the number of segments, allow us to (i) find MAP sequences, (ii) compute posterior probabilities, and (iii) simulate sample paths. We collectively call these recursions k-segment algorithms and illustrate their utility using simulated and real examples. We also highlight the prospective and retrospective use of k-segment constraints for fitting HMMs or exploring existing model fits. Supplementary materials for this article are available online.","250":"Positron emission tomography (PET) is an imaging technique which can be used to investigate chemical changes in human biological processes such as cancer development or neurochemical reactions. Most dynamic PET scans are currently analyzed based on the assumption that linear first-order kinetics can be used to adequately describe the system under observation. However, there has recently been strong evidence that this is not the case. To provide an analysis of PET data which is free from this compartmental assumption, we propose a nonparametric deconvolution and analysis model for dynamic PET data based on functional principal component analysis. This yields flexibility in the possible deconvolved functions while still performing well when a linear compartmental model setup is the true data generating mechanism. As the deconvolution needs to be performed on only a relative small number of basis functions rather than voxel by voxel in the entire three-dimensional volume, the methodology is both robust to typical brain imaging noise levels while also being computationally efficient. The new methodology is investigated through simulations in both one-dimensional functions and 2D images and also applied to a neuroimaging study whose goal is the quantification of opioid receptor concentration in the brain.","251":"LASSO is a popular statistical tool often used in conjunction with generalized linear models that can simultaneously select variables and estimate parameters. When there are many variables of interest, as in current biological and biomedical studies, the power of LASSO can be limited. Fortunately, so much biological and biomedical data have been collected and they may contain useful information about the importance of certain variables. This paper proposes an extension of LASSO, namely, prior LASSO (pLASSO), to incorporate that prior information into penalized generalized linear models. The goal is achieved by adding in the LASSO criterion function an additional measure of the discrepancy between the prior information and the model. For linear regression, the whole solution path of the pLASSO estimator can be found with a procedure similar to the Least Angle Regression (LARS). Asymptotic theories and simulation results show that pLASSO provides significant improvement over LASSO when the prior information is relatively accurate. When the prior information is less reliable, pLASSO shows great robustness to the misspecification. We illustrate the application of pLASSO using a real data set from a genome-wide association study.","252":"This paper studies the estimation of stepwise signal. To determine the number and locations of change-points of the stepwise signal, we formulate a maximum marginal likelihood estimator, which can be computed with a quadratic cost using dynamic programming. We carry out extensive investigation on the choice of the prior distribution and study the asymptotic properties of the maximum marginal likelihood estimator. We propose to treat each possible set of change-points equally and adopt an empirical Bayes approach to specify the prior distribution of segment parameters. Detailed simulation study is performed to compare the effectiveness of this method with other existing methods. We demonstrate our method on single-molecule enzyme reaction data and on DNA array CGH data. Our study shows that this method is applicable to a wide range of models and offers appealing results in practice.","253":"In survival analysis, quantile regression has become a useful approach to account for covariate effects on the distribution of an event time of interest. In this paper, we discuss how quantile regression can be extended to model counting processes, and thus lead to a broader regression framework for survival data. We specifically investigate the proposed modeling of counting processes for recurrent events data. We show that the new recurrent events model retains the desirable features of quantile regression such as easy interpretation and good model flexibility, while accommodating various observation schemes encountered in observational studies. We develop a general theoretical and inferential framework for the new counting process model, which unifies with an existing method for censored quantile regression. As another useful contribution of this work, we propose a sample-based covariance estimation procedure, which provides a useful complement to the prevailing bootstrapping approach. We demonstrate the utility of our proposals via simulation studies and an application to a dataset from the US Cystic Fibrosis Foundation Patient Registry (CFFPR).","254":"We propose a high dimensional classification method that involves nonparametric feature augmentation. Knowing that marginal density ratios are the most powerful univariate classifiers, we use the ratio estimates to transform the original feature measurements. Subsequently, penalized logistic regression is invoked, taking as input the newly transformed or augmented features. This procedure trains models equipped with local complexity and global simplicity, thereby avoiding the curse of dimensionality while creating a flexible nonlinear decision boundary. The resulting method is called Feature Augmentation via Nonparametrics and Selection (FANS). We motivate FANS by generalizing the Naive Bayes model, writing the log ratio of joint densities as a linear combination of those of marginal densities. It is related to generalized additive models, but has better interpretability and computability. Risk bounds are developed for FANS. In numerical analysis, FANS is compared with competing methods, so as to provide a guideline on its best application domain. Real data analysis demonstrates that FANS performs very competitively on benchmark email spam and gene expression data sets. Moreover, FANS is implemented by an extremely fast algorithm through parallel computing.","255":"Penalized regression methods, such as L1 regularization, are routinely used in high-dimensional applications, and there is a rich literature on optimality properties under sparsity assumptions. In the Bayesian paradigm, sparsity is routinely induced through two-component mixture priors having a probability mass at zero, but such priors encounter daunting computational problems in high dimensions. This has motivated continuous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians, facilitating computation. In contrast to the frequentist literature, little is known about the properties of such priors and the convergence and concentration of the corresponding posterior distribution. In this article, we propose a new class of Dirichlet-Laplace priors, which possess optimal posterior concentration and lead to efficient posterior computation. Finite sample performance of Dirichlet-Laplace priors relative to alternatives is assessed in simulated and real data examples.","256":"","257":"The family of sufficient dimension reduction (SDR) methods that produce informative combinations of predictors, or indices, are particularly useful for high dimensional regression analysis. In many such analyses, it becomes increasingly common that there is available a priori subject knowledge of the predictors; e.g., they belong to different groups. While many recent SDR proposals have greatly expanded the scope of the methods' applicability, how to effectively incorporate the prior predictor structure information remains a challenge. In this article, we aim at dimension reduction that recovers full regression information while preserving the predictor group structure. Built upon a new concept of the direct sum envelope, we introduce a systematic way to incorporate the group information in most existing SDR estimators. As a result, the reduction outcomes are much easier to interpret. Moreover, the envelope method provides a principled way to build a variety of prior structures into dimension reduction analysis. Both simulations and real data analysis demonstrate the competent numerical performance of the new method.","258":"In this paper, we introduce a new type of tree-based method, reinforcement learning trees (RLT), which exhibits significantly improved performance over traditional methods such as random forests (Breiman, 2001) under high-dimensional settings. The innovations are three-fold. First, the new method implements reinforcement learning at each selection of a splitting variable during the tree construction processes. By splitting on the variable that brings the greatest future improvement in later splits, rather than choosing the one with largest marginal effect from the immediate split, the constructed tree utilizes the available samples in a more efficient way. Moreover, such an approach enables linear combination cuts at little extra computational cost. Second, we propose a variable muting procedure that progressively eliminates noise variables during the construction of each individual tree. The muting procedure also takes advantage of reinforcement learning and prevents noise variables from being considered in the search for splitting rules, so that towards terminal nodes, where the sample size is small, the splitting rules are still constructed from only strong variables. Last, we investigate asymptotic properties of the proposed method under basic assumptions and discuss rationale in general settings.","259":"Statistical inference on conditional dependence is essential in many fields including genetic association studies and graphical models. The classic measures focus on linear conditional correlations, and are incapable of characterizing non-linear conditional relationship including non-monotonic relationship. To overcome this limitation, we introduces a nonparametric measure of conditional dependence for multivariate random variables with arbitrary dimensions. Our measure possesses the necessary and intuitive properties as a correlation index. Briefly, it is zero almost surely if and only if two multivariate random variables are conditionally independent given a third random variable. More importantly, the sample version of this measure can be expressed elegantly as the root of a V or U-process with random kernels and has desirable theoretical properties. Based on the sample version, we propose a test for conditional independence, which is proven to be more powerful than some recently developed tests through our numerical simulations. The advantage of our test is even greater when the relationship between the multivariate random variables given the third random variable cannot be expressed in a linear or monotonic function of one random variable versus the other. We also show that the sample measure is consistent and weakly convergent, and the test statistic is asymptotically normal. By applying our test in a real data analysis, we are able to identify two conditionally associated gene expressions, which otherwise cannot be revealed. Thus, our measure of conditional dependence is not only an ideal concept, but also has important practical utility.","260":"Motivated by the advent of high dimensional highly correlated data, this work studies the limit behavior of the empirical cumulative distribution function (ecdf) of standard normal random variables under arbitrary correlation. First, we provide a necessary and sufficient condition for convergence of the ecdf to the standard normal distribution. Next, under general correlation, we show that the ecdf limit is a random, possible infinite, mixture of normal distribution functions that depends on a number of latent variables and can serve as an asymptotic approximation to the ecdf in high dimensions. We provide conditions under which the dimension of the ecdf limit, defined as the smallest number of effective latent variables, is finite. Estimates of the latent variables are provided and their consistency proved. We demonstrate these methods in a real high-dimensional data example from brain imaging where it is shown that, while the study exhibits apparently strongly significant results, they can be entirely explained by correlation, as captured by the asymptotic approximation developed here.","261":"","262":"Many approaches for multiple testing begin with the assumption that all tests in a given study should be combined into a global false-discovery-rate analysis. But this may be inappropriate for many of today's large-scale screening problems, where auxiliary information about each test is often available, and where a combined analysis can lead to poorly calibrated error rates within different subsets of the experiment. To address this issue, we introduce an approach called false-discovery-rate regression that directly uses this auxiliary information to inform the outcome of each test. The method can be motivated by a two-groups model in which covariates are allowed to influence the local false discovery rate, or equivalently, the posterior probability that a given observation is a signal. This poses many subtle issues at the interface between inference and computation, and we investigate several variations of the overall approach. Simulation evidence suggests that: (1) when covariate effects are present, FDR regression improves power for a fixed false-discovery rate; and (2) when covariate effects are absent, the method is robust, in the sense that it does not lead to inflated error rates. We apply the method to neural recordings from primary visual cortex. The goal is to detect pairs of neurons that exhibit fine-time-scale interactions, in the sense that they fire together more often than expected due to chance. Our method detects roughly 50% more synchronous pairs versus a standard FDR-controlling analysis. The companion R package FDRreg implements all methods described in the paper.","263":"This work is concerned with testing the population mean vector of nonnormal high-dimensional multivariate data. Several tests for high-dimensional mean vector, based on modifying the classical Hotelling T2 test, have been proposed in the literature. Despite their usefulness, they tend to have unsatisfactory power performance for heavy-tailed multivariate data, which frequently arise in genomics and quantitative finance. This paper proposes a novel high-dimensional nonparametric test for the population mean vector for a general class of multivariate distributions. With the aid of new tools in modern probability theory, we proved that the limiting null distribution of the proposed test is normal under mild conditions when p is substantially larger than n. We further study the local power of the proposed test and compare its relative efficiency with a modified Hotelling T2 test for high-dimensional data. An interesting finding is that the newly proposed test can have even more substantial power gain with large p than the traditional nonparametric multivariate test does with finite fixed p. We study the finite sample performance of the proposed test via Monte Carlo simulations. We further illustrate its application by an empirical analysis of a genomics data set.","264":"Network analysis is often focused on characterizing the dependencies between network relations and node-level attributes. Potential relationships are typically explored by modeling the network as a function of the nodal attributes or by modeling the attributes as a function of the network. These methods require specification of the exact nature of the association between the network and attributes, reduce the network data to a small number of summary statistics, and are unable provide predictions simultaneously for missing attribute and network information. Existing methods that model the attributes and network jointly also assume the data are fully observed. In this article we introduce a unified approach to analysis that addresses these shortcomings. We use a previously developed latent variable model to obtain a low dimensional representation of the network in terms of node-specific network factors. We introduce a novel testing procedure to determine if dependencies exist between the network factors and attributes as a surrogate for a test of dependence between the network and attributes. We also present a joint model for the network relations and attributes, for use if the hypothesis of independence is rejected, that can capture a variety of dependence patterns and be used to make inference and predictions for missing observations.","265":"In estimating ROC curves of multiple tests, some a priori constraints may exist, either between the healthy and diseased populations within a test or between tests within a population. In this paper, we proposed an integrated modeling approach for ROC curves that jointly accounts for stochastic and variability orders. The stochastic order constrains the distributional centers of the diseased and healthy populations within a test, while the variability order constrains the distributional spreads of the tests within each of the populations. Under a Bayesian nonparametric framework, we used features of the Dirichlet process mixture to incorporate these order constraints in a natural way. We applied the proposed approach to data from the Physician Reliability Study that investigated the accuracy of diagnosing endometriosis using different clinical information. To address the issue of no gold standard in the real data, we used a sensitivity analysis approach that exploited diagnosis from a panel of experts. To demonstrate the performance of the methodology, we conducted simulation studies with varying sample sizes, distributional assumptions and order constraints. Supplementary materials for this article are available online.","266":"We propose localized functional principal component analysis (LFPCA), looking for orthogonal basis functions with localized support regions that explain most of the variability of a random process. The LFPCA is formulated as a convex optimization problem through a novel Deflated Fantope Localization method and is implemented through an efficient algorithm to obtain the global optimum. We prove that the proposed LFPCA converges to the original FPCA when the tuning parameters are chosen appropriately. Simulation shows that the proposed LFPCA with tuning parameters chosen by cross validation can almost perfectly recover the true eigenfunctions and significantly improve the estimation accuracy when the eigenfunctions are truly supported on some subdomains. In the scenario that the original eigenfunctions are not localized, the proposed LFPCA also serves as a nice tool in finding orthogonal basis functions that balance between interpretability and the capability of explaining variability of the data. The analyses of a country mortality data reveal interesting features that cannot be found by standard FPCA methods.","267":"We propose a procedure associated with the idea of the E-M algorithm for model selection in the presence of missing data. The idea extends the concept of parameters to include both the model and the parameters under the model, and thus allows the model to be part of the E-M iterations. We develop the procedure, known as the E-MS algorithm, under the assumption that the class of candidate models is finite. Some special cases of the procedure are considered, including E-MS with the generalized information criteria (GIC), and E-MS with the adaptive fence (AF; Jiang et al. 2008). We prove numerical convergence of the E-MS algorithm as well as consistency in model selection of the limiting model of the E-MS convergence, for E-MS with GIC and E-MS with AF. We study the impact on model selection of different missing data mechanisms. Furthermore, we carry out extensive simulation studies on the finite-sample performance of the E-MS with comparisons to other procedures. The methodology is also illustrated on a real data analysis involving QTL mapping for an agricultural study on barley grains.","268":"","269":"In many application areas, data are collected on a categorical response and high-dimensional categorical predictors, with the goals being to build a parsimonious model for classification while doing inferences on the important predictors. In settings such as genomics, there can be complex interactions among the predictors. By using a carefully-structured Tucker factorization, we define a model that can characterize any conditional probability, while facilitating variable selection and modeling of higher-order interactions. Following a Bayesian approach, we propose a Markov chain Monte Carlo algorithm for posterior computation accommodating uncertainty in the predictors to be included. Under near low rank assumptions, the posterior distribution for the conditional probability is shown to achieve close to the parametric rate of contraction even in ultra high-dimensional settings. The methods are illustrated using simulation examples and biomedical applications.","270":"This article develops joint inferential methods for the cause-specific hazard function and the cumulative incidence function of a specific type of failure to assess the effects of a variable on the time to the type of failure of interest in the presence of competing risks. Joint inference for the two functions are needed in practice because (i) they describe different characteristics of a given type of failure, (ii) they do not uniquely determine each other, and (iii) the effects of a variable on the two functions can be different and one often does not know which effects are to be expected. We study both the group comparison problem and the regression problem. We also discuss joint inference for other related functions. Our simulation shows that our joint tests can be considerably more powerful than the Bonferroni method, which has important practical implications to the analysis and design of clinical studies with competing risks data. We illustrate our method using a Hodgkin disease data and a lymphoma data. Supplementary materials for this article are available online.","271":"Case-control design is widely used in epidemiology and other fields to identify factors associated with a disease. Data collected from existing case-control studies can also provide a cost-effective way to investigate the association of risk factors with secondary outcomes. When the secondary outcome is a continuous random variable, most of the existing methods focus on the statistical inference on the mean of the secondary outcome. In this paper, we propose a quantile-based approach to facilitating a comprehensive investigation of covariates' effects on multiple quantiles of the secondary outcome. We construct a new family of estimating equations combining observed and pseudo outcomes, which lead to consistent estimation of conditional quantiles using case-control data. Simulations are conducted to evaluate the performance of our proposed approach, and a case-control study on genetic association with asthma is used to demonstrate the method.","272":"Spatial process models for analyzing geostatistical data entail computations that become prohibitive as the number of spatial locations become large. This article develops a class of highly scalable nearest-neighbor Gaussian process (NNGP) models to provide fully model-based inference for large geostatistical datasets. We establish that the NNGP is a well-defined spatial process providing legitimate finite-dimensional Gaussian densities with sparse precision matrices. We embed the NNGP as a sparsity-inducing prior within a rich hierarchical modeling framework and outline how computationally efficient Markov chain Monte Carlo (MCMC) algorithms can be executed without storing or decomposing large matrices. The floating point operations (flops) per iteration of this algorithm is linear in the number of spatial locations, thereby rendering substantial scalability. We illustrate the computational and inferential benefits of the NNGP over competing methods using simulation studies and also analyze forest biomass from a massive U.S. Forest Inventory dataset at a scale that precludes alternative dimension-reducing methods. Supplementary materials for this article are available online.","273":"With the abundance of high dimensional data in various disciplines, sparse regularized techniques are very popular these days. In this paper, we make use of the structure information among predictors to improve sparse regression models. Typically, such structure information can be modeled by the connectivity of an undirected graph using all predictors as nodes of the graph. Most existing methods use this undirected graph edge-by-edge to encourage the regression coefficients of corresponding connected predictors to be similar. However, such methods do not directly utilize the neighborhood information of the graph. Furthermore, if there are more edges in the predictor graph, the corresponding regularization term will be more complicate. In this paper, we incorporate the graph information node-by-node, instead of edge-by-edge as used in most existing methods. Our proposed method is very general and it includes adaptive Lasso, group Lasso, and ridge regression as special cases. Both theoretical and numerical studies demonstrate the effectiveness of the proposed method for simultaneous estimation, prediction and model selection.","274":"Inference on data with missingness can be challenging, particularly if the knowledge that a measurement was unobserved provides information about its distribution. Our work is motivated by the Commit to Quit II study, a smoking cessation trial that measured smoking status and weight change as weekly outcomes. It is expected that dropout in this study was informative and that patients with missed measurements are more likely to be smoking, even after conditioning on their observed smoking and weight history. We jointly model the categorical smoking status and continuous weight change outcomes by assuming normal latent variables for cessation and by extending the usual pattern mixture model to the bivariate case. The model includes a novel approach to sharing information across patterns through a Bayesian shrinkage framework to improve estimation stability for sparsely observed patterns. To accommodate the presumed informativeness of the missing data in a parsimonious manner, we model the unidentified components of the model under a non-future dependence assumption and specify departures from missing at random through sensitivity parameters, whose distributions are elicited from a subject-matter expert.","275":null,"276":"To maintain proper cellular functions, over 50% of proteins encoded in the genome need to be transported to cellular membranes. The molecular mechanism behind such a process, often referred to as protein targeting, is not well understood. Single-molecule experiments are designed to unveil the detailed mechanisms and reveal the functions of different molecular machineries involved in the process. The experimental data consist of hundreds of stochastic time traces from the fluorescence recordings of the experimental system. We introduce a Bayesian hierarchical model on top of hidden Markov models (HMMs) to analyze these data and use the statistical results to answer the biological questions. In addition to resolving the biological puzzles and delineating the regulating roles of different molecular complexes, our statistical results enable us to propose a more detailed mechanism for the late stages of the protein targeting process.","277":"This comment deals with issues related to the article by Chen, Zeng, and Kosorok. We present several potential modifications of the outcome weighted learning approach. Those modifications are based on truncated l2 loss. One advantage of l2 loss is that it is differentiable everywhere, which makes it more stable and computationally more tractable.","278":"Our work is motivated by a prostate cancer study aimed at identifying mRNA and miRNA biomarkers that are predictive of cancer recurrence after prostatectomy. It has been shown in the literature that incorporating known biological information on pathway memberships and interactions among biomarkers improves feature selection of high-dimensional biomarkers in relation to disease risk. Biological information is often represented by graphs or networks, in which biomarkers are represented by nodes and interactions among them are represented by edges; however, biological information is often not fully known. For example, the role of microRNAs (miRNAs) in regulating gene expression is not fully understood and the miRNA regulatory network is not fully established, in which case new strategies are needed for feature selection. To this end, we treat unknown biological information as missing data (i.e., missing edges in graphs), different from commonly encountered missing data problems where variable values are missing. We propose a new concept of imputing unknown biological information based on observed data and define the imputed information as the novel biological information. In addition, we propose a hierarchical group penalty to encourage sparsity and feature selection at both the pathway level and the within-pathway level, which, combined with the imputation step, allows for incorporation of known and novel biological information. While it is applicable to general regression settings, we develop and investigate the proposed approach in the context of semiparametric accelerated failure time models motivated by our data example. Data application and simulation studies show that incorporation of novel biological information improves performance in risk prediction and feature selection and the proposed penalty outperforms the extensions of several existing penalties.","279":"Practical Bayesian nonparametric methods have been developed across a wide variety of contexts. Here, we develop a novel statistical model that generalizes standard mixed models for longitudinal data that include flexible mean functions as well as combined compound symmetry (CS) and autoregressive (AR) covariance structures. AR structure is often specified through the use of a Gaussian process (GP) with covariance functions that allow longitudinal data to be more correlated if they are observed closer in time than if they are observed farther apart. We allow for AR structure by considering a broader class of models that incorporates a Dirichlet Process Mixture (DPM) over the covariance parameters of the GP. We are able to take advantage of modern Bayesian statistical methods in making full predictive inferences and about characteristics of longitudinal profiles and their differences across covariate combinations. We also take advantage of the generality of our model, which provides for estimation of a variety of covariance structures. We observe that models that fail to incorporate CS or AR structure can result in very poor estimation of a covariance or correlation matrix. In our illustration using hormone data observed on women through the menopausal transition, biology dictates the use of a generalized family of sigmoid functions as a model for time trends across subpopulation categories.","280":"Independence screening is powerful for variable selection when the number of variables is massive. Commonly used independence screening methods are based on marginal correlations or its variants. When some prior knowledge on a certain important set of variables is available, a natural assessment on the relative importance of the other predictors is their conditional contributions to the response given the known set of variables. This results in conditional sure independence screening (CSIS). CSIS produces a rich family of alternative screening methods by different choices of the conditioning set and can help reduce the number of false positive and false negative selections when covariates are highly correlated. This paper proposes and studies CSIS in generalized linear models. We give conditions under which sure screening is possible and derive an upper bound on the number of selected variables. We also spell out the situation under which CSIS yields model selection consistency and the properties of CSIS when a data-driven conditioning set is used. Moreover, we provide two data-driven methods to select the thresholding parameter of conditional screening. The utility of the procedure is illustrated by simulation studies and analysis of two real datasets.","281":"Estimation of change-point locations in the broken-stick model has significant applications in modeling important biological phenomena. In this article we present a computationally economical likelihood-based approach for estimating change-point(s) efficiently in both cross-sectional and longitudinal settings. Our method, based on local smoothing in a shrinking neighborhood of each change-point, is shown via simulations to be computationally more viable than existing methods that rely on search procedures, with dramatic gains in the multiple change-point case. The proposed estimates are shown to have [Formula: see text]-consistency and asymptotic normality - in particular, they are asymptotically efficient in the cross-sectional setting - allowing us to provide meaningful statistical inference. As our primary and motivating (longitudinal) application, we study the Michigan Bone Health and Metabolism Study cohort data to describe patterns of change in log estradiol levels, before and after the final menstrual period, for which a two change-point broken stick model appears to be a good fit. We also illustrate our method on a plant growth data set in the cross-sectional setting.","282":null,"283":"","284":"In dose-finding clinical trials, it is becoming increasingly important to account for individual level heterogeneity while searching for optimal doses to ensure an optimal individualized dose rule (IDR) maximizes the expected beneficial clinical outcome for each individual. In this paper, we advocate a randomized trial design where candidate dose levels assigned to study subjects are randomly chosen from a continuous distribution within a safe range. To estimate the optimal IDR using such data, we propose an outcome weighted learning method based on a nonconvex loss function, which can be solved efficiently using a difference of convex functions algorithm. The consistency and convergence rate for the estimated IDR are derived, and its small-sample performance is evaluated via simulation studies. We demonstrate that the proposed method outperforms competing approaches. Finally, we illustrate this method using data from a cohort study for Warfarin (an anti-thrombotic drug) dosing.","285":"Motivated by the problem of selecting representative portfolios for backtesting counterparty credit risks, we propose a matching quantiles estimation (MQE) method for matching a target distribution by that of a linear combination of a set of random variables. An iterative procedure based on the ordinary least-squares estimation (OLS) is proposed to compute MQE. MQE can be easily modified by adding a LASSO penalty term if a sparse representation is desired, or by restricting the matching within certain range of quantiles to match a part of the target distribution. The convergence of the algorithm and the asymptotic properties of the estimation, both with or without LASSO, are established. A measure and an associated statistical test are proposed to assess the goodness-of-match. The finite sample properties are illustrated by simulation. An application in selecting a counterparty representative portfolio with a real dataset is reported. The proposed MQE also finds applications in portfolio tracking, which demonstrates the usefulness of combining MQE with LASSO.","286":"Mandarin Chinese is characterized by being a tonal language; the pitch (or F0) of its utterances carries considerable linguistic information. However, speech samples from different individuals are subject to changes in amplitude and phase, which must be accounted for in any analysis that attempts to provide a linguistically meaningful description of the language. A joint model for amplitude, phase, and duration is presented, which combines elements from functional data analysis, compositional data analysis, and linear mixed effects models. By decomposing functions via a functional principal component analysis, and connecting registration functions to compositional data analysis, a joint multivariate mixed effect model can be formulated, which gives insights into the relationship between the different modes of variation as well as their dependence on linguistic and nonlinguistic covariates. The model is applied to the COSPRO-1 dataset, a comprehensive database of spoken Taiwanese Mandarin, containing approximately 50,000 phonetically diverse sample F0 contours (syllables), and reveals that phonetic information is jointly carried by both amplitude and phase variation. Supplementary materials for this article are available online.","287":"We have developed a statistical method named IsoDOT to assess differential isoform expression (DIE) and differential isoform usage (DIU) using RNA-seq data. Here isoform usage refers to relative isoform expression given the total expression of the corresponding gene. IsoDOT performs two tasks that cannot be accomplished by existing methods: to test DIE\/DIU with respect to a continuous covariate, and to test DIE\/DIU for one case versus one control. The latter task is not an uncommon situation in practice, e.g., comparing the paternal and maternal alleles of one individual or comparing tumor and normal samples of one cancer patient. Simulation studies demonstrate the high sensitivity and specificity of IsoDOT. We apply IsoDOT to study the effects of haloperidol treatment on the mouse transcriptome and identify a group of genes whose isoform usages respond to haloperidol treatment.","288":"An important goal in image analysis is to cluster and recognize objects of interest according to the shapes of their boundaries. Clustering such objects faces at least four major challenges including a curved shape space, a high-dimensional feature space, a complex spatial correlation structure, and shape variation associated with some covariates (e.g., age or gender). The aim of this paper is to develop a penalized model-based clustering framework to cluster landmark-based planar shape data, while explicitly addressing these challenges. Specifically, a mixture of offset-normal shape factor analyzers (MOSFA) is proposed with mixing proportions defined through a regression model (e.g., logistic) and an offset-normal shape distribution in each component for data in the curved shape space. A latent factor analysis model is introduced to explicitly model the complex spatial correlation. A penalized likelihood approach with both adaptive pairwise fusion Lasso penalty function and L2 penalty function is used to automatically realize variable selection via thresholding and deliver a sparse solution. Our real data analysis has confirmed the excellent finite-sample performance of MOSFA in revealing meaningful clusters in the corpus callosum shape data obtained from the Attention Deficit Hyperactivity Disorder-200 (ADHD-200) study.","289":"Regression analysis of censored failure observations via the proportional hazards model permits time-varying covariates which are observed at death times. In practice, such longitudinal covariates are typically sparse and only measured at infrequent and irregularly spaced follow-up times. Full likelihood analyses of joint models for longitudinal and survival data impose stringent modelling assumptions which are difficult to verify in practice and which are complicated both inferentially and computationally. In this article, a simple kernel weighted score function is proposed with minimal assumptions. Two scenarios are considered: half kernel estimation in which observation ceases at the time of the event and full kernel estimation for data where observation may continue after the event, as with recurrent events data. It is established that these estimators are consistent and asymptotically normal. However, they converge at rates which are slower than the parametric rates which may be achieved with fully observed covariates, with the full kernel method achieving an optimal convergence rate which is superior to that of the half kernel method. Simulation results demonstrate that the large sample approximations are adequate for practical use and may yield improved performance relative to last value carried forward approach and joint modelling method. The analysis of the data from a cardiac arrest study demonstrates the utility of the proposed methods.","290":"Studies of expression quantitative trait loci (eQTLs) offer insight into the molecular mechanisms of loci that were found to be associated with complex diseases and the mechanisms can be classified into cis- and trans-acting regulation. At present, high-throughput RNA sequencing (RNA-seq) is rapidly replacing expression microarrays to assess gene expression abundance. Unlike microarrays that only measure the total expression of each gene, RNA-seq also provides information on allele-specific expression (ASE), which can be used to distinguish cis-eQTLs from trans-eQTLs and, more importantly, enhance cis-eQTL mapping. However, assessing the cis-effect of a candidate eQTL on a gene requires knowledge of the haplotypes connecting the candidate eQTL and the gene, which cannot be inferred with certainty. The existing two-stage approach that first phases the candidate eQTL against the gene and then treats the inferred phase as observed in the association analysis tends to attenuate the estimated cis-effect and reduce the power for detecting a cis-eQTL. In this article, we provide a maximum-likelihood framework for cis-eQTL mapping with RNA-seq data. Our approach integrates the inference of haplotypes and the association analysis into a single stage, and is thus unbiased and statistically powerful. We also develop a pipeline for performing a comprehensive scan of all local eQTLs for all genes in the genome by controlling for false discovery rate, and implement the methods in a computationally efficient software program. The advantages of the proposed methods over the existing ones are demonstrated through realistic simulation studies and an application to empirical breast cancer data from The Cancer Genome Atlas project.","291":"Dependent phenomena, such as relational, spatial and temporal phenomena, tend to be characterized by local dependence in the sense that units which are close in a well-defined sense are dependent. In contrast with spatial and temporal phenomena, though, relational phenomena tend to lack a natural neighbourhood structure in the sense that it is unknown which units are close and thus dependent. Owing to the challenge of characterizing local dependence and constructing random graph models with local dependence, many conventional exponential family random graph models induce strong dependence and are not amenable to statistical inference. We take first steps to characterize local dependence in random graph models, inspired by the notion of finite neighbourhoods in spatial statistics and M-dependence in time series, and we show that local dependence endows random graph models with desirable properties which make them amenable to statistical inference. We show that random graph models with local dependence satisfy a natural domain consistency condition which every model should satisfy, but conventional exponential family random graph models do not satisfy. In addition, we establish a central limit theorem for random graph models with local dependence, which suggests that random graph models with local dependence are amenable to statistical inference. We discuss how random graph models with local dependence can be constructed by exploiting either observed or unobserved neighbourhood structure. In the absence of observed neighbourhood structure, we take a Bayesian view and express the uncertainty about the neighbourhood structure by specifying a prior on a set of suitable neighbourhood structures. We present simulation results and applications to two real world networks with 'ground truth'.","292":"","293":"The aim of this paper is to develop a sparse projection regression modeling (SPReM) framework to perform multivariate regression modeling with a large number of responses and a multivariate covariate of interest. We propose two novel heritability ratios to simultaneously perform dimension reduction, response selection, estimation, and testing, while explicitly accounting for correlations among multivariate responses. Our SPReM is devised to specifically address the low statistical power issue of many standard statistical approaches, such as the Hotelling's T2 test statistic or a mass univariate analysis, for high-dimensional data. We formulate the estimation problem of SPREM as a novel sparse unit rank projection (SURP) problem and propose a fast optimization algorithm for SURP. Furthermore, we extend SURP to the sparse multi-rank projection (SMURP) by adopting a sequential SURP approximation. Theoretically, we have systematically investigated the convergence properties of SURP and the convergence rate of SURP estimates. Our simulation results and real data analysis have shown that SPReM out-performs other state-of-the-art methods.","294":"We introduce a nonparametric method for estimating non-gaussian graphical models based on a new statistical relation called additive conditional independence, which is a three-way relation among random vectors that resembles the logical structure of conditional independence. Additive conditional independence allows us to use one-dimensional kernel regardless of the dimension of the graph, which not only avoids the curse of dimensionality but also simplifies computation. It also gives rise to a parallel structure to the gaussian graphical model that replaces the precision matrix by an additive precision operator. The estimators derived from additive conditional independence cover the recently introduced nonparanormal graphical model as a special case, but outperform it when the gaussian copula assumption is violated. We compare the new method with existing ones by simulations and in genetic pathway analysis.","295":null,"296":"In genetical genomics studies, it is important to jointly analyze gene expression data and genetic variants in exploring their associations with complex traits, where the dimensionality of gene expressions and genetic variants can both be much larger than the sample size. Motivated by such modern applications, we consider the problem of variable selection and estimation in high-dimensional sparse instrumental variables models. To overcome the difficulty of high dimensionality and unknown optimal instruments, we propose a two-stage regularization framework for identifying and estimating important covariate effects while selecting and estimating optimal instruments. The methodology extends the classical two-stage least squares estimator to high dimensions by exploiting sparsity using sparsity-inducing penalty functions in both stages. The resulting procedure is efficiently implemented by coordinate descent optimization. For the representative L1 regularization and a class of concave regularization methods, we establish estimation, prediction, and model selection properties of the two-stage regularized estimators in the high-dimensional setting where the dimensionality of co-variates and instruments are both allowed to grow exponentially with the sample size. The practical performance of the proposed method is evaluated by simulation studies and its usefulness is illustrated by an analysis of mouse obesity data. Supplementary materials for this article are available online.","297":"A phase I\/II clinical trial design is proposed for adaptively and dynamically optimizing each patient's dose in each of two cycles of therapy based on the joint binary efficacy and toxicity outcomes in each cycle. A dose-outcome model is assumed that includes a Bayesian hierarchical latent variable structure to induce association among the outcomes and also facilitate posterior computation. Doses are chosen in each cycle based on posteriors of a model-based objective function, similar to a reinforcement learning or Q-learning function, defined in terms of numerical utilities of the joint outcomes in each cycle. For each patient, the procedure outputs a sequence of two actions, one for each cycle, with each action being the decision to either treat the patient at a chosen dose or not to treat. The cycle 2 action depends on the individual patient's cycle 1 dose and outcomes. In addition, decisions are based on posterior inference using other patients' data, and therefore the proposed method is adaptive both within and between patients. A simulation study of the method is presented, including comparison to two-cycle extensions of the conventional 3+3 algorithm, continual reassessment method, and a Bayesian model-based design, and evaluation of robustness.","298":"High-throughput DNA sequencing allows for the genotyping of common and rare variants for genetic association studies. At the present time and for the foreseeable future, it is not economically feasible to sequence all individuals in a large cohort. A cost-effective strategy is to sequence those individuals with extreme values of a quantitative trait. We consider the design under which the sampling depends on multiple quantitative traits. Under such trait-dependent sampling, standard linear regression analysis can result in bias of parameter estimation, inflation of type I error, and loss of power. We construct a likelihood function that properly reflects the sampling mechanism and utilizes all available data. We implement a computationally efficient EM algorithm and establish the theoretical properties of the resulting maximum likelihood estimators. Our methods can be used to perform separate inference on each trait or simultaneous inference on multiple traits. We pay special attention to gene-level association tests for rare variants. We demonstrate the superiority of the proposed methods over standard linear regression through extensive simulation studies. We provide applications to the Cohorts for Heart and Aging Research in Genomic Epidemiology Targeted Sequencing Study and the National Heart, Lung, and Blood Institute Exome Sequencing Project.","299":"Diagnostic classification models have recently gained prominence in educational assessment, psychiatric evaluation, and many other disciplines. Central to the model specification is the so-called Q-matrix that provides a qualitative specification of the item-attribute relationship. In this paper, we develop theories on the identifiability for the Q-matrix under the DINA and the DINO models. We further propose an estimation procedure for the Q-matrix through the regularized maximum likelihood. The applicability of this procedure is not limited to the DINA or the DINO model and it can be applied to essentially all Q-matrix based diagnostic classification models. Simulation studies are conducted to illustrate its performance. Furthermore, two case studies are presented. The first case is a data set on fraction subtraction (educational application) and the second case is a subsample of the National Epidemiological Survey on Alcohol and Related Conditions concerning the social anxiety disorder (psychiatric application).","300":"We develop a test statistic for testing the equality of two population mean vectors in the \"large-p-small-n\" setting. Such a test must surmount the rank-deficiency of the sample covariance matrix, which breaks down the classic Hotelling T2 test. The proposed procedure, called the generalized component test, avoids full estimation of the covariance matrix by assuming that the p components admit a logical ordering such that the dependence between components is related to their displacement. The test is shown to be competitive with other recently developed methods under ARMA and long-range dependence structures and to achieve superior power for heavy-tailed data. The test does not assume equality of covariance matrices between the two populations, is robust to heteroscedasticity in the component variances, and requires very little computation time, which allows its use in settings with very large p. An analysis of mitochondrial calcium concentration in mouse cardiac muscles over time and of copy number variations in a glioblastoma multiforme data set from The Cancer Genome Atlas are carried out to illustrate the test.","301":"Every newly trained surgeon performs her first unsupervised operation. How do the health outcomes of her patients compare with the patients of experienced surgeons? Using data from 498 hospitals, we compare 1252 pairs comprised of a new surgeon and an experienced surgeon working at the same hospital. We introduce a new form of matching that matches patients of each new surgeon to patients of an otherwise similar experienced surgeon at the same hospital, perfectly balancing 176 surgical procedures and closely balancing a total of 2.9 million categories of patients; additionally, the individual patient pairs are as close as possible. A new goal for matching is introduced, called \"refined covariate balance,\" in which a sequence of nested, ever more refined, nominal covariates is balanced as closely as possible, emphasizing the first or coarsest covariate in that sequence. A new algorithm for matching is proposed and the main new results prove that the algorithm finds the closest match in terms of the total within-pair covariate distances among all matches that achieve refined covariate balance. Unlike previous approaches to forcing balance on covariates, the new algorithm creates multiple paths to a match in a network, where paths that introduce imbalances are penalized and hence avoided to the extent possible. The algorithm exploits a sparse network to quickly optimize a match that is about two orders of magnitude larger than is typical in statistical matching problems, thereby permitting much more extensive use of fine and near-fine balance constraints. The match was constructed in a few minutes using a network optimization algorithm implemented in R. An R package called rcbalance implementing the method is available from CRAN.","302":null,"303":"Dynamic treatment regimes (DTRs) are sequential decision rules for individual patients that can adapt over time to an evolving illness. The goal is to accommodate heterogeneity among patients and find the DTR which will produce the best long term outcome if implemented. We introduce two new statistical learning methods for estimating the optimal DTR, termed backward outcome weighted learning (BOWL), and simultaneous outcome weighted learning (SOWL). These approaches convert individualized treatment selection into an either sequential or simultaneous classification problem, and can thus be applied by modifying existing machine learning techniques. The proposed methods are based on directly maximizing over all DTRs a nonparametric estimator of the expected long-term outcome; this is fundamentally different than regression-based methods, for example Q-learning, which indirectly attempt such maximization and rely heavily on the correctness of postulated regression models. We prove that the resulting rules are consistent, and provide finite sample bounds for the errors using the estimated rules. Simulation results suggest the proposed methods produce superior DTRs compared with Q-learning especially in small samples. We illustrate the methods using data from a clinical trial for smoking cessation.","304":"Genetic studies of complex traits have uncovered only a small number of risk markers explaining a small fraction of heritability and adding little improvement to disease risk prediction. Standard single marker methods may lack power in selecting informative markers or estimating effects. Most existing methods also typically do not account for non-linearity. Identifying markers with weak signals and estimating their joint effects among many non-informative markers remains challenging. One potential approach is to group markers based on biological knowledge such as gene structure. If markers in a group tend to have similar effects, proper usage of the group structure could improve power and efficiency in estimation. We propose a two-stage method relating markers to disease risk by taking advantage of known gene-set structures. Imposing a naive bayes kernel machine (KM) model, we estimate gene-set specific risk models that relate each gene-set to the outcome in stage I. The KM framework efficiently models potentially non-linear effects of predictors without requiring explicit specification of functional forms. In stage II, we aggregate information across gene-sets via a regularization procedure. Estimation and computational efficiency is further improved with kernel principle component analysis. Asymptotic results for model estimation and gene set selection are derived and numerical studies suggest that the proposed procedure could outperform existing procedures for constructing genetic risk models.","305":"We develop a Bayesian nonparametric model for a longitudinal response in the presence of nonignorable missing data. Our general approach is to first specify a working model that flexibly models the missingness and full outcome processes jointly. We specify a Dirichlet process mixture of missing at random (MAR) models as a prior on the joint distribution of the working model. This aspect of the model governs the fit of the observed data by modeling the observed data distribution as the marginalization over the missing data in the working model. We then separately specify the conditional distribution of the missing data given the observed data and dropout. This approach allows us to identify the distribution of the missing data using identifying restrictions as a starting point. We propose a framework for introducing sensitivity parameters, allowing us to vary the untestable assumptions about the missing data mechanism smoothly. Informative priors on the space of missing data assumptions can be specified to combine inferences under many different assumptions into a final inference and accurately characterize uncertainty. These methods are motivated by, and applied to, data from a clinical trial assessing the efficacy of a new treatment for acute Schizophrenia.","306":"We consider a specific situation of correlated data where multiple outcomes are repeatedly measured on each member of a couple. Such multivariate longitudinal data from couples may exhibit multi-faceted correlations which can be further complicated if there are polygamous partnerships. An example is data from cohort studies on human papillomavirus (HPV) transmission dynamics in heterosexual couples. HPV is a common sexually transmitted disease with 14 known oncogenic types causing anogenital cancers. The binary outcomes on the multiple types measured in couples over time may introduce inter-type, intra-couple, and temporal correlations. Simple analysis using generalized estimating equations or random effects models lacks interpretability and cannot fully utilize the available information. We developed a hybrid modeling strategy using Markov transition models together with pairwise composite likelihood for analyzing such data. The method can be used to identify risk factors associated with HPV transmission and persistence, estimate difference in risks between male-to-female and female-to-male HPV transmission, compare type-specific transmission risks within couples, and characterize the inter-type and intra-couple associations. Applying the method to HPV couple data collected in a Ugandan male circumcision (MC) trial, we assessed the effect of MC and the role of gender on risks of HPV transmission and persistence.","307":"Covariate measurement imprecision or errors arise frequently in many areas. It is well known that ignoring such errors can substantially degrade the quality of inference or even yield erroneous results. Although in practice both covariates subject to measurement error and covariates subject to misclassification can occur, research attention in the literature has mainly focused on addressing either one of these problems separately. To fill this gap, we develop estimation and inference methods that accommodate both characteristics simultaneously. Specifically, we consider measurement error and misclassification in generalized linear models under the scenario that an external validation study is available, and systematically develop a number of effective functional and structural methods. Our methods can be applied to different situations to meet various objectives.","308":"Meta-analysis has been widely used to synthesize evidence from multiple studies for common hypotheses or parameters of interest. However, it has not yet been fully developed for incorporating heterogeneous studies, which arise often in applications due to different study designs, populations or outcomes. For heterogeneous studies, the parameter of interest may not be estimable for certain studies, and in such a case, these studies are typically excluded from conventional meta-analysis. The exclusion of part of the studies can lead to a non-negligible loss of information. This paper introduces a metaanalysis for heterogeneous studies by combining the confidence density functions derived from the summary statistics of individual studies, hence referred to as the CD approach. It includes all the studies in the analysis and makes use of all information, direct as well as indirect. Under a general likelihood inference framework, this new approach is shown to have several desirable properties, including: i) it is asymptotically as efficient as the maximum likelihood approach using individual participant data (IPD) from all studies; ii) unlike the IPD analysis, it suffices to use summary statistics to carry out the CD approach. Individual-level data are not required; and iii) it is robust against misspecification of the working covariance structure of the parameter estimates. Besides its own theoretical significance, the last property also substantially broadens the applicability of the CD approach. All the properties of the CD approach are further confirmed by data simulated from a randomized clinical trials setting as well as by real data on aircraft landing performance. Overall, one obtains an unifying approach for combining summary statistics, subsuming many of the existing meta-analysis methods as special cases.","309":"We propose small-variance asymptotic approximations for inference on tumor heterogeneity (TH) using next-generation sequencing data. Understanding TH is an important and open research problem in biology. The lack of appropriate statistical inference is a critical gap in existing methods that the proposed approach aims to fill. We build on a hierarchical model with an exponential family likelihood and a feature allocation prior. The proposed implementation of posterior inference generalizes similar small-variance approximations proposed by Kulis and Jordan (2012) and Broderick et.al (2012b) for inference with Dirichlet process mixture and Indian buffet process prior models under normal sampling. We show that the new algorithm can successfully recover latent structures of different haplotypes and subclones and is magnitudes faster than available Markov chain Monte Carlo samplers. The latter are practically infeasible for high-dimensional genomics data. The proposed approach is scalable, easy to implement and benefits from the exibility of Bayesian nonparametric models. More importantly, it provides a useful tool for applied scientists to estimate cell subtypes in tumor samples. R code is available on http:\/\/www.ma.utexas.edu\/users\/yxu\/.","310":"This paper introduces semiparametric relative-risk regression models for infectious disease data. The units of analysis in these models are pairs of individuals at risk of transmission. The hazard of infectious contact from i to j consists of a baseline hazard multiplied by a relative risk function that can be a function of infectiousness covariates for i, susceptibliity covariates for j, and pairwise covariates. When who-infects-whom is observed, we derive a profile likelihood maximized over all possible baseline hazard functions that is similar to the Cox partial likelihood. When who-infects-whom is not observed, we derive an EM algorithm to maximize the profile likelihood integrated over all possible combinations of who-infected-whom. This extends the most important class of regression models in survival analysis to infectious disease epidemiology. These methods can be implemented in standard statistical software, and they will be able to address important scientific questions about emerging infectious diseases with greater clarity, flexibility, and rigor than current statistical methods allow.","311":"Response-adaptive designs have recently attracted more and more attention in the literature because of its advantages in efficiency and medical ethics. To develop personalized medicine, covariate information plays an important role in both design and analysis of clinical trials. A challenge is how to incorporate covariate information in response-adaptive designs while considering issues of both efficiency and medical ethics. To address this problem, we propose a new and unified family of covariate-adjusted response-adaptive (CARA) designs based on two general measurements of efficiency and ethics. Important properties (including asymptotic properties) of the proposed procedures are studied under categorical covariates. This new family of designs not only introduces new desirable CARA designs, but also unifies several important designs in the literature. We demonstrate the proposed procedures through examples, simulations, and a discussion of related earlier work.","312":"We consider the problem of comparing sizes and shapes of objects when landmark data are prone to measurement error. We show that naive implementation of ordinary Procrustes analysis that ignores measurement error can compromise inference. To account for measurement error, we propose the conditional score method for matching configurations, which guarantees consistent inference under mild model assumptions. The effects of measurement error on inference from naive Procrustes analysis and the performance of the proposed method are illustrated via simulation and application in three real data examples. Supplementary materials for this article are available online.","313":"This paper explores the homogeneity of coefficients in high-dimensional regression, which extends the sparsity concept and is more general and suitable for many applications. Homogeneity arises when regression coefficients corresponding to neighboring geographical regions or a similar cluster of covariates are expected to be approximately the same. Sparsity corresponds to a special case of homogeneity with a large cluster of known atom zero. In this article, we propose a new method called clustering algorithm in regression via data-driven segmentation (CARDS) to explore homogeneity. New mathematics are provided on the gain that can be achieved by exploring homogeneity. Statistical properties of two versions of CARDS are analyzed. In particular, the asymptotic normality of our proposed CARDS estimator is established, which reveals better estimation accuracy for homogeneous parameters than that without homogeneity exploration. When our methods are combined with sparsity exploration, further efficiency can be achieved beyond the exploration of sparsity alone. This provides additional insights into the power of exploring low-dimensional structures in high-dimensional regression: homogeneity and sparsity. Our results also shed lights on the properties of the fussed Lasso. The newly developed method is further illustrated by simulation studies and applications to real data. Supplementary materials for this article are available online.","314":"In this paper, we propose a Bayesian approach to inference on multiple Gaussian graphical models. Specifically, we address the problem of inferring multiple undirected networks in situations where some of the networks may be unrelated, while others share common features. We link the estimation of the graph structures via a Markov random field (MRF) prior which encourages common edges. We learn which sample groups have a shared graph structure by placing a spike-and-slab prior on the parameters that measure network relatedness. This approach allows us to share information between sample groups, when appropriate, as well as to obtain a measure of relative network similarity across groups. Our modeling framework incorporates relevant prior knowledge through an edge-specific informative prior and can encourage similarity to an established network. Through simulations, we demonstrate the utility of our method in summarizing relative network similarity and compare its performance against related methods. We find improved accuracy of network estimation, particularly when the sample sizes within each subgroup are moderate. We also illustrate the application of our model to infer protein networks for various cancer subtypes and under different experimental conditions.","315":"A fundamental problem in wildlife ecology and management is estimation of population size or density. The two dominant methods in this area are capture-recapture (CR) and distance sampling (DS), each with its own largely separate literature. We develop a class of models that synthesizes them. It accommodates a spectrum of models ranging from nonspatial CR models (with no information on animal locations) through to DS and mark-recapture distance sampling (MRDS) models, in which animal locations are observed without error. Between these lie spatially explicit capture-recapture (SECR) models that include only capture locations, and a variety of models with less location data than are typical of DS surveys but more than are normally used on SECR surveys. In addition to unifying CR and DS models, the class provides a means of improving inference from SECR models by adding supplementary location data, and a means of incorporating measurement error into DS and MRDS models. We illustrate their utility by comparing inference on acoustic surveys of gibbons and frogs using only capture locations, using estimated angles (gibbons) and combinations of received signal strength and time-of-arrival data (frogs), and on a visual MRDS survey of whales, comparing estimates with exact and estimated distances. Supplementary materials for this article are available online.","316":"We introduce a dynamic directional model (DDM) for studying brain effective connectivity based on intracranial electrocorticographic (ECoG) time series. The DDM consists of two parts: a set of differential equations describing neuronal activity of brain components (state equations), and observation equations linking the underlying neuronal states to observed data. When applied to functional MRI or EEG data, DDMs usually have complex formulations and thus can accommodate only a few regions, due to limitations in spatial resolution and\/or temporal resolution of these imaging modalities. In contrast, we formulate our model in the context of ECoG data. The combined high temporal and spatial resolution of ECoG data result in a much simpler DDM, allowing investigation of complex connections between many regions. To identify functionally segregated sub-networks, a form of biologically economical brain networks, we propose the Potts model for the DDM parameters. The neuronal states of brain components are represented by cubic spline bases and the parameters are estimated by minimizing a log-likelihood criterion that combines the state and observation equations. The Potts model is converted to the Potts penalty in the penalized regression approach to achieve sparsity in parameter estimation, for which a fast iterative algorithm is developed. The methods are applied to an auditory ECoG dataset.","317":"We consider the problem of quantifying the degree of coordination between transcription and translation, in yeast. Several studies have reported a surprising lack of coordination over the years, in organisms as different as yeast and human, using diverse technologies. However, a close look at this literature suggests that the lack of reported correlation may not reflect the biology of regulation. These reports do not control for between-study biases and structure in the measurement errors, ignore key aspects of how the data connect to the estimand, and systematically underestimate the correlation as a consequence. Here, we design a careful meta-analysis of 27 yeast data sets, supported by a multilevel model, full uncertainty quantification, a suite of sensitivity analyses and novel theory, to produce a more accurate estimate of the correlation between mRNA and protein levels-a proxy for coordination. From a statistical perspective, this problem motivates new theory on the impact of noise, model mis-specifications and non-ignorable missing data on estimates of the correlation between high dimensional responses. We find that the correlation between mRNA and protein levels is quite high under the studied conditions, in yeast, suggesting that post-transcriptional regulation plays a less prominent role than previously thought.","318":"Many popular Bayesian nonparametric priors can be characterized in terms of exchangeable species sampling sequences. However, in some applications, exchangeability may not be appropriate. We introduce a novel and probabilistically coherent family of non-exchangeable species sampling sequences characterized by a tractable predictive probability function with weights driven by a sequence of independent Beta random variables. We compare their theoretical clustering properties with those of the Dirichlet Process and the two parameters Poisson-Dirichlet process. The proposed construction provides a complete characterization of the joint process, differently from existing work. We then propose the use of such process as prior distribution in a hierarchical Bayes modeling framework, and we describe a Markov Chain Monte Carlo sampler for posterior inference. We evaluate the performance of the prior and the robustness of the resulting inference in a simulation study, providing a comparison with popular Dirichlet Processes mixtures and Hidden Markov Models. Finally, we develop an application to the detection of chromosomal aberrations in breast cancer by leveraging array CGH data.","319":"We consider a setting in which we have a treatment and a potentially large number of covariates for a set of observations, and wish to model their relationship with an outcome of interest. We propose a simple method for modeling interactions between the treatment and covariates. The idea is to modify the covariate in a simple way, and then fit a standard model using the modified covariates and no main effects. We show that coupled with an efficiency augmentation procedure, this method produces clinically meaningful estimators in a variety of settings. It can be useful for practicing personalized medicine: determining from a large set of biomarkers the subset of patients that can potentially benefit from a treatment. We apply the method to both simulated datasets and real trial data. The modified covariates idea can be used for other purposes, for example, large scale hypothesis testing for determining which of a set of covariates interact with a treatment variable.","320":"The semiparametric accelerated failure time (AFT) model is one of the most popular models for analyzing time-to-event outcomes. One appealing feature of the AFT model is that the observed failure time data can be transformed to identically independent distributed random variables without covariate effects. We describe a class of estimating equations based on the score functions for the transformed data, which are derived from the full likelihood function under commonly used semiparametric models such as the proportional hazards or proportional odds model. The methods of estimating regression parameters under the AFT model can be applied to traditional right-censored survival data as well as more complex time-to-event data subject to length-biased sampling. We establish the asymptotic properties and evaluate the small sample performance of the proposed estimators. We illustrate the proposed methods through applications in two examples.","321":"Copy number variants (CNVs) and single nucleotide polymorphisms (SNPs) co-exist throughout the human genome and jointly contribute to phenotypic variations. Thus, it is desirable to consider both types of variants, as characterized by allele-specific copy numbers (ASCNs), in association studies of complex human diseases. Current SNP genotyping technologies capture the CNV and SNP information simultaneously via fluorescent intensity measurements. The common practice of calling ASCNs from the intensity measurements and then using the ASCN calls in downstream association analysis has important limitations. First, the association tests are prone to false-positive findings when differential measurement errors between cases and controls arise from differences in DNA quality or handling. Second, the uncertainties in the ASCN calls are ignored. We present a general framework for the integrated analysis of CNVs and SNPs, including the analysis of total copy numbers as a special case. Our approach combines the ASCN calling and the association analysis into a single step while allowing for differential measurement errors. We construct likelihood functions that properly account for case-control sampling and measurement errors. We establish the asymptotic properties of the maximum likelihood estimators and develop EM algorithms to implement the corresponding inference procedures. The advantages of the proposed methods over the existing ones are demonstrated through realistic simulation studies and an application to a genome-wide association study of schizophrenia. Extensions to next-generation sequencing data are discussed.","322":"We introduce a class of scalar-on-function regression models with subject-specific functional predictor domains. The fundamental idea is to consider a bivariate functional parameter that depends both on the functional argument and on the width of the functional predictor domain. Both parametric and nonparametric models are introduced to fit the functional coefficient. The nonparametric model is theoretically and practically invariant to functional support transformation, or support registration. Methods were motivated by and applied to a study of association between daily measures of the Intensive Care Unit (ICU) Sequential Organ Failure Assessment (SOFA) score and two outcomes: in-hospital mortality, and physical impairment at hospital discharge among survivors. Methods are generally applicable to a large number of new studies that record a continuous variables over unequal domains.","323":"In biomedical studies, covariates with measurement error may occur in survival data. Existing approaches mostly require certain replications on the error-contaminated covariates, which may not be available in the data. In this paper, we develop a simple nonparametric correction approach for estimation of the regression parameters in the proportional hazards model using a subset of the sample where instrumental variables are observed. The instrumental variables are related to the covariates through a general nonparametric model, and no distributional assumptions are placed on the error and the underlying true covariates. We further propose a novel generalized methods of moments nonparametric correction estimator to improve the efficiency over the simple correction approach. The efficiency gain can be substantial when the calibration subsample is small compared to the whole sample. The estimators are shown to be consistent and asymptotically normal. Performance of the estimators is evaluated via simulation studies and by an application to data from an HIV clinical trial. Estimation of the baseline hazard function is not addressed.","324":"Gaussian graphical models are useful to analyze and visualize conditional dependence relationships between interacting units. Motivated from network analysis under di erent experimental conditions, such as gene networks for disparate cancer subtypes, we model structural changes over multiple networks with possible heterogeneities. In particular, we estimate multiple precision matrices describing dependencies among interacting units through maximum penalized likelihood. Of particular interest are homogeneous groups of similar entries across and zero-entries of these matrices, referred to as clustering and sparseness structures, respectively. A non-convex method is proposed to seek a sparse representation for each matrix and identify clusters of the entries across the matrices. Computationally, we develop an e cient method on the basis of di erence convex programming, the augmented Lagrangian method and the block-wise coordinate descent method, which is scalable to hundreds of graphs of thousands nodes through a simple necessary and sufficient partition rule, which divides nodes into smaller disjoint subproblems excluding zero-coe cients nodes for arbitrary graphs with convex relaxation. Theoretically, a finite-sample error bound is derived for the proposed method to reconstruct the clustering and sparseness structures. This leads to consistent reconstruction of these two structures simultaneously, permitting the number of unknown parameters to be exponential in the sample size, and yielding the optimal performance of the oracle estimator as if the true structures were given a priori. Simulation studies suggest that the method enjoys the benefit of pursuing these two disparate kinds of structures, and compares favorably against its convex counterpart in the accuracy of structure pursuit and parameter estimation.","325":"In genome-wide association studies, the primary task is to detect biomarkers in the form of Single Nucleotide Polymorphisms (SNPs) that have nontrivial associations with a disease phenotype and some other important clinical\/environmental factors. However, the extremely large number of SNPs comparing to the sample size inhibits application of classical methods such as the multiple logistic regression. Currently the most commonly used approach is still to analyze one SNP at a time. In this paper, we propose to consider the genotypes of the SNPs simultaneously via a logistic analysis of variance (ANOVA) model, which expresses the logit transformed mean of SNP genotypes as the summation of the SNP effects, effects of the disease phenotype and\/or other clinical variables, and the interaction effects. We use a reduced-rank representation of the interaction-effect matrix for dimensionality reduction, and employ the L1-penalty in a penalized likelihood framework to filter out the SNPs that have no associations. We develop a Majorization-Minimization algorithm for computational implementation. In addition, we propose a modified BIC criterion to select the penalty parameters and determine the rank number. The proposed method is applied to a Multiple Sclerosis data set and simulated data sets and shows promise in biomarker detection.","326":null,"327":"Great strides have been made in the field of reconstructing past temperatures based on models relating temperature to temperature-sensitive paleoclimate proxies. One of the goals of such reconstructions is to assess if current climate is anomalous in a millennial context. These regression based approaches model the conditional mean of the temperature distribution as a function of paleoclimate proxies (or vice versa). Some of the recent focus in the area has considered methods which help reduce the uncertainty inherent in such statistical paleoclimate reconstructions, with the ultimate goal of improving the confidence that can be attached to such endeavors. A second important scientific focus in the subject area is the area of forward models for proxies, the goal of which is to understand the way paleoclimate proxies are driven by temperature and other environmental variables. One of the primary contributions of this paper is novel statistical methodology for (1) quantile regression with autoregressive residual structure, (2) estimation of corresponding model parameters, (3) development of a rigorous framework for specifying uncertainty estimates of quantities of interest, yielding (4) statistical byproducts that address the two scientific foci discussed above. We show that by using the above statistical methodology we can demonstrably produce a more robust reconstruction than is possible by using conditional-mean-fitting methods. Our reconstruction shares some of the common features of past reconstructions, but we also gain useful insights. More importantly, we are able to demonstrate a significantly smaller uncertainty than that from previous regression methods. In addition, the quantile regression component allows us to model, in a more complete and flexible way than least squares, the conditional distribution of temperature given proxies. This relationship can be used to inform forward models relating how proxies are driven by temperature.","328":"We propose new, optimal methods for analyzing randomized trials, when it is suspected that treatment effects may differ in two predefined subpopulations. Such subpopulations could be defined by a biomarker or risk factor measured at baseline. The goal is to simultaneously learn which subpopulations benefit from an experimental treatment, while providing strong control of the familywise Type I error rate. We formalize this as a multiple testing problem and show it is computationally infeasible to solve using existing techniques. Our solution involves a novel approach, in which we first transform the original multiple testing problem into a large, sparse linear program. We then solve this problem using advanced optimization techniques. This general method can solve a variety of multiple testing problems and decision theory problems related to optimal trial design, for which no solution was previously available. In particular, we construct new multiple testing procedures that satisfy minimax and Bayes optimality criteria. For a given optimality criterion, our new approach yields the optimal tradeoff between power to detect an effect in the overall population versus power to detect effects in subpopulations. We demonstrate our approach in examples motivated by two randomized trials of new treatments for HIV.","329":"It has become routine to collect data that are structured as multiway arrays (tensors). There is an enormous literature on low rank and sparse matrix factorizations, but limited consideration of extensions to the tensor case in statistics. The most common low rank tensor factorization relies on parallel factor analysis (PARAFAC), which expresses a rank k tensor as a sum of rank one tensors. When observations are only available for a tiny subset of the cells of a big tensor, the low rank assumption is not sufficient and PARAFAC has poor performance. We induce an additional layer of dimension reduction by allowing the effective rank to vary across dimensions of the table. For concreteness, we focus on a contingency table application. Taking a Bayesian approach, we place priors on terms in the factorization and develop an efficient Gibbs sampler for posterior computation. Theory is provided showing posterior concentration rates in high-dimensional settings, and the methods are shown to have excellent performance in simulations and several real data applications.","330":"Expression quantitative trait loci (eQTLs) are genomic locations associated with changes of expression levels of certain genes. By assaying gene expressions and genetic variations simultaneously on a genome-wide scale, scientists wish to discover genomic loci responsible for expression variations of a set of genes. The task can be viewed as a multivariate regression problem with variable selection on both responses (gene expression) and covariates (genetic variations), including also multi-way interactions among covariates. Instead of learning a predictive model of quantitative trait given combinations of genetic markers, we adopt an inverse modeling perspective to model the distribution of genetic markers conditional on gene expression traits. A particular strength of our method is its ability to detect interactive effects of genetic variations with high power even when their marginal effects are weak, addressing a key weakness of many existing eQTL mapping methods. Furthermore, we introduce a hierarchical model to capture the dependence structure among correlated genes. Through simulation studies and a real data example in yeast, we demonstrate how our Bayesian hierarchical partition model achieves a significantly improved power in detecting eQTLs compared to existing methods.","331":"When conducting a randomized experiment, if an allocation yields treatment groups that differ meaningfully with respect to relevant covariates, groups should be rerandomized. The process involves specifying an explicit criterion for whether an allocation is acceptable, based on a measure of covariate balance, and rerandomizing units until an acceptable allocation is obtained. Here we illustrate how rerandomization could have improved the design of an already conducted randomized experiment on vocabulary and mathematics training programs, then provide a rerandomization procedure for covariates that vary in importance, and finally offer other extensions for rerandomization, including methods addressing computational efficiency. When covariates vary in a priori importance, better balance should be required for more important covariates. Rerandomization based on Mahalanobis distance preserves the joint distribution of covariates, but balances all covariates equally. Here we propose rerandomizing based on Mahalanobis distance within tiers of covariate importance. Because balancing covariates in one tier will in general also partially balance covariates in other tiers, for each subsequent tier we explicitly balance only the components orthogonal to covariates in more important tiers.","332":"In some therapeutic areas, treatment evaluation is frequently complicated by a possible placebo effect (i.e., the psychobiological effect of a patient's knowledge or belief of being treated). When a substantial placebo effect is likely to exist, it is important to distinguish the treatment and placebo effects in quantifying the clinical benefit of a new treatment. These causal effects can be formally defined in a joint causal model that includes treatment (e.g., new versus placebo) and treatmentality (i.e., a patient's belief or mentality about which treatment she or he has received) as separate exposures. Information about the treatmentality exposure can be obtained from blinding assessments, which are increasingly common in clinical trials where blinding success is in question. Assuming that treatmentality has a lagged effect and is measured at multiple time points, this article is concerned with joint evaluation of treatment and placebo effects in clinical trials with longitudinal follow-up, possibly with monotone missing data. We describe and discuss several methods adapted from the longitudinal causal inference literature, apply them to a weight loss study, and compare them in simulation experiments that mimic the weight loss study.","333":"Relational data are often represented as a square matrix, the entries of which record the relationships between pairs of objects. Many statistical methods for the analysis of such data assume some degree of similarity or dependence between objects in terms of the way they relate to each other. However, formal tests for such dependence have not been developed. We provide a test for such dependence using the framework of the matrix normal model, a type of multivariate normal distribution parameterized in terms of row- and column-specific covariance matrices. We develop a likelihood ratio test (LRT) for row and column dependence based on the observation of a single relational data matrix. We obtain a reference distribution for the LRT statistic, thereby providing an exact test for the presence of row or column correlations in a square relational data matrix. Additionally, we provide extensions of the test to accommodate common features of such data, such as undefined diagonal entries, a non-zero mean, multiple observations, and deviations from normality. Supplementary materials for this article are available online.","334":"When people in a society want to make inference about some parameter, each person may want to use data collected by other people. Information (data) exchange in social networks is usually costly, so to make reliable statistical decisions, people need to trade off the benefits and costs of information acquisition. Conflicts of interests and coordination problems will arise in the process. Classical statistics does not consider people's incentives and interactions in the data collection process. To address this imperfection, this work explores multi-agent Bayesian inference problems with a game theoretic social network model. Motivated by our interest in aggregate inference at the societal level, we propose a new concept, finite population learning, to address whether with high probability, a large fraction of people in a given finite population network can make \"good\" inference. Serving as a foundation, this concept enables us to study the long run trend of aggregate inference quality as population grows.","335":"This paper investigates marginal screening for detecting the presence of significant predictors in high-dimensional regression. Screening large numbers of predictors is a challenging problem due to the non-standard limiting behavior of post-model-selected estimators. There is a common misconception that the oracle property for such estimators is a panacea, but the oracle property only holds away from the null hypothesis of interest in marginal screening. To address this difficulty, we propose an adaptive resampling test (ART). Our approach provides an alternative to the popular (yet conservative) Bonferroni method of controlling familywise error rates. ART is adaptive in the sense that thresholding is used to decide whether the centered percentile bootstrap applies, and otherwise adapts to the non-standard asymptotics in the tightest way possible. The performance of the approach is evaluated using a simulation study and applied to gene expression data and HIV drug resistance data.","336":"Existing estimation methods for ordinary differential equation (ODE) models are not applicable to discrete data. The generalized ODE (GODE) model is therefore proposed and investigated for the first time. We develop the likelihood-based parameter estimation and inference methods for GODE models. We propose robust computing algorithms and rigorously investigate the asymptotic properties of the proposed estimator by considering both measurement errors and numerical errors in solving ODEs. The simulation study and application of our methods to an influenza viral dynamics study suggest that the proposed methods have a superior performance in terms of accuracy over the existing ODE model estimation approach and the extended smoothing-based (ESB) method.","337":"Modeling object boundaries based on image or point cloud data is frequently necessary in medical and scientific applications ranging from detecting tumor contours for targeted radiation therapy, to the classification of organisms based on their structural information. In low-contrast images or sparse and noisy point clouds, there is often insufficient data to recover local segments of the boundary in isolation. Thus, it becomes critical to model the entire boundary in the form of a closed curve. To achieve this, we develop a Bayesian hierarchical model that expresses highly diverse 2D objects in the form of closed curves. The model is based on a novel multiscale deformation process. By relating multiple objects through a hierarchical formulation, we can successfully recover missing boundaries by borrowing structural information from similar objects at the appropriate scale. Furthermore, the model's latent parameters help interpret the population, indicating dimensions of significant structural variability and also specifying a 'central curve' that summarizes the collection. Theoretical properties of our prior are studied in specific cases and efficient Markov chain Monte Carlo methods are developed, evaluated through simulation examples and applied to panorex teeth images for modeling teeth contours and also to a brain tumor contour detection problem.","338":"The statistics literature on functional data analysis focuses primarily on flexible black-box approaches, which are designed to allow individual curves to have essentially any shape while characterizing variability. Such methods typically cannot incorporate mechanistic information, which is commonly expressed in terms of differential equations. Motivated by studies of muscle activation, we propose a nonparametric Bayesian approach that takes into account mechanistic understanding of muscle physiology. A novel class of hierarchical Gaussian processes is defined that favors curves consistent with differential equations defined on motor, damper, spring systems. A Gibbs sampler is proposed to sample from the posterior distribution and applied to a study of rats exposed to non-injurious muscle activation protocols. Although motivated by muscle force data, a parallel approach can be used to include mechanistic information in broad functional data analysis applications.","339":"Functional magnetic resonance imaging (fMRI) has facilitated major advances in understanding human brain function. Neuroscientists are interested in using fMRI to study the effects of external stimuli on brain activity and causal relationships among brain regions, but have not stated what is meant by causation or defined the effects they purport to estimate. Building on Rubin's causal model, we construct a framework for causal inference using blood oxygenation level dependent (BOLD) fMRI time series data. In the usual statistical literature on causal inference, potential outcomes, assumed to be measured without systematic error, are used to define unit and average causal effects. However, in general the potential BOLD responses are measured with stimulus dependent systematic error. Thus we define unit and average causal effects that are free of systematic error. In contrast to the usual case of a randomized experiment where adjustment for intermediate outcomes leads to biased estimates of treatment effects (Rosenbaum, 1984), here the failure to adjust for task dependent systematic error leads to biased estimates. We therefore adjust for systematic error using measured \"noise covariates\" , using a linear mixed model to estimate the effects and the systematic error. Our results are important for neuroscientists, who typically do not adjust for systematic error. They should also prove useful to researchers in other areas where responses are measured with error and in fields where large amounts of data are collected on relatively few subjects. To illustrate our approach, we re-analyze data from a social evaluative threat task, comparing the findings with results that ignore systematic error.","340":"Motivated by recent work on studying massive imaging data in various neuroimaging studies, we propose a novel spatially varying coefficient model (SVCM) to capture the varying association between imaging measures in a three-dimensional (3D) volume (or 2D surface) with a set of covariates. Two stylized features of neuorimaging data are the presence of multiple piecewise smooth regions with unknown edges and jumps and substantial spatial correlations. To specifically account for these two features, SVCM includes a measurement model with multiple varying coefficient functions, a jumping surface model for each varying coefficient function, and a functional principal component model. We develop a three-stage estimation procedure to simultaneously estimate the varying coefficient functions and the spatial correlations. The estimation procedure includes a fast multiscale adaptive estimation and testing procedure to independently estimate each varying coefficient function, while preserving its edges among different piecewise-smooth regions. We systematically investigate the asymptotic properties (e.g., consistency and asymptotic normality) of the multiscale adaptive parameter estimates. We also establish the uniform convergence rate of the estimated spatial covariance function and its associated eigenvalues and eigenfunctions. Our Monte Carlo simulation and real data analysis have confirmed the excellent performance of SVCM.","341":"","342":null,"343":"Feature selection is fundamental for modeling the high dimensional data, where the number of features can be huge and much larger than the sample size. Since the feature space is so large, many traditional procedures become numerically infeasible. It is hence essential to first remove most apparently non-influential features before any elaborative analysis. Recently, several procedures have been developed for this purpose, which include the sure-independent-screening (SIS) as a widely-used technique. To gain the computational efficiency, the SIS screens features based on their individual predicting power. In this paper, we propose a new screening method via the sparsity-restricted maximum likelihood estimator (SMLE). The new method naturally takes the joint effects of features in the screening process, which gives itself an edge to potentially outperform the existing methods. This conjecture is further supported by the simulation studies under a number of modeling settings. We show that the proposed method is screening consistent in the context of ultra-high-dimensional generalized linear models.","344":"Identifying replicable genetic variants for addiction has been extremely challenging. Besides the common difficulties with genome-wide association studies (GWAS), environmental factors are known to be critical to addiction, and comorbidity is widely observed. Despite the importance of environmental factors and comorbidity for addiction study, few GWAS analyses adequately considered them due to the limitations of the existing statistical methods. Although parametric methods have been developed to adjust for covariates in association analysis, difficulties arise when the traits are multivariate because there is no ready-to-use model for them. Recent nonparametric development includes U-statistics to measure the phenotype-genotype association weighted by a similarity score of covariates. However, it is not clear how to optimize the similarity score. Therefore, we propose a semiparametric method to measure the association adjusted by covariates. In our approach, the nonparametric U-statistic is adjusted by parametric estimates of propensity scores using the idea of inverse probability weighting. The new measurement is shown to be asymptotically unbiased under our null hypothesis while the previous non-weighted and weighted ones are not. Simulation results show that our test improves power as opposed to the non-weighted and two other weighted U-statistic methods, and it is particularly powerful for detecting gene-environment interactions. Finally, we apply our proposed test to the Study of Addiction: Genetics and Environment (SAGE) to identify genetic variants for addiction. Novel genetic variants are found from our analysis, which warrant further investigation in the future.","345":"A practical impediment in adaptive clinical trials is that outcomes must be observed soon enough to apply decision rules to choose treatments for new patients. For example, if outcomes take up to six weeks to evaluate and the accrual rate is one patient per week, on average three new patients will be accrued while waiting to evaluate the outcomes of the previous three patients. The question is how to treat the new patients. This logistical problem persists throughout the trial. Various ad hoc practical solutions are used, none entirely satisfactory. We focus on this problem in phase I-II clinical trials that use binary toxicity and efficacy, defined in terms of event times, to choose doses adaptively for successive cohorts. We propose a general approach to this problem that treats late-onset outcomes as missing data, uses data augmentation to impute missing outcomes from posterior predictive distributions computed from partial follow-up times and complete outcome data, and applies the design's decision rules using the completed data. We illustrate the method with two cancer trials conducted using a phase I-II design based on efficacy-toxicity trade-offs, including a computer stimulation study.","346":"In disease surveillance applications, the disease events are modeled by spatio-temporal point processes. We propose a new class of semiparametric generalized linear mixed model for such data, where the event rate is related to some known risk factors and some unknown latent random effects. We model the latent spatio-temporal process as spatially correlated functional data, and propose Poisson maximum likelihood and composite likelihood methods based on spline approximations to estimate the mean and covariance functions of the latent process. By performing functional principal component analysis to the latent process, we can better understand the correlation structure in the point process. We also propose an empirical Bayes method to predict the latent spatial random effects, which can help highlight hot areas with unusually high event rates. Under an increasing domain and increasing knots asymptotic framework, we establish the asymptotic distribution for the parametric components in the model and the asymptotic convergence rates for the functional principal component estimators. We illustrate the methodology through a simulation study and an application to the Connecticut Tumor Registry data.","347":"The Intubation-Surfactant-Extubation (INSURE) procedure is used worldwide to treat pre-term newborn infants suffering from respiratory distress syndrome, which is caused by an insufficient amount of the chemical surfactant in the lungs. With INSURE, the infant is intubated, surfactant is administered via the tube to the trachea, and at completion the infant is extubated. This improves the infant's ability to breathe and thus decreases the risk of long term neurological or motor disabilities. To perform the intubation safely, the newborn infant first must be sedated. Despite extensive experience with INSURE, there is no consensus on what sedative dose is best. This paper describes a Bayesian sequentially adaptive design for a multi-institution clinical trial to optimize the sedative dose given to pre-term infants undergoing the INSURE procedure. The design is based on three clinical outcomes, two efficacy and one adverse, using elicited numerical utilities of the eight possible elementary outcomes. A flexible Bayesian parametric trivariate dose-outcome model is assumed, with the prior derived from elicited mean outcome probabilities. Doses are chosen adaptively for successive cohorts of infants using posterior mean utilities, subject to safety and efficacy constraints. A computer simulation study of the design is presented.","348":"We propose a Bayesian generalized low rank regression model (GLRR) for the analysis of both high-dimensional responses and covariates. This development is motivated by performing searches for associations between genetic variants and brain imaging phenotypes. GLRR integrates a low rank matrix to approximate the high-dimensional regression coefficient matrix of GLRR and a dynamic factor model to model the high-dimensional covariance matrix of brain imaging phenotypes. Local hypothesis testing is developed to identify significant covariates on high-dimensional responses. Posterior computation proceeds via an efficient Markov chain Monte Carlo algorithm. A simulation study is performed to evaluate the finite sample performance of GLRR and its comparison with several competing approaches. We apply GLRR to investigate the impact of 1,071 SNPs on top 40 genes reported by AlzGene database on the volumes of 93 regions of interest (ROI) obtained from Alzheimer's Disease Neuroimaging Initiative (ADNI).","349":null,"350":"Birth-death processes (BDPs) are continuous-time Markov chains that track the number of \"particles\" in a system over time. While widely used in population biology, genetics and ecology, statistical inference of the instantaneous particle birth and death rates remains largely limited to restrictive linear BDPs in which per-particle birth and death rates are constant. Researchers often observe the number of particles at discrete times, necessitating data augmentation procedures such as expectation-maximization (EM) to find maximum likelihood estimates. For BDPs on finite state-spaces, there are powerful matrix methods for computing the conditional expectations needed for the E-step of the EM algorithm. For BDPs on infinite state-spaces, closed-form solutions for the E-step are available for some linear models, but most previous work has resorted to time-consuming simulation. Remarkably, we show that the E-step conditional expectations can be expressed as convolutions of computable transition probabilities for any general BDP with arbitrary rates. This important observation, along with a convenient continued fraction representation of the Laplace transforms of the transition probabilities, allows for novel and efficient computation of the conditional expectations for all BDPs, eliminating the need for truncation of the state-space or costly simulation. We use this insight to derive EM algorithms that yield maximum likelihood estimation for general BDPs characterized by various rate models, including generalized linear models. We show that our Laplace convolution technique outperforms competing methods when they are available and demonstrate a technique to accelerate EM algorithm convergence. We validate our approach using synthetic data and then apply our methods to cancer cell growth and estimation of mutation parameters in microsatellite evolution.","351":"The varying-coefficient model is an important class of nonparametric statistical model that allows us to examine how the effects of covariates vary with exposure variables. When the number of covariates is large, the issue of variable selection arises. In this paper, we propose and investigate marginal nonparametric screening methods to screen variables in sparse ultra-high dimensional varying-coefficient models. The proposed nonparametric independence screening (NIS) selects variables by ranking a measure of the nonparametric marginal contributions of each covariate given the exposure variable. The sure independent screening property is established under some mild technical conditions when the dimensionality is of nonpolynomial order, and the dimensionality reduction of NIS is quantified. To enhance the practical utility and finite sample performance, two data-driven iterative NIS methods are proposed for selecting thresholding parameters and variables: conditional permutation and greedy methods, resulting in Conditional-INIS and Greedy-INIS. The effectiveness and flexibility of the proposed methods are further illustrated by simulation studies and real data applications.","352":"We develop methods to accurately predict whether pre-symptomatic individuals are at risk of a disease based on their various marker profiles, which offers an opportunity for early intervention well before definitive clinical diagnosis. For many diseases, existing clinical literature may suggest the risk of disease varies with some markers of biological and etiological importance, for example age. To identify effective prediction rules using nonparametric decision functions, standard statistical learning approaches treat markers with clear biological importance (e.g., age) and other markers without prior knowledge on disease etiology interchangeably as input variables. Therefore, these approaches may be inadequate in singling out and preserving the effects from the biologically important variables, especially in the presence of potential noise markers. Using age as an example of a salient marker to receive special care in the analysis, we propose a local smoothing large margin classifier implemented with support vector machine (SVM) to construct effective age-dependent classification rules. The method adaptively adjusts age effect and separately tunes age and other markers to achieve optimal performance. We derive the asymptotic risk bound of the local smoothing SVM, and perform extensive simulation studies to compare with standard approaches. We apply the proposed method to two studies of premanifest Huntington's disease (HD) subjects and controls to construct age-sensitive predictive scores for the risk of HD and risk of receiving HD diagnosis during the study period.","353":null,"354":"We present a methodology for dealing with recent challenges in testing global hypotheses using multivariate observations. The proposed tests target situations, often arising in emerging applications of neuroimaging, where the sample size n is relatively small compared with the observations' dimension K. We employ adaptive designs allowing for sequential modifications of the test statistics adapting to accumulated data. The adaptations are optimal in the sense of maximizing the predictive power of the test at each interim analysis while still controlling the Type I error. Optimality is obtained by a general result applicable to typical adaptive design settings. Further, we prove that the potentially high-dimensional design space of the tests can be reduced to a low-dimensional projection space enabling us to perform simpler power analysis studies, including comparisons to alternative tests. We illustrate the substantial improvement in efficiency that the proposed tests can make over standard tests, especially in the case of n smaller or slightly larger than K. The methods are also studied empirically using both simulated data and data from an EEG study, where the use of prior knowledge substantially increases the power of the test. Supplementary materials for this article are available online.","355":"There is a rich literature on Bayesian variable selection for parametric models. Our focus is on generalizing methods and asymptotic theory established for mixtures of g-priors to semiparametric linear regression models having unknown residual densities. Using a Dirichlet process location mixture for the residual density, we propose a semiparametric g-prior which incorporates an unknown matrix of cluster allocation indicators. For this class of priors, posterior computation can proceed via a straightforward stochastic search variable selection algorithm. In addition, Bayes factor and variable selection consistency is shown to result under a class of proper priors on g even when the number of candidate predictors p is allowed to increase much faster than sample size n, while making sparsity assumptions on the true model size.","356":"The gene regulation network (GRN) is a high-dimensional complex system, which can be represented by various mathematical or statistical models. The ordinary differential equation (ODE) model is one of the popular dynamic GRN models. High-dimensional linear ODE models have been proposed to identify GRNs, but with a limitation of the linear regulation effect assumption. In this article, we propose a sparse additive ODE (SA-ODE) model, coupled with ODE estimation methods and adaptive group LASSO techniques, to model dynamic GRNs that could flexibly deal with nonlinear regulation effects. The asymptotic properties of the proposed method are established and simulation studies are performed to validate the proposed approach. An application example for identifying the nonlinear dynamic GRN of T-cell activation is used to illustrate the usefulness of the proposed method.","357":"Accurate and individualized risk prediction is critical for population control of chronic diseases such as cancer and cardiovascular disease. Large cohort studies provide valuable resources for building risk prediction models, as the risk factors are collected at the baseline and subjects are followed over time until disease occurrence or termination of the study. However, for rare diseases the baseline risk may not be estimated reliably based on cohort data only, due to sparse events. In this paper, we propose to make use of external information to improve efficiency for estimating time-dependent absolute risk. We derive the relationship between external disease incidence rates and the baseline risk, and incorporate the external disease incidence information into estimation of absolute risks, while allowing for potential difference of disease incidence rates between cohort and external sources. The asymptotic properties, namely, uniform consistency and weak convergence, of the proposed estimators are established. Simulation results show that the proposed estimator for absolute risk is more efficient than that based on the Breslow estimator, which does not utilize external disease incidence rates. A large cohort study, the Women's Health Initiative Observational Study, is used to illustrate the proposed method.","358":"In many applications involving functional data, prior information is available about the proportion of curves having different attributes. It is not straightforward to include such information in existing procedures for functional data analysis. Generalizing the functional Dirichlet process (FDP), we propose a class of stick-breaking priors for distributions of functions. These priors incorporate functional atoms drawn from constrained stochastic processes. The stick-breaking weights are specified to allow user-specified prior probabilities for curve attributes, with hyperpriors accommodating uncertainty. Compared with the FDP, the random distribution is enriched for curves having attributes known to be common. Theoretical properties are considered, methods are developed for posterior computation, and the approach is illustrated using data on temperature curves in menstrual cycles.","359":"Using the relationships among ridge regression, LASSO estimation, and measurement error attenuation as motivation, a new measurement-error-model-based approach to variable selection is developed. After describing the approach in the familiar context of linear regression, we apply it to the problem of variable selection in nonparametric classification, resulting in a new kernel-based classifier with LASSO-like shrinkage and variable-selection properties. Finite-sample performance of the new classification method is studied via simulation and real data examples, and consistency of the method is studied theoretically. Supplementary materials for the paper are available online.","360":"We propose a semiparametric method for conducting scale-invariant sparse principal component analysis (PCA) on high dimensional non-Gaussian data. Compared with sparse PCA, our method has weaker modeling assumption and is more robust to possible data contamination. Theoretically, the proposed method achieves a parametric rate of convergence in estimating the parameter of interests under a flexible semiparametric distribution family; Computationally, the proposed method exploits a rank-based procedure and is as efficient as sparse PCA; Empirically, our method outperforms most competing methods on both synthetic and real-world datasets.","361":"In this article, we study the power properties of quadratic-distance-based goodness-of-fit tests. First, we introduce the concept of a root kernel and discuss the considerations that enter the selection of this kernel. We derive an easy to use normal approximation to the power of quadratic distance goodness-of-fit tests and base the construction of a noncentrality index, an analogue of the traditional noncentrality parameter, on it. This leads to a method akin to the Neyman-Pearson lemma for constructing optimal kernels for specific alternatives. We then introduce a midpower analysis as a device for choosing optimal degrees of freedom for a family of alternatives of interest. Finally, we introduce a new diffusion kernel, called the Pearson-normal kernel, and study the extent to which the normal approximation to the power of tests based on this kernel is valid. Supplementary materials for this article are available online.","362":"The hypothalamic-pituitary-adrenal (HPA) axis is crucial in coping with stress and maintaining homeostasis. Hormones produced by the HPA axis exhibit both complex univariate longitudinal profiles and complex relationships among different hormones. Consequently, modeling these multivariate longitudinal hormone profiles is a challenging task. In this paper, we propose a bivariate hierarchical state space model, in which each hormone profile is modeled by a hierarchical state space model, with both population-average and subject-specific components. The bivariate model is constructed by concatenating the univariate models based on the hypothesized relationship. Because of the flexible framework of state space form, the resultant models not only can handle complex individual profiles, but also can incorporate complex relationships between two hormones, including both concurrent and feedback relationship. Estimation and inference are based on marginal likelihood and posterior means and variances. Computationally efficient Kalman filtering and smoothing algorithms are used for implementation. Application of the proposed method to a study of chronic fatigue syndrome and fibromyalgia reveals that the relationships between adrenocorticotropic hormone and cortisol in the patient group are weaker than in healthy controls.","363":"The Seychelles Child Development Study (SCDS) examines the effects of prenatal exposure to methylmercury on the functioning of the central nervous system. The SCDS data include 20 outcomes measured on 9-year old children that can be classified broadly in four outcome classes or \"domains\": cognition, memory, motor, and social behavior. Previous analyses and scientific theory suggest that these outcomes may belong to more than one of these domains, rather than only a single domain as is frequently assumed for modeling. We present a framework for examining the effects of exposure and other covariates when the outcomes may each belong to more than one domain and where we also want to learn about the assignment of outcomes to domains. Each domain is defined by a sentinel outcome which is preassigned to that domain only. All other outcomes can belong to multiple domains and are not preassigned. Our model allows exposure and covariate effects to differ across domains and across outcomes within domains, and includes random subject-specific effects which model correlations between outcomes within and across domains. We take a Bayesian MCMC approach. Results from the Seychelles study and from extensive simulations show that our model can effectively determine sparse domain assignment, and at the same time give increased power to detect overall, domain-specific and outcome-specific exposure and covariate effects relative to separate models for each endpoint. When fit to the Seychelles data, several outcomes were classified as partly belonging to domains other than their originally assigned domains. In retrospect, the new partial domain assignments are reasonable and, as we discuss, suggest important scientific insights about the nature of the outcomes. Checks of model misspecification were improved relative to a model that assumes each outcome is in a single domain.","364":"Causal inference with observational data frequently relies on the notion of the propensity score (PS) to adjust treatment comparisons for observed confounding factors. As decisions in the era of \"big data\" are increasingly reliant on large and complex collections of digital data, researchers are frequently confronted with decisions regarding which of a high-dimensional covariate set to include in the PS model in order to satisfy the assumptions necessary for estimating average causal effects. Typically, simple or ad-hoc methods are employed to arrive at a single PS model, without acknowledging the uncertainty associated with the model selection. We propose three Bayesian methods for PS variable selection and model averaging that 1) select relevant variables from a set of candidate variables to include in the PS model and 2) estimate causal treatment effects as weighted averages of estimates under different PS models. The associated weight for each PS model reflects the data-driven support for that model's ability to adjust for the necessary variables. We illustrate features of our proposed approaches with a simulation study, and ultimately use our methods to compare the effectiveness of surgical vs. nonsurgical treatment for brain tumors among 2,606 Medicare beneficiaries. Supplementary materials are available online.","365":"We propose a novel two-step procedure to combine epidemiological data obtained from diverse sources with the aim to quantify risk factors affecting the probability that an individual develops certain disease such as cancer. In the first step we derive all possible unbiased estimating functions based on a group of cases and a group of controls each time. In the second step, we combine these estimating functions efficiently in order to make full use of the information contained in data. Our approach is computationally simple and flexible. We illustrate its efficacy through simulation and apply it to investigate pancreatic cancer risks based on data obtained from the Connecticut Tumor Registry, a population-based case-control study, and the Behavioral Risk Factor Surveillance System which is a state-based system of health surveys.","366":"This paper is concerned with feature screening and variable selection for varying coefficient models with ultrahigh dimensional covariates. We propose a new feature screening procedure for these models based on conditional correlation coefficient. We systematically study the theoretical properties of the proposed procedure, and establish their sure screening property and the ranking consistency. To enhance the finite sample performance of the proposed procedure, we further develop an iterative feature screening procedure. Monte Carlo simulation studies were conducted to examine the performance of the proposed procedures. In practice, we advocate a two-stage approach for varying coefficient models. The two stage approach consists of (a) reducing the ultrahigh dimensionality by using the proposed procedure and (b) applying regularization methods for dimension-reduced varying coefficient models to make statistical inferences on the coefficient functions. We illustrate the proposed two-stage approach by a real data example.","367":"In DAE (DNA After Enrichment)-seq experiments, genomic regions related with certain biological processes are enriched\/isolated by an assay and are then sequenced on a high-throughput sequencing platform to determine their genomic positions. Statistical analysis of DAE-seq data aims to detect genomic regions with significant aggregations of isolated DNA fragments (\"enriched regions\") versus all the other regions (\"background\"). However, many confounding factors may influence DAE-seq signals. In addition, the signals in adjacent genomic regions may exhibit strong correlations, which invalidate the independence assumption employed by many existing methods. To mitigate these issues, we develop a novel Autoregressive Hidden Markov Model (AR-HMM) to account for covariates effects and violations of the independence assumption. We demonstrate that our AR-HMM leads to improved performance in identifying enriched regions in both simulated and real datasets, especially in those in epigenetic datasets with broader regions of DAE-seq signal enrichment. We also introduce a variable selection procedure in the context of the HMM\/AR-HMM where the observations are not independent and the mean value of each state-specific emission distribution is modeled by some covariates. We study the theoretical properties of this variable selection procedure and demonstrate its efficacy in simulated and real DAE-seq data. In summary, we develop several practical approaches for DAE-seq data analysis that are also applicable to more general problems in statistics.","368":"Nucleosome is the fundamental packing unit of DNA in eukaryotic cells, and its positioning plays a critical role in regulation of gene expression and chromosome functions. Using a recently developed chemical mapping method, nucleosomes can be potentially mapped with an unprecedented single-base-pair resolution. Existence of overlapping nucleosomes due to cell mixture or cell dynamics, however, causes convolution of nucleosome positioning signals. In this paper, we introduce a locally convoluted cluster model and a maximum likelihood deconvolution approach, and illustrate the effectiveness of this approach in quantification of the nucleosome positional signal in the chemical mapping data.","369":"In evaluating familial risk for disease we have two main statistical tasks: assessing the probability of carrying an inherited genetic mutation conferring higher risk; and predicting the absolute risk of developing diseases over time, for those individuals whose mutation status is known. Despite substantial progress, much remains unknown about the role of genetic and environmental risk factors, about the sources of variation in risk among families that carry high-risk mutations, and about the sources of familial aggregation beyond major Mendelian effects. These sources of heterogeneity contribute substantial variation in risk across families. In this paper we present simple and efficient methods for accounting for this variation in familial risk assessment. Our methods are based on frailty models. We implemented them in the context of generalizing Mendelian models of cancer risk, and compared our approaches to others that do not consider heterogeneity across families. Our extensive simulation study demonstrates that when predicting the risk of developing a disease over time conditional on carrier status, accounting for heterogeneity results in a substantial improvement in the area under the curve of the receiver operating characteristic. On the other hand, the improvement for carriership probability estimation is more limited. We illustrate the utility of the proposed approach through the analysis of BRCA1 and BRCA2 mutation carriers in the Washington Ashkenazi Kin-Cohort Study of Breast Cancer.","370":"In many studies with a survival outcome, it is often not feasible to fully observe the primary event of interest. This often leads to heavy censoring and thus, difficulty in efficiently estimating survival or comparing survival rates between two groups. In certain diseases, baseline covariates and the event time of non-fatal intermediate events may be associated with overall survival. In these settings, incorporating such additional information may lead to gains in efficiency in estimation of survival and testing for a difference in survival between two treatment groups. If gains in efficiency can be achieved, it may then be possible to decrease the sample size of patients required for a study to achieve a particular power level or decrease the duration of the study. Most existing methods for incorporating intermediate events and covariates to predict survival focus on estimation of relative risk parameters and\/or the joint distribution of events under semiparametric models. However, in practice, these model assumptions may not hold and hence may lead to biased estimates of the marginal survival. In this paper, we propose a semi-nonparametric two-stage procedure to estimate and compare t-year survival rates by incorporating intermediate event information observed before some landmark time, which serves as a useful approach to overcome semi-competing risks issues. In a randomized clinical trial setting, we further improve efficiency through an additional calibration step. Simulation studies demonstrate substantial potential gains in efficiency in terms of estimation and power. We illustrate our proposed procedures using an AIDS Clinical Trial Protocol 175 dataset by estimating survival and examining the difference in survival between two treatment groups: zidovudine and zidovudine plus zalcitabine.","371":"Under two-phase cohort designs, such as case-cohort and nested case-control sampling, information on observed event times, event indicators, and inexpensive covariates is collected in the first phase, and the first-phase information is used to select subjects for measurements of expensive covariates in the second phase; inexpensive covariates are also used in the data analysis to control for confounding and to evaluate interactions. This paper provides efficient estimation of semiparametric transformation models for such designs, accommodating both discrete and continuous covariates and allowing inexpensive and expensive covariates to be correlated. The estimation is based on the maximization of a modified nonparametric likelihood function through a generalization of the expectation-maximization algorithm. The resulting estimators are shown to be consistent, asymptotically normal and asymptotically efficient with easily estimated variances. Simulation studies demonstrate that the asymptotic approximations are accurate in practical situations. Empirical data from Wilms' tumor studies and the Atherosclerosis Risk in Communities (ARIC) study are presented.","372":"Recently, increasing attention has focused on making causal inference when interference is possible. In the presence of interference, treatment may have several types of effects. In this paper, we consider inference about such effects when the population consists of groups of individuals where interference is possible within groups but not between groups. A two stage randomization design is assumed where in the first stage groups are randomized to different treatment allocation strategies and in the second stage individuals are randomized to treatment or control conditional on the strategy assigned to their group in the first stage. For this design, the asymptotic distributions of estimators of the causal effects are derived when either the number of individuals per group or the number of groups grows large. Under certain homogeneity assumptions, the asymptotic distributions provide justification for Wald-type confidence intervals (CIs) and tests. Empirical results demonstrate the Wald CIs have good coverage in finite samples and are narrower than CIs based on either the Chebyshev or Hoeffding inequalities provided the number of groups is not too small. The methods are illustrated by two examples which consider the effects of cholera vaccination and an intervention to encourage voting.","373":"We develop methodology which combines statistical learning methods with generalized Markov models, thereby enhancing the former to account for time series dependence. Our methodology can accommodate very general and very long-term time dependence structures in an easily estimable and computationally tractable fashion. We apply our methodology to the scoring of sleep behavior in mice. As currently used methods are expensive, invasive, and labor intensive, there is considerable interest in high-throughput automated systems which would allow many mice to be scored cheaply and quickly. Previous efforts have been able to differentiate sleep from wakefulness, but they are unable to differentiate the rare and important state of REM sleep from non-REM sleep. Key difficulties in detecting REM are that (i) REM is much rarer than non-REM and wakefulness, (ii) REM looks similar to non-REM in terms of the observed covariates, (iii) the data are noisy, and (iv) the data contain strong time dependence structures crucial for differentiating REM from non-REM. Our new approach (i) shows improved differentiation of REM from non-REM sleep and (ii) accurately estimates aggregate quantities of sleep in our application to video-based sleep scoring of mice.","374":"Multilevel functional data is collected in many biomedical studies. For example, in a study of the effect of Nimodipine on patients with subarachnoid hemorrhage (SAH), patients underwent multiple 4-hour treatment cycles. Within each treatment cycle, subjects' vital signs were reported every 10 minutes. This data has a natural multilevel structure with treatment cycles nested within subjects and measurements nested within cycles. Most literature on nonparametric analysis of such multilevel functional data focus on conditional approaches using functional mixed effects models. However, parameters obtained from the conditional models do not have direct interpretations as population average effects. When population effects are of interest, we may employ marginal regression models. In this work, we propose marginal approaches to fit multilevel functional data through penalized spline generalized estimating equation (penalized spline GEE). The procedure is effective for modeling multilevel correlated generalized outcomes as well as continuous outcomes without suffering from numerical difficulties. We provide a variance estimator robust to misspecification of correlation structure. We investigate the large sample properties of the penalized spline GEE estimator with multilevel continuous data and show that the asymptotics falls into two categories. In the small knots scenario, the estimated mean function is asymptotically efficient when the true correlation function is used and the asymptotic bias does not depend on the working correlation matrix. In the large knots scenario, both the asymptotic bias and variance depend on the working correlation. We propose a new method to select the smoothing parameter for penalized spline GEE based on an estimate of the asymptotic mean squared error (MSE). We conduct extensive simulation studies to examine property of the proposed estimator under different correlation structures and sensitivity of the variance estimation to the choice of smoothing parameter. Finally, we apply the methods to the SAH study to evaluate a recent debate on discontinuing the use of Nimodipine in the clinical community.","375":"In a case-referent study, cases of disease are compared to non-cases with respect to their antecedent exposure to a treatment in an effort to determine whether exposure causes some cases of the disease. Because exposure is not randomly assigned in the population, as it would be if the population were a vast randomized trial, exposed and unexposed subjects may differ prior to exposure with respect to covariates that may or may not have been measured. After controlling for measured pre-exposure differences, for instance by matching, a sensitivity analysis asks about the magnitude of bias from unmeasured covariates that would need to be present to alter the conclusions of a study that presumed matching for observed covariates removes all bias. The definition of a case of disease affects sensitivity to unmeasured bias. We explore this issue using: (i) an asymptotic tool, the design sensitivity, (ii) a simulation for finite samples, and (iii) an example. Under favorable circumstances, a narrower case definition can yield an increase in the design sensitivity, and hence an increase in the power of a sensitivity analysis. Also, we discuss an adaptive method that seeks to discover the best case definition from the data at hand while controlling for multiple testing. An implementation in R is available as SensitivityCaseControl.","376":"In many applications, it is of interest to study trends over time in relationships among categorical variables, such as age group, ethnicity, religious affiliation, political party and preference for particular policies. At each time point, a sample of individuals provide responses to a set of questions, with different individuals sampled at each time. In such settings, there tends to be abundant missing data and the variables being measured may change over time. At each time point, one obtains a large sparse contingency table, with the number of cells often much larger than the number of individuals being surveyed. To borrow information across time in modeling large sparse contingency tables, we propose a Bayesian autoregressive tensor factorization approach. The proposed model relies on a probabilistic Parafac factorization of the joint pmf characterizing the categorical data distribution at each time point, with autocorrelation included across times. Efficient computational methods are developed relying on MCMC. The methods are evaluated through simulation examples and applied to social survey data.","377":"The nested case-control (NCC) design have been widely adopted as a cost-effective solution in many large cohort studies for risk assessment with expensive markers, such as the emerging biologic and genetic markers. To analyze data from NCC studies, conditional logistic regression (Goldstein and Langholz, 1992; Borgan et al., 1995) and maximum likelihood (Scheike and Juul, 2004; Zeng et al., 2006) based methods have been proposed. However, most of these methods either cannot be easily extended beyond the Cox model (Cox, 1972) or require additional modeling assumptions. More generally applicable approaches based on inverse probability weighting (IPW) have been proposed as useful alternatives (Samuelsen, 1997; Chen, 2001; Samuelsen et al., 2007). However, due to the complex correlation structure induced by repeated finite risk set sampling, interval estimation for such IPW estimators remain challenging especially when the estimation involves non-smooth objective functions or when making simultaneous inferences about functions. Standard resampling procedures such as the bootstrap cannot accommodate the correlation and thus are not directly applicable. In this paper, we propose a resampling procedure that can provide valid estimates for the distribution of a broad class of IPW estimators. Simulation results suggest that the proposed procedures perform well in settings when analytical variance estimator is infeasible to derive or gives less optimal performance. The new procedures are illustrated with data from the Framingham Offspring Study to characterize individual level cardiovascular risks over time based on the Framingham risk score, C-reactive protein (CRP) and a genetic risk score.","378":"It has been repeatedly shown that in case-control association studies, analysis of a secondary trait which ignores the original sampling scheme can produce highly biased risk estimates. Although a number of approaches have been proposed to properly analyze secondary traits, most approaches fail to reproduce the marginal logistic model assumed for the original case-control trait and\/or do not allow for interaction between secondary trait and genotype marker on primary disease risk. In addition, the flexible handling of covariates remains challenging. We present a general retrospective likelihood framework to perform association testing for both binary and continuous secondary traits which respects marginal models and incorporates the interaction term. We provide a computational algorithm, based on a reparameterized approximate profile likelihood, for obtaining the maximum likelihood (ML) estimate and its standard error for the genetic effect on secondary trait, in presence of covariates. For completeness we also present an alternative pseudo-likelihood method for handling covariates. We describe extensive simulations to evaluate the performance of the ML estimator in comparison with the pseudo-likelihood and other competing methods.","379":"Functional principal component analysis (FPCA) has become the most widely used dimension reduction tool for functional data analysis. We consider functional data measured at random, subject-specific time points, contaminated with measurement error, allowing for both sparse and dense functional data, and propose novel information criteria to select the number of principal component in such data. We propose a Bayesian information criterion based on marginal modeling that can consistently select the number of principal components for both sparse and dense functional data. For dense functional data, we also developed an Akaike information criterion (AIC) based on the expected Kullback-Leibler information under a Gaussian assumption. In connecting with factor analysis in multivariate time series data, we also consider the information criteria by Bai &amp; Ng (2002) and show that they are still consistent for dense functional data, if a prescribed undersmoothing scheme is undertaken in the FPCA algorithm. We perform intensive simulation studies and show that the proposed information criteria vastly outperform existing methods for this type of data. Surprisingly, our empirical evidence shows that our information criteria proposed for dense functional data also perform well for sparse functional data. An empirical example using colon carcinogenesis data is also provided to illustrate the results.","380":"Traditional Chinese herbal medications (TCHMs) are comprised of a multitude of compounds and the identification of their active composition is an important area of research. Chromatography provides a visual representation of a TCHM sample's composition by outputting a curve characterized by spikes corresponding to compounds in the sample. Across different experimental conditions, the location of the spikes can be shifted, preventing direct comparison of curves and forcing compound identification to be possible only within each experiment. In this article we propose a sparse semiparametric nonlinear modeling framework for the establishment of a standardized chromatographic fingerprint. Data-driven basis expansion is used to model the common shape of the curves while a parametric time warping function registers across individual curves. Penalized weighted least squares with the adaptive lasso penalty provides a unified criterion for registration, model selection, and estimation. Furthermore, the adaptive lasso estimators possess attractive sampling properties. A back-fitting algorithm is proposed for estimation. Performance is assessed through simulation and we apply the model to chromatographic data of rhubarb collected from different experimental conditions and establish a standardized fingerprint as a first step in TCHM research.","381":"A new formulation for the construction of adaptive confidence bands in non-parametric function estimation problems is proposed. Confidence bands are constructed which have size that adapts to the smoothness of the function while guaranteeing that both the relative excess mass of the function lying outside the band and the measure of the set of points where the function lies outside the band are small. It is shown that the bands adapt over a maximum range of Lipschitz classes. The adaptive confidence band can be easily implemented in standard statistical software with wavelet support. Numerical performance of the procedure is investigated using both simulated and real datasets. The numerical results agree well with the theoretical analysis. The procedure can be easily modified and used for other nonparametric function estimation models.","382":"Dementia is one of the world's major public health challenges. The lifetime risk of dementia is the proportion of individuals who ever develop dementia during their lifetime. Despite its importance to epidemiologists and policy-makers, this measure does not seem to have been estimated in the Canadian population. Data from a birth cohort study of dementia are not available. Instead, we must rely on data from the Canadian Study of Heath and Aging, a large cross-sectional study of dementia with follow-up for survival. These data present challenges because they include substantial loss to follow-up and are not representatively drawn from the target population because of structural sampling biases. A first bias is imparted by the cross-sectional sampling scheme, while a second bias is a result of stratified sampling. Estimation of the lifetime risk and related quantities in the presence of these biases has not been previously addressed in the literature. We develop and study nonparametric estimators of the lifetime risk, the remaining lifetime risk and cumulative risk at specific ages, accounting for these complexities. In particular, we reveal the fact that estimation of the lifetime risk is invariant to stratification by current age at sampling. We present simulation results validating our methodology, and provide novel facts about the epidemiology of dementia in Canada using data from the Canadian Study of Health and Aging.","383":"Partial differential equation (PDE) models are commonly used to model complex dynamic systems in applied sciences such as biology and finance. The forms of these PDE models are usually proposed by experts based on their prior knowledge and understanding of the dynamic system. Parameters in PDE models often have interesting scientific interpretations, but their values are often unknown, and need to be estimated from the measurements of the dynamic system in the present of measurement errors. Most PDEs used in practice have no analytic solutions, and can only be solved with numerical methods. Currently, methods for estimating PDE parameters require repeatedly solving PDEs numerically under thousands of candidate parameter values, and thus the computational load is high. In this article, we propose two methods to estimate parameters in PDE models: a parameter cascading method and a Bayesian approach. In both methods, the underlying dynamic process modeled with the PDE model is represented via basis function expansion. For the parameter cascading method, we develop two nested levels of optimization to estimate the PDE parameters. For the Bayesian method, we develop a joint model for data and the PDE, and develop a novel hierarchical model allowing us to employ Markov chain Monte Carlo (MCMC) techniques to make posterior inference. Simulation studies show that the Bayesian method and parameter cascading method are comparable, and both outperform other available methods in terms of estimation accuracy. The two methods are demonstrated by estimating parameters in a PDE model from LIDAR data.","384":"Motivated by an analysis of US house price index data, we propose nonparametric finite mixture of regression models. We study the identifiability issue of the proposed models, and develop an estimation procedure by employing kernel regression. We further systematically study the sampling properties of the proposed estimators, and establish their asymptotic normality. A modified EM algorithm is proposed to carry out the estimation procedure. We show that our algorithm preserves the ascent property of the EM algorithm in an asymptotic sense. Monte Carlo simulations are conducted to examine the finite sample performance of the proposed estimation procedure. An empirical analysis of the US house price index data is illustrated for the proposed methodology.","385":"A new machine learning task is introduced, called latent supervised learning, where the goal is to learn a binary classifier from continuous training labels which serve as surrogates for the unobserved class labels. A specific model is investigated where the surrogate variable arises from a two-component Gaussian mixture with unknown means and variances, and the component membership is determined by a hyperplane in the covariate space. The estimation of the separating hyperplane and the Gaussian mixture parameters forms what shall be referred to as the change-line classification problem. A data-driven sieve maximum likelihood estimator for the hyperplane is proposed, which in turn can be used to estimate the parameters of the Gaussian mixture. The estimator is shown to be consistent. Simulations as well as empirical data show the estimator has high classification accuracy.","386":"In this paper we present a Bayesian hierarchical modeling approach for imaging genetics, where the interest lies in linking brain connectivity across multiple individuals to their genetic information. We have available data from a functional magnetic resonance (fMRI) study on schizophrenia. Our goals are to identify brain regions of interest (ROIs) with discriminating activation patterns between schizophrenic patients and healthy controls, and to relate the ROIs' activations with available genetic information from single nucleotide polymorphisms (SNPs) on the subjects. For this task we develop a hierarchical mixture model that includes several innovative characteristics: it incorporates the selection of ROIs that discriminate the subjects into separate groups; it allows the mixture components to depend on selected covariates; it includes prior models that capture structural dependencies among the ROIs. Applied to the schizophrenia data set, the model leads to the simultaneous selection of a set of discriminatory ROIs and the relevant SNPs, together with the reconstruction of the correlation structure of the selected regions. To the best of our knowledge, our work represents the first attempt at a rigorous modeling strategy for imaging genetics data that incorporates all such features.","387":"When interactions are identified in analysis of covariance models it becomes important to identify values of the covariates for which there are significant differences or, more generally, significant contrasts among the group mean responses. Inferential procedures that incorporate a priori order restrictions among the group mean responses would be expected to be superior to those that ignore this information. In this paper we focus on analysis of covariance models with pre-specified order restrictions on the mean response across the levels of a grouping variable when the grouping variable may interact with model covariates. In order for the restrictions to hold in the presence of interactions, it is necessary to impose the requirement that the restrictions hold over all levels of interacting categorical covariates and across pre-specified ranges of interacting continuous covariates. The parameter estimation procedure involves solving a quadratic programming minimization problem with a carefully specified constraint matrix. Simultaneous confidence intervals for treatment group contrasts and tests for equality of the ordered group mean responses are determined by exploiting previously unconnected literature. The proposed methods are motivated by a clinical trial of the dopamine agonist pramipexole for the treatment of early-stage Parkinson's disease.","388":"Periodontal disease progression is often quantified by clinical attachment level (CAL) defined as the distance down a tooth's root that is detached from the surrounding bone. Measured at 6 locations per tooth throughout the mouth (excluding the molars), it gives rise to a dependent data set-up. These data are often reduced to a one-number summary, such as the whole mouth average or the number of observations greater than a threshold, to be used as the response in a regression to identify important covariates related to the current state of a subject's periodontal health. Rather than a simple one-number summary, we set forward to analyze all available CAL data for each subject, exploiting the presence of spatial dependence, non-stationarity, and non-normality. Also, many subjects have a considerable proportion of missing teeth which cannot be considered missing at random because periodontal disease is the leading cause of adult tooth loss. Under a Bayesian paradigm, we propose a nonparametric flexible spatial (joint) model of observed CAL and the location of missing tooth via kernel convolution methods, incorporating the aforementioned features of CAL data under a unified framework. Application of this methodology to a data set recording the periodontal health of an African-American population, as well as simulation studies reveal the gain in model fit and inference, and provides a new perspective into unraveling covariate-response relationships in presence of complexities posed by these data.","389":"Many longitudinal studies involve relating an outcome process to a set of possibly time-varying covariates, giving rise to the usual regression models for longitudinal data. When the purpose of the study is to investigate the covariate effects when experimental environment undergoes abrupt changes or to locate the periods with different levels of covariate effects, a simple and easy-to-interpret approach is to introduce change-points in regression coefficients. In this connection, we propose a semiparametric change-point regression model, in which the error process (stochastic component) is nonparametric and the baseline mean function (functional part) is completely unspecified, the observation times are allowed to be subject-specific, and the number, locations and magnitudes of change-points are unknown and need to be estimated. We further develop an estimation procedure which combines the recent advance in semiparametric analysis based on counting process argument and multiple change-points inference, and discuss its large sample properties, including consistency and asymptotic normality, under suitable regularity conditions. Simulation results show that the proposed methods work well under a variety of scenarios. An application to a real data set is also given.","390":"We propose a nonparametric Bayesian local clustering (NoB-LoC) approach for heterogeneous data. NoB-LoC implements inference for nested clusters as posterior inference under a Bayesian model. Using protein expression data as an example, the NoB-LoC model defines a protein (column) cluster as a set of proteins that give rise to the same partition of the samples (rows). In other words, the sample partitions are nested within protein clusters. The common clustering of the samples gives meaning to the protein clusters. Any pair of samples might belong to the same cluster for one protein set but to different clusters for another protein set. These local features are different from features obtained by global clustering approaches such as hierarchical clustering, which create only one partition of samples that applies for all the proteins in the data set. In addition, the NoB-LoC model is different from most other local or nested clustering methods, which define clusters based on common parameters in the sampling model. As an added and important feature, the NoB-LoC method probabilistically excludes sets of irrelevant proteins and samples that do not meaningfully co-cluster with other proteins and samples, thus improving the inference on the clustering of the remaining proteins and samples. Inference is guided by a joint probability model for all the random elements. We provide a simulation study and a motivating example to demonstrate the unique features of the NoB-LoC model.","391":"In traditional schedule or dose-schedule finding designs, patients are assumed to receive their assigned dose-schedule combination throughout the trial even though the combination may be found to have an undesirable toxicity profile, which contradicts actual clinical practice. Since no systematic approach exists to optimize intra-patient dose-schedule assignment, we propose a Phase I clinical trial design that extends existing approaches to optimize dose and schedule solely between patients by incorporating adaptive variations to dose-schedule assignments within patients as the study proceeds. Our design is based on a Bayesian non-mixture cure rate model that incorporates multiple administrations each patient receives with the per-administration dose included as a covariate. Simulations demonstrate that our design identifies safe dose and schedule combinations as well as the traditional method that does not allow for intra-patient dose-schedule reassignments, but with a larger number of patients assigned to safe combinations. Supplementary materials for this article are available online.","392":"Assessing per-protocol treatment effcacy on a time-to-event endpoint is a common objective of randomized clinical trials. The typical analysis uses the same method employed for the intention-to-treat analysis (e.g., standard survival analysis) applied to the subgroup meeting protocol adherence criteria. However, due to potential post-randomization selection bias, this analysis may mislead about treatment efficacy. Moreover, while there is extensive literature on methods for assessing causal treatment effects in compliers, these methods do not apply to a common class of trials where a) the primary objective compares survival curves, b) it is inconceivable to assign participants to be adherent and event-free before adherence is measured, and c) the exclusion restriction assumption fails to hold. HIV vaccine efficacy trials including the recent RV144 trial exemplify this class, because many primary endpoints (e.g., HIV infections) occur before adherence is measured, and nonadherent subjects who receive some of the planned immunizations may be partially protected. Therefore, we develop methods for assessing per-protocol treatment efficacy for this problem class, considering three causal estimands of interest. Because these estimands are not identifiable from the observable data, we develop nonparametric bounds and semiparametric sensitivity analysis methods that yield estimated ignorance and uncertainty intervals. The methods are applied to RV144.","393":"This article presents a new estimation method for the parameters of a time series model. We consider here composite Gaussian processes that are the sum of independent Gaussian processes which, in turn, explain an important aspect of the time series, as is the case in engineering and natural sciences. The proposed estimation method offers an alternative to classical estimation based on the likelihood, that is straightforward to implement and often the only feasible estimation method with complex models. The estimator furnishes results as the optimization of a criterion based on a standardized distance between the sample wavelet variances (WV) estimates and the model-based WV. Indeed, the WV provides a decomposition of the variance process through different scales, so that they contain the information about different features of the stochastic model. We derive the asymptotic properties of the proposed estimator for inference and perform a simulation study to compare our estimator to the MLE and the LSE with different models. We also set sufficient conditions on composite models for our estimator to be consistent, that are easy to verify. We use the new estimator to estimate the stochastic error's parameters of the sum of three first order Gauss-Markov processes by means of a sample of over 800,000 issued from gyroscopes that compose inertial navigation systems. Supplementary materials for this article are available online.","394":"In high-dimensional regression, grouping pursuit and feature selection have their own merits while complementing each other in battling the curse of dimensionality. To seek a parsimonious model, we perform simultaneous grouping pursuit and feature selection over an arbitrary undirected graph with each node corresponding to one predictor. When the corresponding nodes are reachable from each other over the graph, regression coefficients can be grouped, whose absolute values are the same or close. This is motivated from gene network analysis, where genes tend to work in groups according to their biological functionalities. Through a nonconvex penalty, we develop a computational strategy and analyze the proposed method. Theoretical analysis indicates that the proposed method reconstructs the oracle estimator, that is, the unbiased least squares estimator given the true grouping, leading to consistent reconstruction of grouping structures and informative features, as well as to optimal parameter estimation. Simulation studies suggest that the method combines the benefit of grouping pursuit with that of feature selection, and compares favorably against its competitors in selection accuracy and predictive performance. An application to eQTL data is used to illustrate the methodology, where a network is incorporated into analysis through an undirected graph.","395":"Although there is evidence that teenagers are at a high risk of crashes in the early months after licensure, the driving behavior of these teenagers is not well understood. The Naturalistic Teenage Driving Study (NTDS) is the first U.S. study to document continuous driving performance of newly-licensed teenagers during their first 18 months of licensure. Counts of kinematic events such as the number of rapid accelerations are available for each trip, and their incidence rates represent different aspects of driving behavior. We propose a hierarchical Poisson regression model incorporating over-dispersion, heterogeneity, and serial correlation as well as a semiparametric mean structure. Analysis of the NTDS data is carried out with a hierarchical Bayesian framework using reversible jump Markov chain Monte Carlo algorithms to accommodate the flexible mean structure. We show that driving with a passenger and night driving decrease kinematic events, while having risky friends increases these events. Further the within-subject variation in these events is comparable to the between-subject variation. This methodology will be useful for other intensively collected longitudinal count data, where event rates are low and interest focuses on estimating the mean and variance structure of the process. This article has online supplementary materials.","396":"The next generation of telescopes, coming on-line in the next decade, will acquire terabytes of image data each night. Collectively, these large images will contain billions of interesting objects, which astronomers call sources. One critical task for astronomers is to construct from the image data a detailed source catalog that gives the sky coordinates and other properties of all detected sources. The source catalog is the primary data product produced by most telescopes and serves as an important input for studies that build and test new astrophysical theories. To construct an accurate catalog, the sources must first be detected in the image. A variety of effective source detection algorithms exist in the astronomical literature, but few if any provide rigorous statistical control of error rates. A variety of multiple testing procedures exist in the statistical literature that can provide rigorous error control over pixelwise errors, but these do not provide control over errors at the level of sources, which is what astronomers need. In this paper, we propose a technique that is effective at source detection while providing rigorous control on source-wise error rates. We demonstrate our approach with data from the Chandra X-ray Observatory Satellite. Our method is competitive with existing astronomical methods, even finding two new sources that were missed by previous studies, while providing stronger performance guarantees and without requiring costly follow up studies that are commonly required with current techniques.","397":"When comparing a new treatment with a control in a randomized clinical study, the treatment effect is generally assessed by evaluating a summary measure over a specific study population. The success of the trial heavily depends on the choice of such a population. In this paper, we show a systematic, effective way to identify a promising population, for which the new treatment is expected to have a desired benefit, utilizing the data from a current study involving similar comparator treatments. Specifically, using the existing data, we first create a parametric scoring system as a function of multiple multiple baseline covariates to estimate subject-specific treatment differences. Based on this scoring system, we specify a desired level of treatment difference and obtain a subgroup of patients, defined as those whose estimated scores exceed this threshold. An empirically calibrated threshold-specific treatment difference curve across a range of score values is constructed. The subpopulation of patients satisfying any given level of treatment benefit can then be identified accordingly. To avoid bias due to overoptimism, we utilize a cross-training-evaluation method for implementing the above two-step procedure. We then show how to select the best scoring system among all competing models. Furthermore, for cases in which only a single pre-specified working model is involved, inference procedures are proposed for the average treatment difference over a range of score values using the entire data set, and are justified theoretically and numerically. Lastly, the proposals are illustrated with the data from two clinical trials in treating HIV and cardiovascular diseases. Note that if we are not interested in designing a new study for comparing similar treatments, the new procedure can also be quite useful for the management of future patients, so that treatment may be targeted towards those who would receive nontrivial benefits to compensate for the risk or cost of the new treatment.","398":"At both the individual and societal levels, the health and economic burden of disability in older adults is enormous in developed countries, including the U.S. Recent studies have revealed that the disablement process in older adults often comprises episodic periods of impaired functioning and periods that are relatively free of disability, amid a secular and natural trend of decline in functioning. Rather than an irreversible, progressive event that is analogous to a chronic disease, disability is better conceptualized and mathematically modeled as states that do not necessarily follow a strict linear order of good-to-bad. Statistical tools, including Markov models, which allow bidirectional transition between states, and random effects models, which allow individual-specific rate of secular decline, are pertinent. In this paper, we propose a mixed effects, multivariate, hidden Markov model to handle partially ordered disability states. The model generalizes the continuation ratio model for ordinal data in the generalized linear model literature and provides a formal framework for testing the effects of risk factors and\/or an intervention on the transitions between different disability states. Under a generalization of the proportional odds ratio assumption, the proposed model circumvents the problem of a potentially large number of parameters when the number of states and the number of covariates are substantial. We describe a maximum likelihood method for estimating the partially ordered, mixed effects model and show how the model can be applied to a longitudinal data set that consists of N = 2,903 older adults followed for 10 years in the Health Aging and Body Composition Study. We further statistically test the effects of various risk factors upon the probabilities of transition into various severe disability states. The result can be used to inform geriatric and public health science researchers who study the disablement process.","399":"Constructing classification rules for accurate diagnosis of a disorder is an important goal in medical practice. In many clinical applications, there is no clinically significant anatomical or physiological deviation exists to identify the gold standard disease status to inform development of classification algorithms. Despite absence of perfect disease class identifiers, there are usually one or more disease-informative auxiliary markers along with feature variables comprising known symptoms. Existing statistical learning approaches do not effectively draw information from auxiliary prognostic markers. We propose a large margin classification method, with particular emphasis on the support vector machine (SVM), assisted by available informative markers in order to classify disease without knowing a subject's true disease status. We view this task as statistical learning in the presence of missing data, and introduce a pseudo-EM algorithm to the classification. A major distinction with a regular EM algorithm is that we do not model the distribution of missing data given the observed feature variables either parametrically or semiparametrically. We also propose a sparse variable selection method embedded in the pseudo-EM algorithm. Theoretical examination shows that the proposed classification rule is Fisher consistent, and that under a linear rule, the proposed selection has an oracle variable selection property and the estimated coefficients are asymptotically normal. We apply the methods to build decision rules for including subjects in clinical trials of a new psychiatric disorder and present four applications to data available at the UCI Machine Learning Repository.","400":"The Canadian Study of Health and Aging (CSHA) employed a prevalent cohort design to study survival after onset of dementia, where patients with dementia were sampled and the onset time of dementia was determined retrospectively. The prevalent cohort sampling scheme favors individuals who survive longer. Thus, the observed survival times are subject to length bias. In recent years, there has been a rising interest in developing estimation procedures for prevalent cohort survival data that not only account for length bias but also actually exploit the incidence distribution of the disease to improve efficiency. This article considers semiparametric estimation of the Cox model for the time from dementia onset to death under a stationarity assumption with respect to the disease incidence. Under the stationarity condition, the semiparametric maximum likelihood estimation is expected to be fully efficient yet difficult to perform for statistical practitioners, as the likelihood depends on the baseline hazard function in a complicated way. Moreover, the asymptotic properties of the semiparametric maximum likelihood estimator are not well-studied. Motivated by the composite likelihood method (Besag 1974), we develop a composite partial likelihood method that retains the simplicity of the popular partial likelihood estimator and can be easily performed using standard statistical software. When applied to the CSHA data, the proposed method estimates a significant difference in survival between the vascular dementia group and the possible Alzheimer's disease group, while the partial likelihood method for left-truncated and right-censored data yields a greater standard error and a 95% confidence interval covering 0, thus highlighting the practical value of employing a more efficient methodology. To check the assumption of stable disease for the CSHA data, we also present new graphical and numerical tests in the article. The R code used to obtain the maximum composite partial likelihood estimator for the CSHA data is available in the online Supplementary Material, posted on the journal web site.","401":"We describe a new approach to analyze chirp syllables of free-tailed bats from two regions of Texas in which they are predominant: Austin and College Station. Our goal is to characterize any systematic regional differences in the mating chirps and assess whether individual bats have signature chirps. The data are analyzed by modeling spectrograms of the chirps as responses in a Bayesian functional mixed model. Given the variable chirp lengths, we compute the spectrograms on a relative time scale interpretable as the relative chirp position, using a variable window overlap based on chirp length. We use 2D wavelet transforms to capture correlation within the spectrogram in our modeling and obtain adaptive regularization of the estimates and inference for the regions-specific spectrograms. Our model includes random effect spectrograms at the bat level to account for correlation among chirps from the same bat, and to assess relative variability in chirp spectrograms within and between bats. The modeling of spectrograms using functional mixed models is a general approach for the analysis of replicated nonstationary time series, such as our acoustical signals, to relate aspects of the signals to various predictors, while accounting for between-signal structure. This can be done on raw spectrograms when all signals are of the same length, and can be done using spectrograms defined on a relative time scale for signals of variable length in settings where the idea of defining correspondence across signals based on relative position is sensible.","402":"Peer influence and social interactions can give rise to spillover effects in which the exposure of one individual may affect outcomes of other individuals. Even if the intervention under study occurs at the group or cluster level as in group-randomized trials, spillover effects can occur when the mediator of interest is measured at a lower level than the treatment. Evaluators who choose groups rather than individuals as experimental units in a randomized trial often anticipate that the desirable changes in targeted social behaviors will be reinforced through interference among individuals in a group exposed to the same treatment. In an empirical evaluation of the effect of a school-wide intervention on reducing individual students' depressive symptoms, schools in matched pairs were randomly assigned to the 4Rs intervention or the control condition. Class quality was hypothesized as an important mediator assessed at the classroom level. We reason that the quality of one classroom may affect outcomes of children in another classroom because children interact not simply with their classmates but also with those from other classes in the hallways or on the playground. In investigating the role of class quality as a mediator, failure to account for such spillover effects of one classroom on the outcomes of children in other classrooms can potentially result in bias and problems with interpretation. Using a counterfactual conceptualization of direct, indirect and spillover effects, we provide a framework that can accommodate issues of mediation and spillover effects in group randomized trials. We show that the total effect can be decomposed into a natural direct effect, a within-classroom mediated effect and a spillover mediated effect. We give identification conditions for each of the causal effects of interest and provide results on the consequences of ignoring \"interference\" or \"spillover effects\" when they are in fact present. Our modeling approach disentangles these effects. The analysis examines whether the 4Rs intervention has an effect on children's depressive symptoms through changing the quality of other classes as well as through changing the quality of a child's own class.","403":"Gaussian factor models have proven widely useful for parsimoniously characterizing dependence in multivariate data. There is a rich literature on their extension to mixed categorical and continuous variables, using latent Gaussian variables or through generalized latent trait models acommodating measurements in the exponential family. However, when generalizing to non-Gaussian measured variables the latent variables typically influence both the dependence structure and the form of the marginal distributions, complicating interpretation and introducing artifacts. To address this problem we propose a novel class of Bayesian Gaussian copula factor models which decouple the latent factors from the marginal distributions. A semiparametric specification for the marginals based on the extended rank likelihood yields straightforward implementation and substantial computational gains. We provide new theoretical and empirical justifications for using this likelihood in Bayesian inference. We propose new default priors for the factor loadings and develop efficient parameter-expanded Gibbs sampling for posterior computation. The methods are evaluated through simulations and applied to a dataset in political science. The models in this paper are implemented in the R package bfa.","404":"Large- and finite-sample efficiency and resistance to outliers are the key goals of robust statistics. Although often not simultaneously attainable, we develop and study a linear regression estimator that comes close. Efficiency obtains from the estimator's close connection to generalized empirical likelihood, and its favorable robustness properties are obtained by constraining the associated sum of (weighted) squared residuals. We prove maximum attainable finite-sample replacement breakdown point, and full asymptotic efficiency for normal errors. Simulation evidence shows that compared to existing robust regression estimators, the new estimator has relatively high efficiency for small sample sizes, and comparable outlier resistance. The estimator is further illustrated and compared to existing methods via application to a real data set with purported outliers.","405":"Robust variable selection procedures through penalized regression have been gaining increased attention in the literature. They can be used to perform variable selection and are expected to yield robust estimates. However, to the best of our knowledge, the robustness of those penalized regression procedures has not been well characterized. In this paper, we propose a class of penalized robust regression estimators based on exponential squared loss. The motivation for this new procedure is that it enables us to characterize its robustness that has not been done for the existing procedures, while its performance is near optimal and superior to some recently developed methods. Specifically, under defined regularity conditions, our estimators are [Formula: see text] and possess the oracle property. Importantly, we show that our estimators can achieve the highest asymptotic breakdown point of 1\/2 and that their influence functions are bounded with respect to the outliers in either the response or the covariate domain. We performed simulation studies to compare our proposed method with some recent methods, using the oracle method as the benchmark. We consider common sources of influential points. Our simulation studies reveal that our proposed method performs similarly to the oracle method in terms of the model error and the positive selection rate even in the presence of influential points. In contrast, other existing procedures have a much lower non-causal selection rate. Furthermore, we re-analyze the Boston Housing Price Dataset and the Plasma Beta-Carotene Level Dataset that are commonly used examples for regression diagnostics of influential points. Our analysis unravels the discrepancies of using our robust method versus the other penalized regression method, underscoring the importance of developing and applying robust penalized regression methods.","406":"We examine the use of fixed-effects and random-effects moment-based meta-analytic methods for analysis of binary adverse event data. Special attention is paid to the case of rare adverse events which are commonly encountered in routine practice. We study estimation of model parameters and between-study heterogeneity. In addition, we examine traditional approaches to hypothesis testing of the average treatment effect and detection of the heterogeneity of treatment effect across studies. We derive three new methods, simple (unweighted) average treatment effect estimator, a new heterogeneity estimator, and a parametric bootstrapping test for heterogeneity. We then study the statistical properties of both the traditional and new methods via simulation. We find that in general, moment-based estimators of combined treatment effects and heterogeneity are biased and the degree of bias is proportional to the rarity of the event under study. The new methods eliminate much, but not all of this bias. The various estimators and hypothesis testing methods are then compared and contrasted using an example dataset on treatment of stable coronary artery disease.","407":"We propose a unified estimation method for semiparametric linear transformation models under general biased sampling schemes. The new estimator is obtained from a set of counting process-based unbiased estimating equations, developed through introducing a general weighting scheme that offsets the sampling bias. The usual asymptotic properties, including consistency and asymptotic normality, are established under suitable regularity conditions. A closed-form formula is derived for the limiting variance and the plug-in estimator is shown to be consistent. We demonstrate the unified approach through the special cases of left truncation, length-bias, the case-cohort design and variants thereof. Simulation studies and applications to real data sets are presented.","408":"Infection and cardiovascular disease are leading causes of hospitalization and death in older patients on dialysis. Our recent work found an increase in the relative incidence of cardiovascular outcomes during the ~ 30 days after infection-related hospitalizations using the case series model, which adjusts for measured and unmeasured baseline confounders. However, a major challenge in modeling\/assessing the infection-cardiovascular risk hypothesis is that the exact time of infection, or more generally \"exposure,\" onsets cannot be ascertained based on hospitalization data. Only imprecise markers of the timing of infection onsets are available. Although there is a large literature on measurement error in the predictors in regression modeling, to date there is no work on measurement error on the timing of a time-varying exposure to our knowledge. Thus, we propose a new method, the measurement error case series (MECS) models, to account for measurement error in time-varying exposure onsets. We characterized the general nature of bias resulting from estimation that ignores measurement error and proposed a bias-corrected estimation for the MECS models. We examined in detail the accuracy of the proposed method to estimate the relative incidence. Hospitalization data from United States Renal Data System, which captures nearly all (&gt; 99%) patients with end-stage renal disease in the U.S. over time, is used to illustrate the proposed method. The results suggest that the estimate of the cardiovascular incidence following the 30 days after infections, a period where acute effects of infection on vascular endothelium may be most pronounced, is substantially attenuated in the presence of infection onset measurement error.","409":null,"410":"There is increasing interest in discovering individualized treatment rules for patients who have heterogeneous responses to treatment. In particular, one aims to find an optimal individualized treatment rule which is a deterministic function of patient specific characteristics maximizing expected clinical outcome. In this paper, we first show that estimating such an optimal treatment rule is equivalent to a classification problem where each subject is weighted proportional to his or her clinical outcome. We then propose an outcome weighted learning approach based on the support vector machine framework. We show that the resulting estimator of the treatment rule is consistent. We further obtain a finite sample bound for the difference between the expected outcome using the estimated individualized treatment rule and that of the optimal treatment rule. The performance of the proposed approach is demonstrated via simulation studies and an analysis of chronic depression data.","411":"In the present study, we consider the problem of classifying spatial data distorted by a linear transformation or convolution and contaminated by additive random noise. In this setting, we show that classifier performance can be improved if we carefully invert the data before the classifier is applied. However, the inverse transformation is not constructed so as to recover the original signal, and in fact, we show that taking the latter approach is generally inadvisable. We introduce a fully data-driven procedure based on cross-validation, and use several classifiers to illustrate numerical properties of our approach. Theoretical arguments are given in support of our claims. Our procedure is applied to data generated by light detection and ranging (Lidar) technology, where we improve on earlier approaches to classifying aerosols. This article has supplementary materials online.","412":"Current methods for reconstructing human populations of the past by age and sex are deterministic or do not formally account for measurement error. We propose a method for simultaneously estimating age-specific population counts, fertility rates, mortality rates, and net international migration flows from fragmentary data that incorporates measurement error. Inference is based on joint posterior probability distributions that yield fully probabilistic interval estimates. It is designed for the kind of data commonly collected in modern demographic surveys and censuses. Population dynamics over the period of reconstruction are modeled by embedding formal demographic accounting relationships in a Bayesian hierarchical model. Informative priors are specified for vital rates, migration rates, population counts at baseline, and their respective measurement error variances. We investigate calibration of central posterior marginal probability intervals by simulation and demonstrate the method by reconstructing the female population of Burkina Faso from 1960 to 2005. Supplementary materials for this article are available online and the method is implemented in the R package \"popReconstruct.\"","413":"","414":"It has become common for data sets to contain large numbers of variables in studies conducted in areas such as genetics, machine vision, image analysis and many others. When analyzing such data, parametric models are often too inflexible while nonparametric procedures tend to be non-robust because of insufficient data on these high dimensional spaces. This is particularly true when interest lies in building efficient classifiers in the presence of many predictor variables. When dealing with these types of data, it is often the case that most of the variability tends to lie along a few directions, or more generally along a much smaller dimensional submanifold of the data space. In this article, we propose a class of models that flexibly learn about this submanifold while simultaneously performing dimension reduction in classification. This methodology, allows the cell probabilities to vary nonparametrically based on a few coordinates expressed as linear combinations of the predictors. Also, as opposed to many black-box methods for dimensionality reduction, the proposed model is appealing in having clearly interpretable and identifiable parameters which provide insight into which predictors are important in determining accurate classification boundaries. Gibbs sampling methods are developed for posterior computation, and the methods are illustrated using simulated and real data applications.","415":"For high-dimensional data, particularly when the number of predictors greatly exceeds the sample size, selection of relevant predictors for regression is a challenging problem. Methods such as sure screening, forward selection, or penalized regressions are commonly used. Bayesian variable selection methods place prior distributions on the parameters along with a prior over model space, or equivalently, a mixture prior on the parameters having mass at zero. Since exhaustive enumeration is not feasible, posterior model probabilities are often obtained via long MCMC runs. The chosen model can depend heavily on various choices for priors and also posterior thresholds. Alternatively, we propose a conjugate prior only on the full model parameters and use sparse solutions within posterior credible regions to perform selection. These posterior credible regions often have closed-form representations, and it is shown that these sparse solutions can be computed via existing algorithms. The approach is shown to outperform common methods in the high-dimensional setting, particularly under correlation. By searching for a sparse solution within a joint credible region, consistent model selection is established. Furthermore, it is shown that, under certain conditions, the use of marginal credible intervals can give consistent selection up to the case where the dimension grows exponentially in the sample size. The proposed approach successfully accomplishes variable selection in the high-dimensional setting, while avoiding pitfalls that plague typical Bayesian variable selection methods.","416":"The current goal of initial antiretroviral (ARV) therapy is suppression of plasma human immunodeficiency virus (HIV)-1 RNA levels to below 200 copies per milliliter. A proportion of HIV-infected patients who initiate antiretroviral therapy in clinical practice or antiretroviral clinical trials either fail to suppress HIV-1 RNA or have HIV-1 RNA levels rebound on therapy. Frequently, these patients have sustained CD4 cell counts responses and limited or no clinical symptoms and, therefore, have potentially limited indications for altering therapy which they may be tolerating well despite increased viral replication. On the other hand, increased viral replication on therapy leads to selection of resistance mutations to the antiretroviral agents comprising their therapy and potentially cross-resistance to other agents in the same class decreasing the likelihood of response to subsequent antiretroviral therapy. The optimal time to switch antiretroviral therapy to ensure sustained virologic suppression and prevent clinical events in patients who have rebound in their HIV-1 RNA, yet are stable, is not known. Randomized clinical trials to compare early versus delayed switching have been difficult to design and more difficult to enroll. In some clinical trials, such as the AIDS Clinical Trials Group (ACTG) Study A5095, patients randomized to initial antiretroviral treatment combinations, who fail to suppress HIV-1 RNA or have a rebound of HIV-1 RNA on therapy are allowed to switch from the initial ARV regimen to a new regimen, based on clinician and patient decisions. We delineate a statistical framework to estimate the effect of early versus late regimen change using data from ACTG A5095 in the context of two-stage designs.In causal inference, a large class of doubly robust estimators are derived through semiparametric theory with applications to missing data problems. This class of estimators is motivated through geometric arguments and relies on large samples for good performance. By now, several authors have noted that a doubly robust estimator may be suboptimal when the outcome model is misspecified even if it is semiparametric efficient when the outcome regression model is correctly specified. Through auxiliary variables, two-stage designs, and within the contextual backdrop of our scientific problem and clinical study, we propose improved doubly robust, locally efficient estimators of a population mean and average causal effect for early versus delayed switching to second-line ARV treatment regimens. Our analysis of the ACTG A5095 data further demonstrates how methods that use auxiliary variables can improve over methods that ignore them. Using the methods developed here, we conclude that patients who switch within 8 weeks of virologic failure have better clinical outcomes, on average, than patients who delay switching to a new second-line ARV regimen after failing on the initial regimen. Ordinary statistical methods fail to find such differences. This article has online supplementary material.","417":"In recent years, a wide range of markers have become available as potential tools to predict risk or progression of disease. In addition to such biological and genetic markers, short term outcome information may be useful in predicting long term disease outcomes. When such information is available, it would be desirable to combine this along with predictive markers to improve the prediction of long term survival. Most existing methods for incorporating censored short term event information in predicting long term survival focus on modeling the disease process and are derived under restrictive parametric models in a multi-state survival setting. When such model assumptions fail to hold, the resulting prediction of long term outcomes may be invalid or inaccurate. When there is only a single discrete baseline covariate, a fully non-parametric estimation procedure to incorporate short term event time information has been previously proposed. However, such an approach is not feasible for settings with one or more continuous covariates due to the curse of dimensionality. In this paper, we propose to incorporate short term event time information along with multiple covariates collected up to a landmark point via a flexible varying-coefficient model. To evaluate and compare the prediction performance of the resulting landmark prediction rule, we use robust non-parametric procedures which do not require the correct specification of the proposed varying coefficient model. Simulation studies suggest that the proposed procedures perform well in finite samples. We illustrate them here using a dataset of post-dialysis patients with end-stage renal disease.","418":"We introduce the large portfolio selection using gross-exposure constraints. We show that with gross-exposure constraint the empirically selected optimal portfolios based on estimated covariance matrices have similar performance to the theoretical optimal ones and there is no error accumulation effect from estimation of vast covariance matrices. This gives theoretical justification to the empirical results in Jagannathan and Ma (2003). We also show that the no-short-sale portfolio can be improved by allowing some short positions. The applications to portfolio selection, tracking, and improvements are also addressed. The utility of our new approach is illustrated by simulation and empirical studies on the 100 Fama-French industrial portfolios and the 600 stocks randomly selected from Russell 3000.","419":"We propose a nested Gaussian process (nGP) as a locally adaptive prior for Bayesian nonparametric regression. Specified through a set of stochastic differential equations (SDEs), the nGP imposes a Gaussian process prior for the function's mth-order derivative. The nesting comes in through including a local instantaneous mean function, which is drawn from another Gaussian process inducing adaptivity to locally-varying smoothness. We discuss the support of the nGP prior in terms of the closure of a reproducing kernel Hilbert space, and consider theoretical properties of the posterior. The posterior mean under the nGP prior is shown to be equivalent to the minimizer of a nested penalized sum-of-squares involving penalties for both the global and local roughness of the function. Using highly-efficient Markov chain Monte Carlo for posterior inference, the proposed method performs well in simulation studies compared to several alternatives, and is scalable to massive data, illustrated through a proteomics application.","420":"This paper introduces a new approach to prediction by bringing together two different nonparametric ideas: distribution free inference and nonparametric smoothing. Specifically, we consider the problem of constructing nonparametric tolerance\/prediction sets. We start from the general conformal prediction approach and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution. The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cut-off values. Under standard smoothness conditions, we get an asymptotic efficiency result that is near optimal for a wide range of function classes. But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size. The performance of our method is investigated through simulation studies and illustrated in a real data example.","421":"Classical regression methods treat covariates as a vector and estimate a corresponding vector of regression coefficients. Modern applications in medical imaging generate covariates of more complex form such as multidimensional arrays (tensors). Traditional statistical and computational methods are proving insufficient for analysis of these high-throughput data due to their ultrahigh dimensionality as well as complex structure. In this article, we propose a new family of tensor regression models that efficiently exploit the special structure of tensor covariates. Under this framework, ultrahigh dimensionality is reduced to a manageable level, resulting in efficient estimation and prediction. A fast and highly scalable estimation algorithm is proposed for maximum likelihood estimation and its associated asymptotic properties are studied. Effectiveness of the new methods is demonstrated on both synthetic and real MRI imaging data.","422":"The World Health Organization (WHO) guidelines for monitoring the effectiveness of HIV treatment in resource-limited settings (RLS) are mostly based on clinical and immunological markers (e.g., CD4 cell counts). Recent research indicates that the guidelines are inadequate and can result in high error rates. Viral load (VL) is considered the \"gold standard\", yet its widespread use is limited by cost and infrastructure. In this paper, we propose a diagnostic algorithm that uses information from routinely-collected clinical and immunological markers to guide a selective use of VL testing for diagnosing HIV treatment failure, under the assumption that VL testing is available only at a certain portion of patient visits. Our algorithm identifies the patient sub-population, such that the use of limited VL testing on them minimizes a pre-defined risk (e.g., misdiagnosis error rate). Diagnostic properties of our proposal algorithm are assessed by simulations. For illustration, data from the Miriam Hospital Immunology Clinic (RI, USA) are analyzed.","423":"Portfolio allocation with gross-exposure constraint is an effective method to increase the efficiency and stability of portfolios selection among a vast pool of assets, as demonstrated in Fan et al. (2011). The required high-dimensional volatility matrix can be estimated by using high frequency financial data. This enables us to better adapt to the local volatilities and local correlations among vast number of assets and to increase significantly the sample size for estimating the volatility matrix. This paper studies the volatility matrix estimation using high-dimensional high-frequency data from the perspective of portfolio selection. Specifically, we propose the use of \"pairwise-refresh time\" and \"all-refresh time\" methods based on the concept of \"refresh time\" proposed by Barndorff-Nielsen et al. (2008) for estimation of vast covariance matrix and compare their merits in the portfolio selection. We establish the concentration inequalities of the estimates, which guarantee desirable properties of the estimated volatility matrix in vast asset allocation with gross exposure constraints. Extensive numerical studies are made via carefully designed simulations. Comparing with the methods based on low frequency daily data, our methods can capture the most recent trend of the time varying volatility and correlation, hence provide more accurate guidance for the portfolio allocation in the next time period. The advantage of using high-frequency data is significant in our simulation and empirical studies, which consist of 50 simulated assets and 30 constituent stocks of Dow Jones Industrial Average index.","424":"Mediation analysis is often used in the behavioral sciences to investigate the role of intermediate variables that lie on the causal path between a randomized treatment and an outcome variable. Typically, mediation is assessed using structural equation models (SEMs), with model coefficients interpreted as causal effects. In this article, we present an extension of SEMs to the functional data analysis (FDA) setting that allows the mediating variable to be a continuous function rather than a single scalar measure, thus providing the opportunity to study the functional effects of the mediator on the outcome. We provide sufficient conditions for identifying the average causal effects of the functional mediators using the extended SEM, as well as weaker conditions under which an instrumental variable estimand may be interpreted as an effect. The method is applied to data from a functional magnetic resonance imaging (fMRI) study of thermal pain that sought to determine whether activation in certain brain regions mediated the effect of applied temperature on self-reported pain. Our approach provides valuable information about the timing of the mediating effect that is not readily available when using the standard nonfunctional approach. To the best of our knowledge, this work provides the first application of causal inference to the FDA framework.","425":"","426":"Diffusion process models are widely used in science, engineering and finance. Most diffusion processes are described by stochastic differential equations in continuous time. In practice, however, data is typically only observed at discrete time points. Except for a few very special cases, no analytic form exists for the likelihood of such discretely observed data. For this reason, parametric inference is often achieved by using discrete-time approximations, with accuracy controlled through the introduction of missing data. We present a new multiresolution Bayesian framework to address the inference difficulty. The methodology relies on the use of multiple approximations and extrapolation, and is significantly faster and more accurate than known strategies based on Gibbs sampling. We apply the multiresolution approach to three data-driven inference problems - one in biophysics and two in finance - one of which features a multivariate diffusion model with an entirely unobserved component.","427":null,"428":"Statistical agencies and other organizations that disseminate data are obligated to protect data subjects' confidentiality. For example, ill-intentioned individuals might link data subjects to records in other databases by matching on common characteristics (keys). Successful links are particularly problematic for data subjects with combinations of keys that are unique in the population. Hence, as part of their assessments of disclosure risks, many data stewards estimate the probabilities that sample uniques on sets of discrete keys are also population uniques on those keys. This is typically done using log-linear modeling on the keys. However, log-linear models can yield biased estimates of cell probabilities for sparse contingency tables with many zero counts, which often occurs in databases with many keys. This bias can result in unreliable estimates of probabilities of uniqueness and, hence, misrepresentations of disclosure risks. We propose an alternative to log-linear models for datasets with sparse keys based on a Bayesian version of grade of membership (GoM) models. We present a Bayesian GoM model for multinomial variables and offer an MCMC algorithm for fitting the model. We evaluate the approach by treating data from a recent US Census Bureau public use microdata sample as a population, taking simple random samples from that population, and benchmarking estimated probabilities of uniqueness against population values. Compared to log-linear models, GoM models provide more accurate estimates of the total number of uniques in the samples. Additionally, they offer record-level predictions of uniqueness that dominate those based on log-linear models.","429":"We propose recursively imputed survival tree (RIST) regression for right-censored data. This new nonparametric regression procedure uses a novel recursive imputation approach combined with extremely randomized trees that allows significantly better use of censored data than previous tree based methods, yielding improved model fit and reduced prediction error. The proposed method can also be viewed as a type of Monte Carlo EM algorithm which generates extra diversity in the tree-based fitting process. Simulation studies and data analyses demonstrate the superior performance of RIST compared to previous methods.","430":"Ultra-high dimensional data often display heterogeneity due to either heteroscedastic variance or other forms of non-location-scale covariate effects. To accommodate heterogeneity, we advocate a more general interpretation of sparsity which assumes that only a small number of covariates influence the conditional distribution of the response variable given all candidate covariates; however, the sets of relevant covariates may differ when we consider different segments of the conditional distribution. In this framework, we investigate the methodology and theory of nonconvex penalized quantile regression in ultra-high dimension. The proposed approach has two distinctive features: (1) it enables us to explore the entire conditional distribution of the response variable given the ultra-high dimensional covariates and provides a more realistic picture of the sparsity pattern; (2) it requires substantially weaker conditions compared with alternative methods in the literature; thus, it greatly alleviates the difficulty of model checking in the ultra-high dimension. In theoretic development, it is challenging to deal with both the nonsmooth loss function and the nonconvex penalty function in ultra-high dimensional parameter space. We introduce a novel sufficient optimality condition which relies on a convex differencing representation of the penalized loss function and the subdifferential calculus. Exploring this optimality condition enables us to establish the oracle property for sparse quantile regression in the ultra-high dimension under relaxed conditions. The proposed method greatly enhances existing tools for ultra-high dimensional data analysis. Monte Carlo simulations demonstrate the usefulness of the proposed procedure. The real data example we analyzed demonstrates that the new approach reveals substantially more information compared with alternative methods.","431":"We present new statistical analyses of data arising from a clinical trial designed to compare two-stage dynamic treatment regimes (DTRs) for advanced prostate cancer. The trial protocol mandated that patients were to be initially randomized among four chemotherapies, and that those who responded poorly were to be rerandomized to one of the remaining candidate therapies. The primary aim was to compare the DTRs' overall success rates, with success defined by the occurrence of successful responses in each of two consecutive courses of the patient's therapy. Of the one hundred and fifty study participants, forty seven did not complete their therapy per the algorithm. However, thirty five of them did so for reasons that precluded further chemotherapy; i.e. toxicity and\/or progressive disease. Consequently, rather than comparing the overall success rates of the DTRs in the unrealistic event that these patients had remained on their assigned chemotherapies, we conducted an analysis that compared viable switch rules defined by the per-protocol rules but with the additional provision that patients who developed toxicity or progressive disease switch to a non-prespecified therapeutic or palliative strategy. This modification involved consideration of bivariate per-course outcomes encoding both efficacy and toxicity. We used numerical scores elicited from the trial's Principal Investigator to quantify the clinical desirability of each bivariate per-course outcome, and defined one endpoint as their average over all courses of treatment. Two other simpler sets of scores as well as log survival time also were used as endpoints. Estimation of each DTR-specific mean score was conducted using inverse probability weighted methods that assumed that missingness in the twelve remaining drop-outs was informative but explainable in that it only depended on past recorded data. We conducted additional worst-best case analyses to evaluate sensitivity of our findings to extreme departures from the explainable drop-out assumption.","432":"The proportional odds model may serve as a useful alternative to the Cox proportional hazards model to study association between covariates and their survival functions in medical studies. In this article, we study an extended proportional odds model that incorporates the so-called \"external\" time-varying covariates. In the extended model, regression parameters have a direct interpretation of comparing survival functions, without specifying the baseline survival odds function. Semiparametric and maximum likelihood estimation procedures are proposed to estimate the extended model. Our methods are demonstrated by Monte-Carlo simulations, and applied to a landmark randomized clinical trial of a short course Nevirapine (NVP) for mother-to-child transmission (MTCT) of human immunodeficiency virus type-1 (HIV-1). Additional application includes analysis of the well-known Veterans Administration (VA) Lung Cancer Trial.","433":"In applications to dependent data, first and foremost relational data, a number of discrete exponential family models has turned out to be near-degenerate and problematic in terms of Markov chain Monte Carlo simulation and statistical inference. We introduce the notion of instability with an eye to characterize, detect, and penalize discrete exponential family models that are near-degenerate and problematic in terms of Markov chain Monte Carlo simulation and statistical inference. We show that unstable discrete exponential family models are characterized by excessive sensitivity and near-degeneracy. In special cases, the subset of the natural parameter space corresponding to non-degenerate distributions and mean-value parameters far from the boundary of the mean-value parameter space turns out to be a lower-dimensional subspace of the natural parameter space. These characteristics of unstable discrete exponential family models tend to obstruct Markov chain Monte Carlo simulation and statistical inference. In applications to relational data, we show that discrete exponential family models with Markov dependence tend to be unstable and that the parameter space of some curved exponential families contains unstable subsets.","434":"This paper is concerned with screening features in ultrahigh dimensional data analysis, which has become increasingly important in diverse scientific fields. We develop a sure independence screening procedure based on the distance correlation (DC-SIS, for short). The DC-SIS can be implemented as easily as the sure independence screening procedure based on the Pearson correlation (SIS, for short) proposed by Fan and Lv (2008). However, the DC-SIS can significantly improve the SIS. Fan and Lv (2008) established the sure screening property for the SIS based on linear models, but the sure screening property is valid for the DC-SIS under more general settings including linear models. Furthermore, the implementation of the DC-SIS does not require model specification (e.g., linear model or generalized linear model) for responses or predictors. This is a very appealing property in ultrahigh dimensional data analysis. Moreover, the DC-SIS can be used directly to screen grouped predictor variables and for multivariate response variables. We establish the sure screening property for the DC-SIS, and conduct simulations to examine its finite sample performance. Numerical comparison indicates that the DC-SIS performs much better than the SIS in various models. We also illustrate the DC-SIS through a real data example.","435":"Identifying the risk factors for comorbidity is important in psychiatric research. Empirically, studies have shown that testing multiple, correlated traits simultaneously is more powerful than testing a single trait at a time in association analysis. Furthermore, for complex diseases, especially mental illnesses and behavioral disorders, the traits are often recorded in different scales such as dichotomous, ordinal and quantitative. In the absence of covariates, nonparametric association tests have been developed for multiple complex traits to study comorbidity. However, genetic studies generally contain measurements of some covariates that may affect the relationship between the risk factors of major interest (such as genes) and the outcomes. While it is relatively easy to adjust these covariates in a parametric model for quantitative traits, it is challenging for multiple complex traits with possibly different scales. In this article, we propose a nonparametric test for multiple complex traits that can adjust for covariate effects. The test aims to achieve an optimal scheme of adjustment by using a maximum statistic calculated from multiple adjusted test statistics. We derive the asymptotic null distribution of the maximum test statistic, and also propose a resampling approach, both of which can be used to assess the significance of our test. Simulations are conducted to compare the type I error and power of the nonparametric adjusted test to the unadjusted test and other existing adjusted tests. The empirical results suggest that our proposed test increases the power through adjustment for covariates when there exist environmental effects, and is more robust to model misspecifications than some existing parametric adjusted tests. We further demonstrate the advantage of our test by analyzing a data set on genetics of alcoholism.","436":"In high-dimensional data analysis, feature selection becomes one means for dimension reduction, which proceeds with parameter estimation. Concerning accuracy of selection and estimation, we study nonconvex constrained and regularized likelihoods in the presence of nuisance parameters. Theoretically, we show that constrained L(0)-likelihood and its computational surrogate are optimal in that they achieve feature selection consistency and sharp parameter estimation, under one necessary condition required for any method to be selection consistent and to achieve sharp parameter estimation. It permits up to exponentially many candidate features. Computationally, we develop difference convex methods to implement the computational surrogate through prime and dual subproblems. These results establish a central role of L(0)-constrained and regularized likelihoods in feature selection and parameter estimation involving selection. As applications of the general method and theory, we perform feature selection in linear regression and logistic regression, and estimate a precision matrix in Gaussian graphical models. In these situations, we gain a new theoretical insight and obtain favorable numerical results. Finally, we discuss an application to predict the metastasis status of breast cancer patients with their gene expression profiles.","437":"Although Bayesian nonparametric mixture models for continuous data are well developed, there is a limited literature on related approaches for count data. A common strategy is to use a mixture of Poissons, which unfortunately is quite restrictive in not accounting for distributions having variance less than the mean. Other approaches include mixing multinomials, which requires finite support, and using a Dirichlet process prior with a Poisson base measure, which does not allow smooth deviations from the Poisson. As a broad class of alternative models, we propose to use nonparametric mixtures of rounded continuous kernels. An efficient Gibbs sampler is developed for posterior computation, and a simulation study is performed to assess performance. Focusing on the rounded Gaussian case, we generalize the modeling framework to account for multivariate count data, joint modeling with continuous and categorical variables, and other complications. The methods are illustrated through applications to a developmental toxicity study and marketing data. This article has supplementary material online.","438":"Latent class models (LCMs) are used increasingly for addressing a broad variety of problems, including sparse modeling of multivariate and longitudinal data, model-based clustering, and flexible inferences on predictor effects. Typical frequentist LCMs require estimation of a single finite number of classes, which does not increase with the sample size, and have a well-known sensitivity to parametric assumptions on the distributions within a class. Bayesian nonparametric methods have been developed to allow an infinite number of classes in the general population, with the number represented in a sample increasing with sample size. In this article, we propose a new nonparametric Bayes model that allows predictors to flexibly impact the allocation to latent classes, while limiting sensitivity to parametric assumptions by allowing class-specific distributions to be unknown subject to a stochastic ordering constraint. An efficient MCMC algorithm is developed for posterior computation. The methods are validated using simulation studies and applied to the problem of ranking medical procedures in terms of the distribution of patient morbidity.","439":"Genomewide association studies have become the primary tool for discovering the genetic basis of complex human diseases. Such studies are susceptible to the confounding effects of population stratification, in that the combination of allele-frequency heterogeneity with disease-risk heterogeneity among different ancestral subpopulations can induce spurious associations between genetic variants and disease. This article provides a statistically rigorous and computationally feasible solution to this challenging problem of unmeasured confounders. We show that the odds ratio of disease with a genetic variant is identifiable if and only if the genotype is independent of the unknown population substructure conditional on a set of observed ancestry-informative markers in the disease-free population. Under this condition, the odds ratio of interest can be estimated by fitting a semiparametric logistic regression model with an arbitrary function of a propensity score relating the genotype probability to ancestry-informative markers. Approximating the unknown function of the propensity score by B-splines, we derive a consistent and asymptotically normal estimator for the odds ratio of interest with a consistent variance estimator. Simulation studies demonstrate that the proposed inference procedures perform well in realistic settings. An application to the well-known Wellcome Trust Case-Control Study is presented. Supplemental materials are available online.","440":"In longitudinal biomedical studies, there is often interest in the rate functions, which describe the functional rates of change of biomarker profiles. This paper proposes a semiparametric approach to model these functions as the realizations of stochastic processes defined by stochastic differential equations. These processes are dependent on the covariates of interest and vary around a specified parametric function. An efficient Markov chain Monte Carlo algorithm is developed for inference. The proposed method is compared with several existing methods in terms of goodness-of-fit and more importantly the ability to forecast future functional data in a simulation study. The proposed methodology is applied to prostate-specific antigen profiles for illustration. Supplementary materials for this paper are available online.","441":"The continual reassessment method (CRM) is a commonly used dose-finding design for phase I clinical trials. Practical applications of this method have been restricted by two limitations: (1) the requirement that the toxicity outcome needs to be observed shortly after the initiation of the treatment; and (2) the potential sensitivity to the prespecified toxicity probability at each dose. To overcome these limitations, we naturally treat the unobserved toxicity outcomes as missing data, and use the expectation-maximization (EM) algorithm to estimate the dose toxicity probabilities based on the incomplete data to direct dose assignment. To enhance the robustness of the design, we propose prespecifying multiple sets of toxicity probabilities, each set corresponding to an individual CRM model. We carry out these multiple CRMs in parallel, across which model selection and model averaging procedures are used to make more robust inference. We evaluate the operating characteristics of the proposed robust EM-CRM designs through simulation studies and show that the proposed methods satisfactorily resolve both limitations of the CRM. Besides improving the MTD selection percentage, the new designs dramatically shorten the duration of the trial, and are robust to the prespecification of the toxicity probabilities.","442":"We devise methods to estimate probability density functions of several populations using observations with uncertain population membership, meaning from which population an observation comes is unknown. The probability of an observation being sampled from any given population can be calculated. We develop general estimation procedures and bandwidth selection methods for our setting. We establish large-sample properties and study finite-sample performance using simulation studies. We illustrate our methods with data from a nutrition study.","443":"Gaussian latent factor models are routinely used for modeling of dependence in continuous, binary, and ordered categorical data. For unordered categorical variables, Gaussian latent factor models lead to challenging computation and complex modeling structures. As an alternative, we propose a novel class of simplex factor models. In the single-factor case, the model treats the different categorical outcomes as independent with unknown marginals. The model can characterize flexible dependence structures parsimoniously with few factors, and as factors are added, any multivariate categorical data distribution can be accurately approximated. Using a Bayesian approach for computation and inferences, a Markov chain Monte Carlo (MCMC) algorithm is proposed that scales well with increasing dimension, with the number of factors treated as unknown. We develop an efficient proposal for updating the base probability vector in hierarchical Dirichlet models. Theoretical properties are described, and we evaluate the approach through simulation examples. Applications are described for modeling dependence in nucleotide sequences and prediction from high-dimensional categorical features.","444":"The aim of this paper is to develop a semiparametric model for describing the variability of the medial representation of subcortical structures, which belongs to a Riemannian manifold, and establishing its association with covariates of interest, such as diagnostic status, age and gender. We develop a two-stage estimation procedure to calculate the parameter estimates. The first stage is to calculate an intrinsic least squares estimator of the parameter vector using the annealing evolutionary stochastic approximation Monte Carlo algorithm and then the second stage is to construct a set of estimating equations to obtain a more efficient estimate with the intrinsic least squares estimate as the starting point. We use Wald statistics to test linear hypotheses of unknown parameters and establish their limiting distributions. Simulation studies are used to evaluate the accuracy of our parameter estimates and the finite sample performance of the Wald statistics. We apply our methods to the detection of the difference in the morphological changes of the left and right hippocampi between schizophrenia patients and healthy controls using medial shape description.","445":"It is frequently of interest to estimate the intervention effect that adjusts for post-randomization variables in clinical trials. In the recently completed HPTN 035 trial, there is differential condom use between the three microbicide gel arms and the No Gel control arm, so that intention to treat (ITT) analyses only assess the net treatment effect that includes the indirect treatment effect mediated through differential condom use. Various statistical methods in causal inference have been developed to adjust for post-randomization variables. We extend the principal stratification framework to time-varying behavioral variables in HIV prevention trials with a time-to-event endpoint, using a partially hidden Markov model (pHMM). We formulate the causal estimand of interest, establish assumptions that enable identifiability of the causal parameters, and develop maximum likelihood methods for estimation. Application of our model on the HPTN 035 trial reveals an interesting pattern of prevention effectiveness among different condom-use principal strata.","446":"We consider a random effects quantile regression analysis of clustered data and propose a semiparametric approach using empirical likelihood. The random regression coefficients are assumed independent with a common mean, following parametrically specified distributions. The common mean corresponds to the population-average effects of explanatory variables on the conditional quantile of interest, while the random coefficients represent cluster specific deviations in the covariate effects. We formulate the estimation of the random coefficients as an estimating equations problem and use empirical likelihood to incorporate the parametric likelihood of the random coefficients. A likelihood-like statistical criterion function is yield, which we show is asymptotically concave in a neighborhood of the true parameter value and motivates its maximizer as a natural estimator. We use Markov Chain Monte Carlo (MCMC) samplers in the Bayesian framework, and propose the resulting quasi-posterior mean as an estimator. We show that the proposed estimator of the population-level parameter is asymptotically normal and the estimators of the random coefficients are shrunk toward the population-level parameter in the first order asymptotic sense. These asymptotic results do not require Gaussian random effects, and the empirical likelihood based likelihood-like criterion function is free of parameters related to the error densities. This makes the proposed approach both flexible and computationally simple. We illustrate the methodology with two real data examples.","447":"Length-biased sampling has been well recognized in economics, industrial reliability, etiology applications, epidemiological, genetic and cancer screening studies. Length-biased right-censored data have a unique data structure different from traditional survival data. The nonparametric and semiparametric estimations and inference methods for traditional survival data are not directly applicable for length-biased right-censored data. We propose new expectation-maximization algorithms for estimations based on full likelihoods involving infinite dimensional parameters under three settings for length-biased data: estimating nonparametric distribution function, estimating nonparametric hazard function under an increasing failure rate constraint, and jointly estimating baseline hazards function and the covariate coefficients under the Cox proportional hazards model. Extensive empirical simulation studies show that the maximum likelihood estimators perform well with moderate sample sizes and lead to more efficient estimators compared to the estimating equation approaches. The proposed estimates are also more robust to various right-censoring mechanisms. We prove the strong consistency properties of the estimators, and establish the asymptotic normality of the semi-parametric maximum likelihood estimators under the Cox model using modern empirical processes theory. We apply the proposed methods to a prevalent cohort medical study. Supplemental materials are available online.","448":"Functional data are increasingly encountered in scientific studies, and their high dimensionality and complexity lead to many analytical challenges. Various methods for functional data analysis have been developed, including functional response regression methods that involve regression of a functional response on univariate\/multivariate predictors with nonparametrically represented functional coefficients. In existing methods, however, the functional regression can be sensitive to outlying curves and outlying regions of curves, so is not robust. In this paper, we introduce a new Bayesian method, robust functional mixed models (R-FMM), for performing robust functional regression within the general functional mixed model framework, which includes multiple continuous or categorical predictors and random effect functions accommodating potential between-function correlation induced by the experimental design. The underlying model involves a hierarchical scale mixture model for the fixed effects, random effect and residual error functions. These modeling assumptions across curves result in robust nonparametric estimators of the fixed and random effect functions which down-weight outlying curves and regions of curves, and produce statistics that can be used to flag global and local outliers. These assumptions also lead to distributions across wavelet coefficients that have outstanding sparsity and adaptive shrinkage properties, with great flexibility for the data to determine the sparsity and the heaviness of the tails. Together with the down-weighting of outliers, these within-curve properties lead to fixed and random effect function estimates that appear in our simulations to be remarkably adaptive in their ability to remove spurious features yet retain true features of the functions. We have developed general code to implement this fully Bayesian method that is automatic, requiring the user to only provide the functional data and design matrices. It is efficient enough to handle large data sets, and yields posterior samples of all model parameters that can be used to perform desired Bayesian estimation and inference. Although we present details for a specific implementation of the R-FMM using specific distributional choices in the hierarchical model, 1D functions, and wavelet transforms, the method can be applied more generally using other heavy-tailed distributions, higher dimensional functions (e.g. images), and using other invertible transformations as alternatives to wavelets.","449":"A variable screening procedure via correlation learning was proposed in Fan and Lv (2008) to reduce dimensionality in sparse ultra-high dimensional models. Even when the true model is linear, the marginal regression can be highly nonlinear. To address this issue, we further extend the correlation learning to marginal nonparametric learning. Our nonparametric independence screening is called NIS, a specific member of the sure independence screening. Several closely related variable screening procedures are proposed. Under general nonparametric models, it is shown that under some mild technical conditions, the proposed independence screening methods enjoy a sure screening property. The extent to which the dimensionality can be reduced by independence screening is also explicitly quantified. As a methodological extension, a data-driven thresholding and an iterative nonparametric independence screening (INIS) are also proposed to enhance the finite sample performance for fitting sparse additive models. The simulation results and a real data analysis demonstrate that the proposed procedure works well with moderate sample size and large dimension and performs better than competing methods.","450":"We propose a general framework for performing full Bayesian analysis under linear inequality parameter constraints. The proposal is motivated by the BioCycle Study, a large cohort study of hormone levels of healthy women where certain well-established linear inequality constraints on the log-hormone levels should be accounted for in the statistical inferential procedure. Based on the Minkowski-Weyl decomposition of polyhedral regions, we propose a class of priors that are fully supported on the parameter space with linear inequality constraints, and we fit a Bayesian linear mixed model to the BioCycle data using such a prior. We observe positive associations between estrogen and progesterone levels and F2-isoprostanes, a marker for oxidative stress. These findings are of particular interest to reproductive epidemiologists.","451":"In a randomized trial, subjects are assigned to treatment or control by the flip of a fair coin. In many nonrandomized or observational studies, subjects find their way to treatment or control in two steps, either or both of which may lead to biased comparisons. By a vague process perhaps affected by proximity or sociodemographic issues, subjects find their way to institutions that provide treatment. Once at such an institution, a second process, perhaps thoughtful and deliberate, assigns individuals to treatment or control. In the current paper, the institutions are hospitals, and the treatment under study is the use of general anesthesia alone versus some use of regional anesthesia during surgery. For a specific operation, the use of regional anesthesia may be typical in one hospital and atypical in another. A new matched design is proposed for studies of this sort, one that creates two types of nonoverlapping matched pairs. Using a new extension of optimal matching with fine balance, pairs of the first type exactly balance treatment assignment across institutions, so each institution appears in the treated group with the same frequency that it appears in the control group; hence, differences between institutions that affect everyone in the same way cannot bias this comparison. Pairs of the second type compare institutions that assign most subjects to treatment and other institutions that assign most subjects to control, so each institution is represented in the treated group if it typically assigns subjects to treatment or alternatively in the control group if it typically assigns subjects to control, and no institution appears in both groups. By and large, in the second type of matched pair, subjects became treated subjects or controls by choosing an institution, not by a thoughtful and deliberate process of selecting subjects for treatment within institutions. The design provides two evidence factors, that is, two tests of the null hypothesis of no treatment effect that are independent when the null hypothesis is true, where each factor is largely unaffected by certain unmeasured biases that could readily invalidate the other factor. The two factors permit separate and combined sensitivity analyses, where the magnitude of bias affecting the two factors may differ. The case of knee surgery in the study of regional versus general anesthesia is considered in detail.","452":"An experimental unit is an opportunity to randomly apply or withhold a treatment. There is interference between units if the application of the treatment to one unit may also affect other units. In cognitive neuroscience, a common form of experiment presents a sequence of stimuli or requests for cognitive activity at random to each experimental subject and measures biological aspects of brain activity that follow these requests. Each subject is then many experimental units, and interference between units within an experimental subject is likely, in part because the stimuli follow one another quickly and in part because human subjects learn or become experienced or primed or bored as the experiment proceeds. We use a recent fMRI experiment concerned with the inhibition of motor activity to illustrate and further develop recently proposed methodology for inference in the presence of interference. A simulation evaluates the power of competing procedures.","453":"The goal of this paper is to model cognitive control related activation among predefined regions of interest (ROIs) of the human brain while properly adjusting for the underlying spatio-temporal correlations. Standard approaches to fMRI analysis do not simultaneously take into account both the spatial and temporal correlations that are prevalent in fMRI data. This is primarily due to the computational complexity of estimating the spatio-temporal covariance matrix. More specifically, they do not take into account multi-scale spatial correlation (between-ROIs and within-ROI). To address these limitations, we propose a spatio-spectral mixed effects model. Working in the spectral domain simplifies the temporal covariance structure because the Fourier coefficients are approximately uncorrelated across frequencies. Additionally, by incorporating voxel-specific and ROI-specific random effects, the model is able to capture the multi-scale spatial covariance structure: distance-dependent local correlation (within an ROI), and distance-independent global correlation (between-ROIs). Building on existing theory on linear mixed effects models to conduct estimation and inference, we applied our model to fMRI data to study activation in pre-specified ROIs in the prefontal cortex and estimate the correlation structure in the network. Simulation studies demonstrate that ignoring the multi-scale correlation leads to higher false positives.","454":"Accurate estimation of the false discovery proportion (FDP) and false discovery rate (FDR) is a problem that has been eagerly waiting for a solution. Fan, Han and Gu have found one. They have achieved this by both clarifying the concept of the problem and providing a feasible algorithmic solution. In this comment, I discuss some of the central concepts involving FDP and its estimation via conditioning in contrast with the estimators by Efron (2007) and Friguet et al. (2009).","455":"Multiple hypothesis testing is a fundamental problem in high dimensional inference, with wide applications in many scientific fields. In genome-wide association studies, tens of thousands of tests are performed simultaneously to find if any SNPs are associated with some traits and those tests are correlated. When test statistics are correlated, false discovery control becomes very challenging under arbitrary dependence. In the current paper, we propose a novel method based on principal factor approximation, which successfully subtracts the common dependence and weakens significantly the correlation structure, to deal with an arbitrary dependence structure. We derive an approximate expression for false discovery proportion (FDP) in large scale multiple testing when a common threshold is used and provide a consistent estimate of realized FDP. This result has important applications in controlling FDR and FDP. Our estimate of realized FDP compares favorably with Efron (2007)'s approach, as demonstrated in the simulated examples. Our approach is further illustrated by some real data applications. We also propose a dependence-adjusted procedure, which is more powerful than the fixed threshold procedure.","456":"In many applications the graph structure in a network arises from two sources: intrinsic connections and connections due to external effects. We introduce a sparse estimation procedure for graphical models that is capable of isolating the intrinsic connections by removing the external effects. Technically, this is formulated as a conditional graphical model, in which the external effects are modeled as predictors, and the graph is determined by the conditional precision matrix. We introduce two sparse estimators of this matrix using the reproduced kernel Hilbert space combined with lasso and adaptive lasso. We establish the sparsity, variable selection consistency, oracle property, and the asymptotic distributions of the proposed estimators. We also develop their convergence rate when the dimension of the conditional precision matrix goes to infinity. The methods are compared with sparse estimators for unconditional graphical models, and with the constrained maximum likelihood estimate that assumes a known graph structure. The methods are applied to a genetic data set to construct a gene network conditioning on single-nucleotide polymorphisms.","457":"A major aim of longitudinal analyses of life course data is to describe the within- and between-individual variability in a behavioral outcome, such as crime. Statistical analyses of such data typically draw on mixture and mixed-effects growth models. In this work, we present a functional analytic point of view and develop an alternative method that models individual crime trajectories as departures from a population age-crime curve. Drawing on empirical and theoretical claims in criminology, we assume a unimodal population age-crime curve and allow individual expected crime trajectories to differ by their levels of offending and patterns of temporal misalignment. We extend Bayesian hierarchical curve registration methods to accommodate count data and to incorporate influence of baseline covariates on individual behavioral trajectories. Analyzing self-reported counts of yearly marijuana use from the Denver Youth Survey, we examine the influence of race and gender categories on differences in levels and timing of marijuana smoking. We find that our approach offers a flexible model for longitudinal crime trajectories and allows for a rich array of inferences of interest to criminologists and drug abuse researchers.","458":"This work presents methods for estimating genotype-specific distributions from genetic epidemiology studies where the event times are subject to right censoring, the genotypes are not directly observed, and the data arise from a mixture of scientifically meaningful subpopulations. Examples of such studies include kin-cohort studies and quantitative trait locus (QTL) studies. Current methods for analyzing censored mixture data include two types of nonparametric maximum likelihood estimators (NPMLEs) which do not make parametric assumptions on the genotype-specific density functions. Although both NPMLEs are commonly used, we show that one is inefficient and the other inconsistent. To overcome these deficiencies, we propose three classes of consistent nonparametric estimators which do not assume parametric density models and are easy to implement. They are based on the inverse probability weighting (IPW), augmented IPW (AIPW), and nonparametric imputation (IMP). The AIPW achieves the efficiency bound without additional modeling assumptions. Extensive simulation experiments demonstrate satisfactory performance of these estimators even when the data are heavily censored. We apply these estimators to the Cooperative Huntington's Observational Research Trial (COHORT), and provide age-specific estimates of the effect of mutation in the Huntington gene on mortality using a sample of family members. The close approximation of the estimated non-carrier survival rates to that of the U.S. population indicates small ascertainment bias in the COHORT family sample. Our analyses underscore an elevated risk of death in Huntington gene mutation carriers compared to non-carriers for a wide age range, and suggest that the mutation equally affects survival rates in both genders. The estimated survival rates are useful in genetic counseling for providing guidelines on interpreting the risk of death associated with a positive genetic testing, and in facilitating future subjects at risk to make informed decisions on whether to undergo genetic mutation testings.","459":"","460":"Standard assumptions incorporated into Bayesian model selection procedures result in procedures that are not competitive with commonly used penalized likelihood methods. We propose modifications of these methods by imposing nonlocal prior densities on model parameters. We show that the resulting model selection procedures are consistent in linear model settings when the number of possible covariates p is bounded by the number of observations n, a property that has not been extended to other model selection procedures. In addition to consistently identifying the true model, the proposed procedures provide accurate estimates of the posterior probability that each identified model is correct. Through simulation studies, we demonstrate that these model selection procedures perform as well or better than commonly used penalized likelihood methods in a range of simulation settings. Proofs of the primary theorems are provided in the Supplementary Material that is available online.","461":"In the presence of time-varying confounders affected by prior treatment, standard statistical methods for failure time analysis may be biased. Methods that correctly adjust for this type of covariate include the parametric g-formula, inverse probability weighted estimation of marginal structural Cox proportional hazards models, and g-estimation of structural nested accelerated failure time models. In this article, we propose a novel method to estimate the causal effect of a time-dependent treatment on failure in the presence of informative right-censoring and time-dependent confounders that may be affected by past treatment: g-estimation of structural nested cumulative failure time models (SNCFTMs). An SNCFTM considers the conditional effect of a final treatment at time m on the outcome at each later time k by modeling the ratio of two counterfactual cumulative risks at time k under treatment regimes that differ only at time m. Inverse probability weights are used to adjust for informative censoring. We also present a procedure that, under certain \"no-interaction\" conditions, uses the g-estimates of the model parameters to calculate unconditional cumulative risks under nondynamic (static) treatment regimes. The procedure is illustrated with an example using data from a longitudinal cohort study, in which the \"treatments\" are healthy behaviors and the outcome is coronary heart disease.","462":"Real world networks exhibit a complex set of phenomena such as underlying hierarchical organization, multiscale interaction, and varying topologies of communities. Most existing methods do not adequately capture the intrinsic interplay among such phenomena. We propose a nonparametric Multiscale Community Blockmodel (MSCB) to model the generation of hierarchies in social communities, selective membership of actors to subsets of these communities, and the resultant networks due to within- and cross-community interactions. By using the nested Chinese Restaurant Process, our model automatically infers the hierarchy structure from the data. We develop a collapsed Gibbs sampling algorithm for posterior inference, conduct extensive validation using synthetic networks, and demonstrate the utility of our model in real-world datasets such as predator-prey networks and citation networks.","463":"We provide a novel and completely different approach to dimension-reduction problems from the existing literature. We cast the dimension-reduction problem in a semiparametric estimation framework and derive estimating equations. Viewing this problem from the new angle allows us to derive a rich class of estimators, and obtain the classical dimension reduction techniques as special cases in this class. The semiparametric approach also reveals that in the inverse regression context while keeping the estimation structure intact, the common assumption of linearity and\/or constant variance on the covariates can be removed at the cost of performing additional nonparametric regression. The semiparametric estimators without these common assumptions are illustrated through simulation studies and a real data example. This article has online supplementary material.","464":"Modeling of multivariate unordered categorical (nominal) data is a challenging problem, particularly in high dimensions and cases in which one wishes to avoid strong assumptions about the dependence structure. Commonly used approaches rely on the incorporation of latent Gaussian random variables or parametric latent class models. The goal of this article is to develop a nonparametric Bayes approach, which defines a prior with full support on the space of distributions for multiple unordered categorical variables. This support condition ensures that we are not restricting the dependence structure a priori. We show this can be accomplished through a Dirichlet process mixture of product multinomial distributions, which is also a convenient form for posterior computation. Methods for nonparametric testing of violations of independence are proposed, and the methods are applied to model positional dependence within transcription factor binding motifs.","465":"Margin-based classifiers have been popular in both machine learning and statistics for classification problems. Among numerous classifiers, some are hard classifiers while some are soft ones. Soft classifiers explicitly estimate the class conditional probabilities and then perform classification based on estimated probabilities. In contrast, hard classifiers directly target on the classification decision boundary without producing the probability estimation. These two types of classifiers are based on different philosophies and each has its own merits. In this paper, we propose a novel family of large-margin classifiers, namely large-margin unified machines (LUMs), which covers a broad range of margin-based classifiers including both hard and soft ones. By offering a natural bridge from soft to hard classification, the LUM provides a unified algorithm to fit various classifiers and hence a convenient platform to compare hard and soft classification. Both theoretical consistency and numerical performance of LUMs are explored. Our numerical study sheds some light on the choice between hard and soft classifiers in various classification problems.","466":"Genome-wide association studies commonly involve simultaneous tests of millions of single nucleotide polymorphisms (SNP) for disease association. The SNPs in nearby genomic regions, however, are often highly correlated due to linkage disequilibrium (LD, a genetic term for correlation). Simple Bonferonni correction for multiple comparisons is therefore too conservative. Permutation tests, which are often employed in practice, are both computationally expensive for genome-wide studies and limited in their scopes. We present an accurate and computationally efficient method, based on Poisson de-clumping heuristics, for approximating genome-wide significance of SNP associations. Compared with permutation tests and other multiple comparison adjustment approaches, our method computes the most accurate and robust p-value adjustments for millions of correlated comparisons within seconds. We demonstrate analytically that the accuracy and the efficiency of our method are nearly independent of the sample size, the number of SNPs, and the scale of p-values to be adjusted. In addition, our method can be easily adopted to estimate false discovery rate. When applied to genome-wide SNP datasets, we observed highly variable p-value adjustment results evaluated from different genomic regions. The variation in adjustments along the genome, however, are well conserved between the European and the African populations. The p-value adjustments are significantly correlated with LD among SNPs, recombination rates, and SNP densities. Given the large variability of sequence features in the genome, we further discuss a novel approach of using SNP-specific (local) thresholds to detect genome-wide significant associations. This article has supplementary material online.","467":null,"468":"Partially linear models provide a useful class of tools for modeling complex data by naturally incorporating a combination of linear and nonlinear effects within one framework. One key question in partially linear models is the choice of model structure, that is, how to decide which covariates are linear and which are nonlinear. This is a fundamental, yet largely unsolved problem for partially linear models. In practice, one often assumes that the model structure is given or known and then makes estimation and inference based on that structure. Alternatively, there are two methods in common use for tackling the problem: hypotheses testing and visual screening based on the marginal fits. Both methods are quite useful in practice but have their drawbacks. First, it is difficult to construct a powerful procedure for testing multiple hypotheses of linear against nonlinear fits. Second, the screening procedure based on the scatterplots of individual covariate fits may provide an educated guess on the regression function form, but the procedure is ad hoc and lacks theoretical justifications. In this article, we propose a new approach to structure selection for partially linear models, called the LAND (Linear And Nonlinear Discoverer). The procedure is developed in an elegant mathematical framework and possesses desired theoretical and computational properties. Under certain regularity conditions, we show that the LAND estimator is able to identify the underlying true model structure correctly and at the same time estimate the multivariate regression function consistently. The convergence rate of the new estimator is established as well. We further propose an iterative algorithm to implement the procedure and illustrate its performance by simulated and real examples. Supplementary materials for this article are available online.","469":"The estimated test error of a learned classifier is the most commonly reported measure of classifier performance. However, constructing a high quality point estimator of the test error has proved to be very difficult. Furthermore, common interval estimators (e.g. confidence intervals) are based on the point estimator of the test error and thus inherit all the difficulties associated with the point estimation problem. As a result, these confidence intervals do not reliably deliver nominal coverage. In contrast we directly construct the confidence interval by use of smooth data-dependent upper and lower bounds on the test error. We prove that for linear classifiers, the proposed confidence interval automatically adapts to the non-smoothness of the test error, is consistent under fixed and local alternatives, and does not require that the Bayes classifier be linear. Moreover, the method provides nominal coverage on a suite of test problems using a range of classification algorithms and sample sizes.","470":"","471":"In randomized studies, treatment comparisons conditional on intermediate post-randomization outcomes using standard analytic methods do not have a causal interpretation. An alternate approach entails treatment comparisons within principal strata defined by the potential outcomes for the intermediate outcome that would be observed under each treatment assignment. In this paper, we develop methods for randomization-based inference within principal strata. The proposed methods are compared with existing large-sample methods as well as traditional intent-to-treat approaches. This research is motivated by HIV prevention studies where few infections are expected and inference is desired within the always-infected principal stratum, i.e., all individuals who would become infected regardless of randomization assignment.","472":"In a cocaine dependence treatment study, we use linear and nonlinear regression models to model posttreatment cocaine craving scores and first cocaine relapse time. A subset of the covariates are summary statistics derived from baseline daily cocaine use trajectories, such as baseline cocaine use frequency and average daily use amount. These summary statistics are subject to estimation error and can therefore cause biased estimators for the regression coefficients. Unlike classical measurement error problems, the error we encounter here is heteroscedastic with an unknown distribution, and there are no replicates for the error-prone variables or instrumental variables. We propose two robust methods to correct for the bias: a computationally efficient method-of-moments-based method for linear regression models and a subsampling extrapolation method that is generally applicable to both linear and nonlinear regression models. Simulations and an application to the cocaine dependence treatment data are used to illustrate the efficacy of the proposed methods. Asymptotic theory and variance estimation for the proposed subsampling extrapolation method and some additional simulation results are described in the online supplementary material.","473":"In the context of large-scale multiple hypothesis testing, the hypotheses often possess certain group structures based on additional information such as Gene Ontology in gene expression data and phenotypes in genome-wide association studies. It is hence desirable to incorporate such information when dealing with multiplicity problems to increase statistical power. In this article, we demonstrate the benefit of considering group structure by presenting a p-value weighting procedure which utilizes the relative importance of each group while controlling the false discovery rate under weak conditions. The procedure is easy to implement and shown to be more powerful than the classical Benjamini-Hochberg procedure in both theoretical and simulation studies. By estimating the proportion of true null hypotheses, the data-driven procedure controls the false discovery rate asymptotically. Our analysis on one breast cancer dataset confirms that the procedure performs favorably compared with the classical method.","474":"The term structure of interest rates is used to price defaultable bonds and credit derivatives, as well as to infer the quality of bonds for risk management purposes. We introduce a model that jointly estimates term structures by means of a Bayesian hierarchical model with a prior probability model based on Dirichlet process mixtures. The modeling methodology borrows strength across term structures for purposes of estimation. The main advantage of our framework is its ability to produce reliable estimators at the company level even when there are only a few bonds per company. After describing the proposed model, we discuss an empirical application in which the term structure of 197 individual companies is estimated. The sample of 197 consists of 143 companies with only one or two bonds. In-sample and out-of-sample tests are used to quantify the improvement in accuracy that results from approximating the term structure of corporate bonds with estimators by company rather than by credit rating, the latter being a popular choice in the financial literature. A complete description of a Markov chain Monte Carlo (MCMC) scheme for the proposed model is available as Supplementary Material.","475":"Recent genomic studies have shown that significant chromosomal spatial correlation exists in gene expression of many organisms. Interestingly, coexpression has been observed among genes separated by a fixed interval in specific regions of a chromosome chain, which is likely caused by three-dimensional (3D) chromosome folding structures. Modeling such spatial correlation explicitly may lead to essential understandings of 3D chromosome structures and their roles in transcriptional regulation. In this paper, we explore chromosomal spatial correlation induced by 3D chromosome structures, and propose a hierarchical Bayesian method based on helical structures to formally model and incorporate the correlation into the analysis of gene expression microarray data. It is the first study to quantify and infer 3D chromosome structures in vivo using expression microarrays. Simulation studies show computing feasibility of the proposed method and that, under the assumption of helical chromosome structures, it can lead to precise estimation of structural parameters and gene expression levels. Real data applications demonstrate an intriguing biological phenomenon that functionally associated genes, which are far apart along the chromosome chain, are brought into physical proximity by chromosomal folding in 3D space to facilitate their coexpression. It leads to important biological insight into relationship between chromosome structure and function.","476":"Inspired by the non-regular framework studied in Laber and Murphy (2011), we propose a family of adaptive classifiers. We discuss briefly their asymptotic properties and show that under the non-regular framework these classifiers have an \"oracle property,\" and consequently have smaller asymptotic variance and smaller asymptotic test error variance than those of the original classifier. We also show that confidence intervals for the test error of the adaptive classifiers, based on either normal approximation or centered percentile bootstrap, are consistent.","477":"As the discipline of functional neuroimaging grows there is an increasing interest in meta analysis of brain imaging studies. A typical neuroimaging meta analysis collects peak activation coordinates (foci) from several studies and identifies areas of consistent activation. Most imaging meta analysis methods only produce null hypothesis inferences and do not provide an interpretable fitted model. To overcome these limitations, we propose a Bayesian spatial hierarchical model using a marked independent cluster process. We model the foci as offspring of a latent study center process, and the study centers are in turn offspring of a latent population center process. The posterior intensity function of the population center process provides inference on the location of population centers, as well as the inter-study variability of foci about the population centers. We illustrate our model with a meta analysis consisting of 437 studies from 164 publications, show how two subpopulations of studies can be compared and assess our model via sensitivity analyses and simulation studies. Supplemental materials are available online.","478":"In many applications we can expect that, or are interested to know if, a density function or a regression curve satisfies some specific shape constraints. For example, when the explanatory variable, X, represents the value taken by a treatment or dosage, the conditional mean of the response, Y , is often anticipated to be a monotone function of X. Indeed, if this regression mean is not monotone (in the appropriate direction) then the medical or commercial value of the treatment is likely to be significantly curtailed, at least for values of X that lie beyond the point at which monotonicity fails. In the case of a density, common shape constraints include log-concavity and unimodality. If we can correctly guess the shape of a curve, then nonparametric estimators can be improved by taking this information into account. Addressing such problems requires a method for testing the hypothesis that the curve of interest satisfies a shape constraint, and, if the conclusion of the test is positive, a technique for estimating the curve subject to the constraint. Nonparametric methodology for solving these problems already exists, but only in cases where the covariates are observed precisely. However in many problems, data can only be observed with measurement errors, and the methods employed in the error-free case typically do not carry over to this error context. In this paper we develop a novel approach to hypothesis testing and function estimation under shape constraints, which is valid in the context of measurement errors. Our method is based on tilting an estimator of the density or the regression mean until it satisfies the shape constraint, and we take as our test statistic the distance through which it is tilted. Bootstrap methods are used to calibrate the test. The constrained curve estimators that we develop are also based on tilting, and in that context our work has points of contact with methodology in the error-free case.","479":"An inverse regression methodology for assessing predictor performance in the censored data setup is developed along with inference procedures and a computational algorithm. The technique developed here allows for conditioning on the unobserved failure time along with a weighting mechanism that accounts for the censoring. The implementation is nonparametric and computationally fast. This provides an efficient methodological tool that can be used especially in cases where the usual modeling assumptions are not applicable to the data under consideration. It can also be a good diagnostic tool that can be used in the model selection process. We have provided theoretical justification of consistency and asymptotic normality of the methodology. Simulation studies and two data analyses are provided to illustrate the practical utility of the procedure.","480":"For comparison of multiple outcomes commonly encountered in biomedical research, Huang et al. (2005) improved O'Brien's (1984) rank-sum tests through the replacement of the ad hoc variance by the asymptotic variance of the test statistics. The improved tests control the Type I error rate at the desired level and gain power when the differences between the two comparison groups in each outcome variable fall into the same direction. However, they may lose power when the differences are in different directions (e.g., some are positive and some are negative). These tests and the popular Bonferroni correction failed to show important significant difference when applied to compare heart rates from a clinical trial to evaluate the effect of a procedure to remove the cardioprotective solution HTK. We propose an alternative test statistic, taking the maximum of the individual rank-sum statistics, which controls the type I error and maintains satisfactory power regardless of the directions of the differences. Simulation studies show the proposed test to be of higher power than other tests in certain alternative parameter space of interest. Furthermore, when used to analyze the heart rates data the proposed test yields more satisfactory results.","481":"We consider inference in randomized longitudinal studies with missing data that is generated by skipped clinic visits and loss to follow-up. In this setting, it is well known that full data estimands are not identified unless unverified assumptions are imposed. We assume a non-future dependence model for the drop-out mechanism and partial ignorability for the intermittent missingness. We posit an exponential tilt model that links non-identifiable distributions and distributions identified under partial ignorability. This exponential tilt model is indexed by non-identified parameters, which are assumed to have an informative prior distribution, elicited from subject-matter experts. Under this model, full data estimands are shown to be expressed as functionals of the distribution of the observed data. To avoid the curse of dimensionality, we model the distribution of the observed data using a Bayesian shrinkage model. In a simulation study, we compare our approach to a fully parametric and a fully saturated model for the distribution of the observed data. Our methodology is motivated by, and applied to, data from the Breast Cancer Prevention Trial.","482":"Array-based comparative genomic hybridization (aCGH) is a high-resolution high-throughput technique for studying the genetic basis of cancer. The resulting data consists of log fluorescence ratios as a function of the genomic DNA location and provides a cytogenetic representation of the relative DNA copy number variation. Analysis of such data typically involves estimation of the underlying copy number state at each location and segmenting regions of DNA with similar copy number states. Most current methods proceed by modeling a single sample\/array at a time, and thus fail to borrow strength across multiple samples to infer shared regions of copy number aberrations. We propose a hierarchical Bayesian random segmentation approach for modeling aCGH data that utilizes information across arrays from a common population to yield segments of shared copy number changes. These changes characterize the underlying population and allow us to compare different population aCGH profiles to assess which regions of the genome have differential alterations. Our method, referred to as BDSAcgh (Bayesian Detection of Shared Aberrations in aCGH), is based on a unified Bayesian hierarchical model that allows us to obtain probabilities of alteration states as well as probabilities of differential alteration that correspond to local false discovery rates. We evaluate the operating characteristics of our method via simulations and an application using a lung cancer aCGH data set.","483":"In seasonal influenza epidemics, pathogens such as respiratory syncytial virus (RSV) often co-circulate with influenza and cause influenza-like illness (ILI) in human hosts. However, it is often impractical to test for each potential pathogen or to collect specimens for each observed ILI episode, making inference about influenza transmission difficult. In the setting of infectious diseases, missing outcomes impose a particular challenge because of the dependence among individuals. We propose a Bayesian competing-risk model for multiple co-circulating pathogens for inference on transmissibility and intervention efficacies under the assumption that missingness in the biological confirmation of the pathogen is ignorable. Simulation studies indicate a reasonable performance of the proposed model even if the number of potential pathogens is misspecified. They also show that a moderate amount of missing laboratory test results has only a small impact on inference about key parameters in the setting of close contact groups. Using the proposed model, we found that a non-pharmaceutical intervention is marginally protective against transmission of influenza A in a study conducted in elementary schools.","484":"Tropospheric ozone is one of the six criteria pollutants regulated by the United States Environmental Protection Agency under the Clean Air Act and has been linked with several adverse health effects, including mortality. Due to the strong dependence on weather conditions, ozone may be sensitive to climate change and there is great interest in studying the potential effect of climate change on ozone, and how this change may affect public health. In this paper we develop a Bayesian spatial model to predict ozone under different meteorological conditions, and use this model to study spatial and temporal trends and to forecast ozone concentrations under different climate scenarios. We develop a spatial quantile regression model that does not assume normality and allows the covariates to affect the entire conditional distribution, rather than just the mean. The conditional distribution is allowed to vary from site-to-site and is smoothed with a spatial prior. For extremely large datasets our model is computationally infeasible, and we develop an approximate method. We apply the approximate version of our model to summer ozone from 1997-2005 in the Eastern U.S., and use deterministic climate models to project ozone under future climate conditions. Our analysis suggests that holding all other factors fixed, an increase in daily average temperature will lead to the largest increase in ozone in the Industrial Midwest and Northeast.","485":"In longitudinal clinical trials, when outcome variables at later time points are only defined for patients who survive to those times, the evaluation of the causal effect of treatment is complicated. In this paper, we describe an approach that can be used to obtain the causal effect of three treatment arms with ordinal outcomes in the presence of death using a principal stratification approach. We introduce a set of flexible assumptions to identify the causal effect and implement a sensitivity analysis for non-identifiable assumptions which we parameterize parsimoniously. Methods are illustrated on quality of life data from a recent colorectal cancer clinical trial.","486":"There is substantial observational evidence that long-term exposure to particulate air pollution is associated with premature death in urban populations. Estimates of the magnitude of these effects derive largely from cross-sectional comparisons of adjusted mortality rates among cities with varying pollution levels. Such estimates are potentially confounded by other differences among the populations correlated with air pollution, for example, socioeconomic factors. An alternative approach is to study covariation of particulate matter and mortality across time within a city, as has been done in investigations of short-term exposures. In either event, observational studies like these are subject to confounding by unmeasured variables. Therefore the ability to detect such confounding and to derive estimates less affected by confounding are a high priority. In this article, we describe and apply a method of decomposing the exposure variable into components with variation at distinct temporal, spatial, and time by space scales, here focusing on the components involving time. Starting from a proportional hazard model, we derive a Poisson regression model and estimate two regression coefficients: the \"global\" coefficient that measures the association between national trends in pollution and mortality; and the \"local\" coefficient, derived from space by time variation, that measures the association between location-specific trends in pollution and mortality adjusted by the national trends. Absent unmeasured confounders and given valid model assumptions, the scale-specific coefficients should be similar; substantial differences in these coefficients constitute a basis for questioning the model. We derive a backfitting algorithm to fit our model to very large spatio-temporal datasets. We apply our methods to the Medicare Cohort Air Pollution Study (MCAPS), which includes individual-level information on time of death and age on a population of 18.2 million for the period 2000-2006. Results based on the global coefficient indicate a large increase in the national life expectancy for reductions in the yearly national average of PM2.5. However, this coefficient based on national trends in PM2.5 and mortality is likely to be confounded by other variables trending on the national level. Confounding of the local coefficient by unmeasured factors is less likely, although it cannot be ruled out. Based on the local coefficient alone, we are not able to demonstrate any change in life expectancy for a reduction in PM2.5. We use additional survey data available for a subset of the data to investigate sensitivity of results to the inclusion of additional covariates, but both coefficients remain largely unchanged.","487":"Independent component analysis (ICA) is an effective data-driven method for blind source separation. It has been successfully applied to separate source signals of interest from their mixtures. Most existing ICA procedures are carried out by relying solely on the estimation of the marginal density functions, either parametrically or nonparametrically. In many applications, correlation structures within each source also play an important role besides the marginal distributions. One important example is functional magnetic resonance imaging (fMRI) analysis where the brain-function-related signals are temporally correlated. In this article, we consider a novel approach to ICA that fully exploits the correlation structures within the source signals. Specifically, we propose to estimate the spectral density functions of the source signals instead of their marginal density functions. This is made possible by virtue of the intrinsic relationship between the (unobserved) sources and the (observed) mixed signals. Our methodology is described and implemented using spectral density functions from frequently used time series models such as autoregressive moving average (ARMA) processes. The time series parameters and the mixing matrix are estimated via maximizing the Whittle likelihood function. We illustrate the performance of the proposed method through extensive simulation studies and a real fMRI application. The numerical results indicate that our approach outperforms several popular methods including the most widely used fastICA algorithm. This article has supplementary material online.","488":"We introduce efficient Markov chain Monte Carlo methods for inference and model determination in multivariate and matrix-variate Gaussian graphical models. Our framework is based on the G-Wishart prior for the precision matrix associated with graphs that can be decomposable or non-decomposable. We extend our sampling algorithms to a novel class of conditionally autoregressive models for sparse estimation in multivariate lattice data, with a special emphasis on the analysis of spatial data. These models embed a great deal of flexibility in estimating both the correlation structure across outcomes and the spatial correlation structure, thereby allowing for adaptive smoothing and spatial autocorrelation parameters. Our methods are illustrated using a simulated example and a real-world application which concerns cancer mortality surveillance. Supplementary materials with computer code and the datasets needed to replicate our numerical results together with additional tables of results are available online.","489":"Chromatin immunoprecipitation followed by sequencing (ChIP-Seq) has revolutionalized experiments for genome-wide profiling of DNA-binding proteins, histone modifications, and nucleosome occupancy. As the cost of sequencing is decreasing, many researchers are switching from microarray-based technologies (ChIP-chip) to ChIP-Seq for genome-wide study of transcriptional regulation. Despite its increasing and well-deserved popularity, there is little work that investigates and accounts for sources of biases in the ChIP-Seq technology. These biases typically arise from both the standard pre-processing protocol and the underlying DNA sequence of the generated data. We study data from a naked DNA sequencing experiment, which sequences non-cross-linked DNA after deproteinizing and shearing, to understand factors affecting background distribution of data generated in a ChIP-Seq experiment. We introduce a background model that accounts for apparent sources of biases such as mappability and GC content and develop a flexible mixture model named MOSAiCS for detecting peaks in both one- and two-sample analyses of ChIP-Seq data. We illustrate that our model fits observed ChIP-Seq data well and further demonstrate advantages of MOSAiCS over commonly used tools for ChIP-Seq data analysis with several case studies.","490":"Agglomerative hierarchical clustering is a popular class of methods for understanding the structure of a dataset. The nature of the clustering depends on the choice of linkage-that is, on how one measures the distance between clusters. In this article we investigate minimax linkage, a recently introduced but little-studied linkage. Minimax linkage is unique in naturally associating a prototype chosen from the original dataset with every interior node of the dendrogram. These prototypes can be used to greatly enhance the interpretability of a hierarchical clustering. Furthermore, we prove that minimax linkage has a number of desirable theoretical properties; for example, minimax-linkage dendrograms cannot have inversions (unlike centroid linkage) and is robust against certain perturbations of a dataset. We provide an efficient implementation and illustrate minimax linkage's strengths as a data analysis and visualization tool on a study of words from encyclopedia articles and on a dataset of images of human faces.","491":"High-throughput functional proteomic technologies provide a way to quantify the expression of proteins of interest. Statistical inference centers on identifying the activation state of proteins and their patterns of molecular interaction formalized as dependence structure. Inference on dependence structure is particularly important when proteins are selected because they are part of a common molecular pathway. In that case, inference on dependence structure reveals properties of the underlying pathway. We propose a probability model that represents molecular interactions at the level of hidden binary latent variables that can be interpreted as indicators for active versus inactive states of the proteins. The proposed approach exploits available expert knowledge about the target pathway to define an informative prior on the hidden conditional dependence structure. An important feature of this prior is that it provides an instrument to explicitly anchor the model space to a set of interactions of interest, favoring a local search approach to model determination. We apply our model to reverse-phase protein array data from a study on acute myeloid leukemia. Our inference identifies relevant subpathways in relation to the unfolding of the biological process under study.","492":"We are interested in predicting one or more continuous forest variables (e.g., biomass, volume, age) at a fine resolution (e.g., pixel level) across a specified domain. Given a definition of forest\/nonforest, this prediction is typically a two-step process. The first step predicts which locations are forested. The second step predicts the value of the variable for only those forested locations. Rarely is the forest\/nonforest status predicted without error. However, the uncertainty in this prediction is typically not propagated through to the subsequent prediction of the forest variable of interest. Failure to acknowledge this error can result in biased estimates of forest variable totals within a domain. In response to this problem, we offer a modeling framework that will allow propagation of this uncertainty. Here we envision two latent processes generating the data. The first is a continuous spatial process while the second is a binary spatial process. The continuous spatial process controls the spatial association structure of the forest variable of interest, while the binary process indicates presence of a possible nonzero value for the forest variable at a given location. The proposed models are applied to georeferenced National Forest Inventory (NFI) data and spatially coinciding remotely sensed predictor variables. Due to the large number of observed locations in this dataset we seek dimension reduction not just in the likelihood, but also for unobserved stochastic processes. We demonstrate how a low-rank predictive process can be adapted to our setting and reduce the dimensionality of the data and ease the computational burden.","493":"We address the problem of sparse selection in linear models. A number of nonconvex penalties have been proposed in the literature for this purpose, along with a variety of convex-relaxation algorithms for finding good solutions. In this article we pursue a coordinate-descent approach for optimization, and study its convergence properties. We characterize the properties of penalties suitable for this approach, study their corresponding threshold functions, and describe a df-standardizing reparametrization that assists our pathwise algorithm. The MC+ penalty is ideally suited to this task, and we use it to demonstrate the performance of our algorithm. Certain technical derivations and experiments related to this article are included in the Supplementary Materials section.","494":"Images, often stored in multidimensional arrays, are fast becoming ubiquitous in medical and public health research. Analyzing populations of images is a statistical problem that raises a host of daunting challenges. The most significant challenge is the massive size of the datasets incorporating images recorded for hundreds or thousands of subjects at multiple visits. We introduce the population value decomposition (PVD), a general method for simultaneous dimensionality reduction of large populations of massive images. We show how PVD can be seamlessly incorporated into statistical modeling, leading to a new, transparent, and rapid inferential framework. Our PVD methodology was motivated by and applied to the Sleep Heart Health Study, the largest community-based cohort study of sleep containing more than 85 billion observations on thousands of subjects at two visits. This article has supplementary material online.","495":"There is often interest in predicting an individual's latent health status based on high-dimensional biomarkers that vary over time. Motivated by time-course gene expression array data that we have collected in two influenza challenge studies performed with healthy human volunteers, we develop a novel time-aligned Bayesian dynamic factor analysis methodology. The time course trajectories in the gene expressions are related to a relatively low-dimensional vector of latent factors, which vary dynamically starting at the latent initiation time of infection. Using a nonparametric cure rate model for the latent initiation times, we allow selection of the genes in the viral response pathway, variability among individuals in infection times, and a subset of individuals who are not infected. As we demonstrate using held-out data, this statistical framework allows accurate predictions of infected individuals in advance of the development of clinical symptoms, without labeled data and even when the number of biomarkers vastly exceeds the number of individuals under study. Biological interpretation of several of the inferred pathways (factors) is provided.","496":"Conventional agreement studies have been confined to addressing the sense of reproducibility, and therefore are limited to assessing measurements on the same scale. In this work, we propose a new concept, called \"broad sense agreement,\" which extends the classical framework of agreement to evaluate the capability of interpreting a continuous measurement in an ordinal scale. We present a natural measure for broad sense agreement. Nonparametric estimation and inference procedures are developed for the proposed measure along with theoretical justifications. We also consider longitudinal settings which involve agreement assessments at multiple time points. Simulation studies have demonstrated good performance of the proposed method with small sample sizes. We illustrate our methods via an application to a mental health study.","497":"Gene regulation is a complicated process. The interaction of many genes and their products forms an intricate biological network. Identification of this dynamic network will help us understand the biological process in a systematic way. However, the construction of such a dynamic network is very challenging for a high-dimensional system. In this article we propose to use a set of ordinary differential equations (ODE), coupled with dimensional reduction by clustering and mixed-effects modeling techniques, to model the dynamic gene regulatory network (GRN). The ODE models allow us to quantify both positive and negative gene regulations as well as feedback effects of one set of genes in a functional module on the dynamic expression changes of the genes in another functional module, which results in a directed graph network. A five-step procedure, Clustering, Smoothing, regulation Identification, parameter Estimates refining and Function enrichment analysis (CSIEF) is developed to identify the ODE-based dynamic GRN. In the proposed CSIEF procedure, a series of cutting-edge statistical methods and techniques are employed, that include non-parametric mixed-effects models with a mixture distribution for clustering, nonparametric mixed-effects smoothing-based methods for ODE models, the smoothly clipped absolute deviation (SCAD)-based variable selection, and stochastic approximation EM (SAEM) approach for mixed-effects ODE model parameter estimation. The key step, the SCAD-based variable selection of the proposed procedure is justified by investigating its asymptotic properties and validated by Monte Carlo simulations. We apply the proposed method to identify the dynamic GRN for yeast cell cycle progression data. We are able to annotate the identified modules through function enrichment analyses. Some interesting biological findings are discussed. The proposed procedure is a promising tool for constructing a general dynamic GRN and more complicated dynamic networks.","498":null,"499":"Analysis of high dimensional data often seeks to identify a subset of important features and assess their effects on the outcome. Traditional statistical inference procedures based on standard regression methods often fail in the presence of high-dimensional features. In recent years, regularization methods have emerged as promising tools for analyzing high dimensional data. These methods simultaneously select important features and provide stable estimation of their effects. Adaptive LASSO and SCAD for instance, give consistent and asymptotically normal estimates with oracle properties. However, in finite samples, it remains difficult to obtain interval estimators for the regression parameters. In this paper, we propose perturbation resampling based procedures to approximate the distribution of a general class of penalized parameter estimates. Our proposal, justified by asymptotic theory, provides a simple way to estimate the covariance matrix and confidence regions. Through finite sample simulations, we verify the ability of this method to give accurate inference and compare it to other widely used standard deviation and confidence interval estimates. We also illustrate our proposals with a data set used to study the association of HIV drug resistance and a large number of genetic mutations.","500":"To evaluate the clinical utility of new risk markers, a crucial step is to measure their predictive accuracy with prospective studies. However, it is often infeasible to obtain marker values for all study participants. The nested case-control (NCC) design is a useful cost-effective strategy for such settings. Under the NCC design, markers are only ascertained for cases and a fraction of controls sampled randomly from the risk sets. The outcome dependent sampling generates a complex data structure and therefore a challenge for analysis. Existing methods for analyzing NCC studies focus primarily on association measures. Here, we propose a class of non-parametric estimators for commonly used accuracy measures. We derived asymptotic expansions for accuracy estimators based on both finite population and Bernoulli sampling and established asymptotic equivalence between the two. Simulation results suggest that the proposed procedures perform well in finite samples. The new procedures were illustrated with data from the Framingham Offspring study.","501":"With the recent explosion of scientific data of unprecedented size and complexity, feature ranking and screening are playing an increasingly important role in many scientific studies. In this article, we propose a novel feature screening procedure under a unified model framework, which covers a wide variety of commonly used parametric and semiparametric models. The new method does not require imposing a specific model structure on regression functions, and thus is particularly appealing to ultrahigh-dimensional regressions, where there are a huge number of candidate predictors but little information about the actual model forms. We demonstrate that, with the number of predictors growing at an exponential rate of the sample size, the proposed procedure possesses consistency in ranking, which is both useful in its own right and can lead to consistency in selection. The new procedure is computationally efficient and simple, and exhibits a competent empirical performance in our intensive simulations and real data analysis.","502":null,"503":"Massively Parallel Signature Sequencing (MPSS) is a high-throughput counting-based technology available for gene expression profiling. It produces output that is similar to Serial Analysis of Gene Expression (SAGE) and is ideal for building complex relational databases for gene expression. Our goal is to compare the in vivo global gene expression profiles of tissues infected with different strains of Salmonella obtained using the MPSS technology. In this article, we develop an exact ANOVA type model for this count data using a zero-inflated Poisson (ZIP) distribution, different from existing methods that assume continuous densities. We adopt two Bayesian hierarchical models-one parametric and the other semiparametric with a Dirichlet process prior that has the ability to \"borrow strength\" across related signatures, where a signature is a specific arrangement of the nucleotides, usually 16-21 base-pairs long. We utilize the discreteness of Dirichlet process prior to cluster signatures that exhibit similar differential expression profiles. Tests for differential expression are carried out using non-parametric approaches, while controlling the false discovery rate. We identify several differentially expressed genes that have important biological significance and conclude with a summary of the biological discoveries.","504":"While Distance Weighted Discrimination (DWD) is an appealing approach to classification in high dimensions, it was designed for balanced datasets. In the case of unequal costs, biased sampling, or unbalanced data, there are major improvements available, using appropriately weighted versions of DWD (wDWD). A major contribution of this paper is the development of optimal weighting schemes for various nonstandard classification problems. In addition, we discuss several alternative criteria and propose an adaptive weighting scheme (awDWD) and demonstrate its advantages over nonadaptive weighting schemes under some situations. The second major contribution is a theoretical study of weighted DWD. Both high-dimensional low sample-size asymptotics and Fisher consistency of DWD are studied. The performance of weighted DWD is evaluated using simulated examples and two real data examples. The theoretical results are also confirmed by simulations.","505":"We use data collected in the National Comorbidity Survey - Adolescent (NCS-A) to develop a methodology to estimate the small-area prevalence of serious emotional distress (SED) in schools in the United States, exploiting the clustering of the main NCS-A sample by school. The NCS-A instrument includes both a short screening scale, the K6, and extensive diagnostic assessments of the individual disorders and associated impairment that determine the diagnosis of SED. We fitted a Bayesian bivariate multilevel regression model with correlated effects for the probability of SED and a modified K6 score at the individual and school levels. Our results provide evidence for the existence of variation in the prevalence of SED across schools and geographical regions. Although the concordance between the modified K6 scale and SED is only modest for individuals, the school-level random effects for the two measures are strongly correlated. Under this model we obtain a prediction equation for the rate of SED based on the mean K6 score and covariates. This finding supports the feasibility of using short screening scales like the K6 as an alternative to more comprehensive lay assessments in estimating school-level rates of SED. These methods may be applicable to other studies aiming at small-area estimation for geographical units.","506":"Classical statistical approaches for multiclass probability estimation are typically based on regression techniques such as multiple logistic regression, or density estimation approaches such as linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA). These methods often make certain assumptions on the form of probability functions or on the underlying distributions of subclasses. In this article, we develop a model-free procedure to estimate multiclass probabilities based on large-margin classifiers. In particular, the new estimation scheme is employed by solving a series of weighted large-margin classifiers and then systematically extracting the probability information from these multiple classification rules. A main advantage of the proposed probability estimation technique is that it does not impose any strong parametric assumption on the underlying distribution and can be applied for a wide range of large-margin classification methods. A general computational algorithm is developed for class probability estimation. Furthermore, we establish asymptotic consistency of the probability estimates. Both simulated and real data examples are presented to illustrate competitive performance of the new approach and compare it with several other existing methods.","507":"In situations where individuals are screened for an infectious disease or other binary characteristic and where resources for testing are limited, group testing can offer substantial benefits. Group testing, where subjects are tested in groups (pools) initially, has been successfully applied to problems in blood bank screening, public health, drug discovery, genetics, and many other areas. In these applications, often the goal is to identify each individual as positive or negative using initial group tests and subsequent retests of individuals within positive groups. Many group testing identification procedures have been proposed; however, the vast majority of them fail to incorporate heterogeneity among the individuals being screened. In this paper, we present a new approach to identify positive individuals when covariate information is available on each. This covariate information is used to structure how retesting is implemented within positive groups; therefore, we call this new approach \"informative retesting.\" We derive closed-form expressions and implementation algorithms for the probability mass functions for the number of tests needed to decode positive groups. These informative retesting procedures are illustrated through a number of examples and are applied to chlamydia and gonorrhea testing in Nebraska for the Infertility Prevention Project. Overall, our work shows compelling evidence that informative retesting can dramatically decrease the number of tests while providing accuracy similar to established non-informative retesting procedures.","508":null,"509":"We propose a new set of test statistics to examine the association between two ordinal categorical variables X and Y after adjusting for continuous and\/or categorical covariates Z. Our approach first fits multinomial (e.g., proportional odds) models of X and Y, separately, on Z. For each subject, we then compute the conditional distributions of X and Y given Z. If there is no relationship between X and Y after adjusting for Z, then these conditional distributions will be independent, and the observed value of (X, Y) for a subject is expected to follow the product distribution of these conditional distributions. We consider two simple ways of testing the null of conditional independence, both of which treat X and Y equally, in the sense that they do not require specifying an outcome and a predictor variable. The first approach adds these product distributions across all subjects to obtain the expected distribution of (X, Y) under the null and then contrasts it with the observed unconditional distribution of (X, Y). Our second approach computes \"residuals\" from the two multinomial models and then tests for correlation between these residuals; we define a new individual-level residual for models with ordinal outcomes. We present methods for computing p-values using either the empirical or asymptotic distributions of our test statistics. Through simulations, we demonstrate that our test statistics perform well in terms of power and Type I error rate when compared to proportional odds models which treat X as either a continuous or categorical predictor. We apply our methods to data from a study of visual impairment in children and to a study of cervical abnormalities in human immunodeficiency virus (HIV)-infected women. Supplemental materials for the article are available online.","510":"We consider the problem of clustering observations using a potentially large set of features. One might expect that the true underlying clusters present in the data differ only with respect to a small fraction of the features, and will be missed if one clusters the observations using the full set of features. We propose a novel framework for sparse clustering, in which one clusters the observations using an adaptively chosen subset of the features. The method uses a lasso-type penalty to select the features. We use this framework to develop simple methods for sparse K-means and sparse hierarchical clustering. A single criterion governs both the selection of the features and the resulting clusters. These approaches are demonstrated on simulated data and on genomic data sets.","511":"We consider nonparametric regression of a scalar outcome on a covariate when the outcome is missing at random (MAR) given the covariate and other observed auxiliary variables. We propose a class of augmented inverse probability weighted (AIPW) kernel estimating equations for nonparametric regression under MAR. We show that AIPW kernel estimators are consistent when the probability that the outcome is observed, that is, the selection probability, is either known by design or estimated under a correctly specified model. In addition, we show that a specific AIPW kernel estimator in our class that employs the fitted values from a model for the conditional mean of the outcome given covariates and auxiliaries is double-robust, that is, it remains consistent if this model is correctly specified even if the selection probabilities are modeled or specified incorrectly. Furthermore, when both models happen to be right, this double-robust estimator attains the smallest possible asymptotic variance of all AIPW kernel estimators and maximally extracts the information in the auxiliary variables. We also describe a simple correction to the AIPW kernel estimating equations that while preserving double-robustness it ensures efficiency improvement over nonaugmented IPW estimation when the selection model is correctly specified regardless of the validity of the second model used in the augmentation term. We perform simulations to evaluate the finite sample performance of the proposed estimators, and apply the methods to the analysis of the AIDS Costs and Services Utilization Survey data. Technical proofs are available online.","512":"We study a general class of partially linear transformation models, which extend linear transformation models by incorporating nonlinear covariate effects in survival data analysis. A new martingale-based estimating equation approach, consisting of both global and kernel-weighted local estimation equations, is developed for estimating the parametric and nonparametric covariate effects in a unified manner. We show that with a proper choice of the kernel bandwidth parameter, one can obtain the consistent and asymptotically normal parameter estimates for the linear effects. Asymptotic properties of the estimated nonlinear effects are established as well. We further suggest a simple resampling method to estimate the asymptotic variance of the linear estimates and show its effectiveness. To facilitate the implementation of the new procedure, an iterative algorithm is developed. Numerical examples are given to illustrate the finite-sample performance of the procedure.","513":"In many genetics studies, especially in the investigation of mental illness and behavioral disorders, it is common for researchers to collect multiple phenotypes to characterize the complex disease of interest. It may be advantageous to analyze those phenotypic measurements simultaneously if they share a similar genetic mechanism. In this study, we present a nonparametric approach to studying multiple traits together rather than examining each trait separately. Through simulation we compared the nominal type I error and power of our proposed test to an existing test, i.e., a generalized family-based association test. The empirical results suggest that our proposed approach is superior to the existing test in the analysis of ordinal traits. The advantage is demonstrated on a data set concerning alcohol dependence. In this application, the use of our methods enhanced the signal of the association test.","514":"Modern cancer treatments have substantially improved cure rates and have generated a great interest in and need for proper statistical tools to analyze survival data with non-negligible cure fractions. Data with cure fractions are often complicated by dependent censoring, and the analysis of this type of data typically involves untestable parametric assumptions on the dependence of the censoring mechanism and the true survival times. Motivated by the analysis of prostate cancer survival trends, we propose a class of semiparametric transformation cure models that allows for dependent censoring without making parametric assumptions on the dependence relationship. The proposed class of models encompasses a number of common models for the latency survival function, including the proportional hazards model and the proportional odds model, and also allows for time-dependent covariates. An inverse censoring probability reweighting scheme is used to derive unbiased estimating equations. We validate small sample properties with simulations and demonstrate the method with a data application.","515":"Extracting grouping structure or identifying homogenous subgroups of predictors in regression is crucial for high-dimensional data analysis. A low-dimensional structure in particular-grouping, when captured in a regression model, enables to enhance predictive performance and to facilitate a model's interpretability Grouping pursuit extracts homogenous subgroups of predictors most responsible for outcomes of a response. This is the case in gene network analysis, where grouping reveals gene functionalities with regard to progression of a disease. To address challenges in grouping pursuit, we introduce a novel homotopy method for computing an entire solution surface through regularization involving a piecewise linear penalty. This nonconvex and overcomplete penalty permits adaptive grouping and nearly unbiased estimation, which is treated with a novel concept of grouped subdifferentials and difference convex programming for efficient computation. Finally, the proposed method not only achieves high performance as suggested by numerical analysis, but also has the desired optimality with regard to grouping pursuit and prediction as showed by our theoretical results.","516":"We introduce a new class of functional generalized linear models, where the response is a scalar and some of the covariates are functional. We assume that the response depends on multiple covariates, a finite number of latent features in the functional predictor, and interaction between the two. To achieve parsimony, the interaction between the multiple covariates and the functional predictor is modeled semiparametrically with a single-index structure. We propose a two step estimation procedure based on local estimating equations, and investigate two situations: (a) when the basis functions are pre-determined, e.g., Fourier or wavelet basis functions and the functional features of interest are known; and (b) when the basis functions are data driven, such as with functional principal components. Asymptotic properties are developed. Notably, we show that when the functional features are data driven, the parameter estimates have an increased asymptotic variance, due to the estimation error of the basis functions. Our methods are illustrated with a simulation study and applied to an empirical data set, where a previously unknown interaction is detected. Technical proofs of our theoretical results are provided in the online supplemental materials.","517":"We apply the nonconcave penalized likelihood approach to obtain variable selections as well as shrinkage estimators. This approach relies heavily on the choice of regularization parameter, which controls the model complexity. In this paper, we propose employing the generalized information criterion (GIC), encompassing the commonly used Akaike information criterion (AIC) and Bayesian information criterion (BIC), for selecting the regularization parameter. Our proposal makes a connection between the classical variable selection criteria and the regularization parameter selections for the nonconcave penalized likelihood approaches. We show that the BIC-type selector enables identification of the true model consistently, and the resulting estimator possesses the oracle property in the terminology of Fan and Li (2001). In contrast, however, the AIC-type selector tends to overfit with positive probability. We further show that the AIC-type selector is asymptotically loss efficient, while the BIC-type selector is not. Our simulation results confirm these theoretical findings, and an empirical example is presented. Some technical proofs are given in the online supplementary material.","518":"This article expands upon recent interest in Bayesian hierarchical models in quantitative genetics by developing spatial process models for inference on additive and dominance genetic variance within the context of large spatially referenced trial datasets of multiple traits of interest. Direct application of such multivariate models to large spatial datasets is often computationally infeasible because of cubic order matrix algorithms involved in estimation. The situation is even worse in Markov chain Monte Carlo (MCMC) contexts where such computations are performed for several thousand iterations. Here, we discuss approaches that help obviate these hurdles without sacrificing the richness in modeling. For genetic effects, we demonstrate how an initial spectral decomposition of the relationship matrices negates the expensive matrix inversions required in previously proposed MCMC methods. For spatial effects we discuss a multivariate predictive process that reduces the computational burden by projecting the original process onto a subspace generated by realizations of the original process at a specified set of locations (or knots). We illustrate the proposed methods using a synthetic dataset with multivariate additive and dominant genetic effects and anisotropic spatial residuals, and a large dataset from a scots pine (Pinus sylvestris L.) progeny study conducted in northern Sweden. Our approaches enable us to provide a comprehensive analysis of this large trial which amply demonstrates that, in addition to violating basic assumptions of the linear model, ignoring spatial effects can result in downwardly biased measures of heritability.","519":"Developing, targeting, and evaluating genomic strategies for population-based disease prevention require population-based data. In response to this urgent need, genotyping has been conducted within the Third National Health and Nutrition Examination (NHANES III), the nationally-representative household-interview health survey in the U.S. However, before these genetic analyses can occur, family relationships within households must be accurately ascertained. Unfortunately, reported family relationships within NHANES III households based on questionnaire data are incomplete and inconclusive with regards to actual biological relatedness of family members. We inferred family relationships within households using DNA fingerprints (Identifiler(R)) that contain the DNA loci used by law enforcement agencies for forensic identification of individuals. However, performance of these loci for relationship inference is not well understood. We evaluated two competing statistical methods for relationship inference on pairs of household members: an exact likelihood ratio relying on allele frequencies to an Identical By State (IBS) likelihood ratio that only requires matching alleles. We modified these methods to account for genotyping errors and population substructure. The two methods usually agree on the rankings of the most likely relationships. However, the IBS method underestimates the likelihood ratio by not accounting for the informativeness of matching rare alleles. The likelihood ratio is sensitive to estimates of population substructure, and parent-child relationships are sensitive to the specified genotyping error rate. These loci were unable to distinguish second-degree relationships and cousins from being unrelated. The genetic data is also useful for verifying reported relationships and identifying data quality issues. An important by-product is the first explicitly nationally-representative estimates of allele frequencies at these ubiquitous forensic loci.","520":"By allowing the regression coefficients to change with certain covariates, the class of varying coefficient models offers a flexible approach to modeling nonlinearity and interactions between covariates. This paper proposes a novel estimation procedure for the varying coefficient models based on local ranks. The new procedure provides a highly efficient and robust alternative to the local linear least squares method, and can be conveniently implemented using existing R software package. Theoretical analysis and numerical simulations both reveal that the gain of the local rank estimator over the local linear least squares estimator, measured by the asymptotic mean squared error or the asymptotic mean integrated squared error, can be substantial. In the normal error case, the asymptotic relative efficiency for estimating both the coefficient functions and the derivative of the coefficient functions is above 96%; even in the worst case scenarios, the asymptotic relative efficiency has a lower bound 88.96% for estimating the coefficient functions, and a lower bound 89.91% for estimating their derivatives. The new estimator may achieve the nonparametric convergence rate even when the local linear least squares method fails due to infinite random error variance. We establish the large sample theory of the proposed procedure by utilizing results from generalized U-statistics, whose kernel function may depend on the sample size. We also extend a resampling approach, which perturbs the objective function repeatedly, to the generalized U-statistics setting; and demonstrate that it can accurately estimate the asymptotic covariance matrix.","521":"We introduce Generalized Multilevel Functional Linear Models (GMFLMs), a novel statistical framework for regression models where exposure has a multilevel functional structure. We show that GMFLMs are, in fact, generalized multilevel mixed models (GLMMs). Thus, GMFLMs can be analyzed using the mixed effects inferential machinery and can be generalized within a well researched statistical framework. We propose and compare two methods for inference: 1) a two-stage frequentist approach; and 2) a joint Bayesian analysis. Our methods are motivated by and applied to the Sleep Heart Health Study (SHHS), the largest community cohort study of sleep. However, our methods are general and easy to apply to a wide spectrum of emerging biological and medical data sets. Supplemental materials for this article are available online.","522":null,"523":null,"524":null,"525":"Hierarchical functional data are widely seen in complex studies where sub-units are nested within units, which in turn are nested within treatment groups. We propose a general framework of functional mixed effects model for such data: within unit and within sub-unit variations are modeled through two separate sets of principal components; the sub-unit level functions are allowed to be correlated. Penalized splines are used to model both the mean functions and the principal components functions, where roughness penalties are used to regularize the spline fit. An EM algorithm is developed to fit the model, while the specific covariance structure of the model is utilized for computational efficiency to avoid storage and inversion of large matrices. Our dimension reduction with principal components provides an effective solution to the difficult tasks of modeling the covariance kernel of a random function and modeling the correlation between functions. The proposed methodology is illustrated using simulations and an empirical data set from a colon carcinogenesis study. Supplemental materials are available online.","526":"Motivated by medical studies in which patients could be cured of disease but the disease event time may be subject to interval censoring, we presents a semiparametric non-mixture cure model for the regression analysis of interval-censored time-to-event datxa. We develop semiparametric maximum likelihood estimation for the model using the expectation-maximization method for interval-censored data. The maximization step for the baseline function is nonparametric and numerically challenging. We develop an efficient and numerically stable algorithm via modern convex optimization techniques, yielding a self-consistency algorithm for the maximization step. We prove the strong consistency of the maximum likelihood estimators under the Hellinger distance, which is an appropriate metric for the asymptotic property of the estimators for interval-censored data. We assess the performance of the estimators in a simulation study with small to moderate sample sizes. To illustrate the method, we also analyze a real data set from a medical study for the biochemical recurrence of prostate cancer among patients who have undergone radical prostatectomy. Supplemental materials for the computational algorithm are available online.","527":"We develop a model for stochastic processes with random marginal distributions. Our model relies on a stick-breaking construction for the marginal distribution of the process, and introduces dependence across locations by using a latent Gaussian copula model as the mechanism for selecting the atoms. The resulting latent stick-breaking process (LaSBP) induces a random partition of the index space, with points closer in space having a higher probability of being in the same cluster. We develop an efficient and straightforward Markov chain Monte Carlo (MCMC) algorithm for computation and discuss applications in financial econometrics and ecology. This article has supplementary material online.","528":"Motivated by DNA copy number variation (CNV) analysis based on high-density single nucleotide polymorphism (SNP) data, we consider the problem of detecting and identifying sparse short segments in a long one-dimensional sequence of data with additive Gaussian white noise, where the number, length and location of the segments are unknown. We present a statistical characterization of the identifiable region of a segment where it is possible to reliably separate the segment from noise. An efficient likelihood ratio selection (LRS) procedure for identifying the segments is developed, and the asymptotic optimality of this method is presented in the sense that the LRS can separate the signal segments from the noise as long as the signal segments are in the identifiable regions. The proposed method is demonstrated with simulations and analysis of a real data set on identification of copy number variants based on high-density SNP data. The results show that the LRS procedure can yield greater gain in power for detecting the true segments than some standard signal identification methods.","529":"Local polynomial estimators are popular techniques for nonparametric regression estimation and have received great attention in the literature. Their simplest version, the local constant estimator, can be easily extended to the errors-in-variables context by exploiting its similarity with the deconvolution kernel density estimator. The generalization of the higher order versions of the estimator, however, is not straightforward and has remained an open problem for the last 15 years. We propose an innovative local polynomial estimator of any order in the errors-in-variables context, derive its design-adaptive asymptotic properties and study its finite sample performance on simulated examples. We provide not only a solution to a long-standing open problem, but also provide methodological contributions to error-invariable regression, including local polynomial estimation of derivative functions.","530":"We introduce new variance estimation procedures for second-order statistics that are computed from a single realization of intensity reweighted stationary spatial point processes. The statistics are defined either on a subset B of the observation window or on the whole window. For the former, we use subblocks that have the same size and shape as B as \"replicates\" of B in order to estimate the target variance. For the latter, we develop a subsampling estimator for a key component in the target variance and estimate its other components by method-of-moment methods. Under some suitable conditions, we prove that the proposed variance estimators are consistent for the target variances in both cases. Simulations and an application to a real data example are used to demonstrate the usefulness of the proposed methods. This article has supplemental material online.","531":"Classical prediction methods such as Fisher's linear discriminant function were designed for small-scale problems, where the number of predictors N is much smaller than the number of observations n. Modern scientific devices often reverse this situation. A microarray analysis, for example, might include n = 100 subjects measured on N = 10,000 genes, each of which is a potential predictor. This paper proposes an empirical Bayes approach to large-scale prediction, where the optimum Bayes prediction rule is estimated employing the data from all the predictors. Microarray examples are used to illustrate the method. The results show a close connection with the shrunken centroids algorithm of Tibshirani et al. (2002), a frequentist regularization approach to large-scale prediction, and also with false discovery rate theory.","532":"Regression quantiles can be substantially biased when the covariates are measured with error. In this paper we propose a new method that produces consistent linear quantile estimation in the presence of covariate measurement error. The method corrects the measurement error induced bias by constructing joint estimating equations that simultaneously hold for all the quantile levels. An iterative EM-type estimation algorithm to obtain the solutions to such joint estimation equations is provided. The finite sample performance of the proposed method is investigated in a simulation study, and compared to the standard regression calibration approach. Finally, we apply our methodology to part of the National Collaborative Perinatal Project growth data, a longitudinal study with an unusual measurement error structure.","533":"Altered alternative splicing has been identified as an important factor in tumorigenesis. The Affymetrix exon tiling array is designed for detecting alternative splicing events in a transcriptome-wide fashion; however, there are currently few analysis tools that are well studied for effective detection of alternative splicing events. We propose a new screening procedure based on singular value decomposition (SVD) of the residual matrix from a robust additive model fit to probe selection region (PSR) data. With this approach, we analyze the exon tiling array data from a brain cancer study conducted at the M. D. Anderson Cancer Center, and show that the proposed SVD-based approach is able to better accommodate outlying measures and capitalize on the multidimensional group-by-PSR gene expression profiles for more effective detection of group-specific alternative splicing events as well as the PSRs that are most likely associated with the alternative splicing. Lab validation confirmed some of our findings, but the list of candidates detected with our proposed method may provide a better signpost to guide further investigations.","534":"Interest in predicting protein backbone conformational angles has prompted the development of modeling and inference procedures for bivariate angular distributions. We present a Bayesian approach to density estimation for bivariate angular data that uses a Dirichlet process mixture model and a bivariate von Mises distribution. We derive the necessary full conditional distributions to fit the model, as well as the details for sampling from the posterior predictive distribution. We show how our density estimation method makes it possible to improve current approaches for protein structure prediction by comparing the performance of the so-called \"whole\" and \"half\" position distributions. Current methods in the field are based on whole position distributions, as density estimation for the half positions requires techniques, such as ours, that can provide good estimates for small datasets. With our method we are able to demonstrate that half position data provides a better approximation for the distribution of conformational angles at a given sequence position, therefore providing increased efficiency and accuracy in structure prediction.","535":"In this paper we develop a method to estimate both individual social network size (i.e., degree) and the distribution of network sizes in a population by asking respondents how many people they know in specific subpopulations (e.g., people named Michael). Building on the scale-up method of Killworth et al. (1998b) and other previous attempts to estimate individual network size, we propose a latent non-random mixing model which resolves three known problems with previous approaches. As a byproduct, our method also provides estimates of the rate of social mixing between population groups. We demonstrate the model using a sample of 1,370 adults originally collected by McCarty et al. (2001). Based on insights developed during the statistical modeling, we conclude by offering practical guidelines for the design of future surveys to estimate social network size. Most importantly, we show that if the first names to be asked about are chosen properly, the simple scale-up degree estimates can enjoy the same bias-reduction as that from the our more complex latent non-random mixing model.","536":"State-space models provide an important body of techniques for analyzing time-series, but their use requires estimating unobserved states. The optimal estimate of the state is its conditional expectation given the observation histories, and computing this expectation is hard when there are nonlinearities. Existing filtering methods, including sequential Monte Carlo, tend to be either inaccurate or slow. In this paper, we study a nonlinear filter for nonlinear\/non-Gaussian state-space models, which uses Laplace's method, an asymptotic series expansion, to approximate the state's conditional mean and variance, together with a Gaussian conditional distribution. This Laplace-Gaussian filter (LGF) gives fast, recursive, deterministic state estimates, with an error which is set by the stochastic characteristics of the model and is, we show, stable over time. We illustrate the estimation ability of the LGF by applying it to the problem of neural decoding and compare it to sequential Monte Carlo both in simulations and with real data. We find that the LGF can deliver superior results in a small fraction of the computing time.","537":"The aim of this paper is to develop an intrinsic regression model for the analysis of positive-definite matrices as responses in a Riemannian manifold and their association with a set of covariates, such as age and gender, in a Euclidean space. The primary motivation and application of the proposed methodology is in medical imaging. Because the set of positive-definite matrices do not form a vector space, directly applying classical multivariate regression may be inadequate in establishing the relationship between positive-definite matrices and covariates of interest, such as age and gender, in real applications. Our intrinsic regression model, which is a semiparametric model, uses a link function to map from the Euclidean space of covariates to the Riemannian manifold of positive-definite matrices. We develop an estimation procedure to calculate parameter estimates and establish their limiting distributions. We develop score statistics to test linear hypotheses on unknown parameters and develop a test procedure based on a resampling method to simultaneously assess the statistical significance of linear hypotheses across a large region of interest. Simulation studies are used to demonstrate the methodology and examine the finite sample performance of the test procedure for controlling the family-wise error rate. We apply our methods to the detection of statistical significance of diagnostic effects on the integrity of white matter in a diffusion tensor study of human immunodeficiency virus. Supplemental materials for this article are available online.","538":"The mean residual life function is an attractive alternative to the survival function or the hazard function of a survival time in practice. It provides the remaining life expectancy of a subject surviving up to time t. In this study, we propose a class of transformed mean residual life models for fitting survival data under right censoring. To estimate the model parameters, we make use of the inverse probability of censoring weighting approach and develop a system of estimating equations. Efficiency and robustness of the estimators are also studied. Both asymptotic and finite sample properties of the proposed estimators are established and the approach is applied to two real-life datasets collected from clinical trials.","539":"Joint models for the association of a longitudinal binary and a longitudinal continuous process are proposed for situations in which their association is of direct interest. The models are parameterized such that the dependence between the two processes is characterized by unconstrained regression coefficients. Bayesian variable selection techniques are used to parsimoniously model these coefficients. A Markov chain Monte Carlo (MCMC) sampling algorithm is developed for sampling from the posterior distribution, using data augmentation steps to handle missing data. Several technical issues are addressed to implement the MCMC algorithm efficiently. The models are motivated by, and are used for, the analysis of a smoking cessation clinical trial in which an important question of interest was the effect of the (exercise) treatment on the relationship between smoking cessation and weight gain.","540":"Self-rated health is an important indicator of future morbidity and mortality. Past research has indicated that self-rated health is related to both levels of and changes in physical functioning. However, no previous study has jointly modeled longitudinal functional status and self-rated health trajectories. We propose a joint model for self-rated health and physical functioning that describes the relationship between perceptions of health and the rate of change of physical functioning or disability. Our joint model uses a non-homogeneous Markov process for discrete physical functioning states and connects this to a logistic regression model for \"healthy\" versus \"unhealthy\" self-rated health through parameters of the physical functioning model. We use simulation studies to establish finite sample properties of our estimators and show that this model is robust to misspecification of the functional form of the relationship between self-rated health and rate of change of physical functioning. We also show that our joint model performs better than an empirical model based on observed changes in functional status. We apply our joint model to data from the Cardiovascular Health Study (CHS), a large, multi-center, longitudinal study of older adults. Our analysis indicates that self-rated health is associated both with level of functioning as indicated by difficulty with activities of daily living (ADL) and instrumental activities of daily living (IADL), and the risk of increasing difficulty with ADLs and IADLs.","541":"Hierarchical classification is critical to knowledge management and exploration, as in gene function prediction and document categorization. In hierarchical classification, an input is classified according to a structured hierarchy. In a situation as such, the central issue is how to effectively utilize the inter-class relationship to improve the generalization performance of flat classification ignoring such dependency. In this article, we propose a novel large margin method through constraints characterizing a multi-path hierarchy, where class membership can be non-exclusive. The proposed method permits a treatment of various losses for hierarchical classification. For implementation, we focus on the symmetric difference loss and two large margin classifiers: support vector machines and psi-learning. Finally, theoretical and numerical analyses are conducted, in addition to an application to gene function prediction. They suggest that the proposed method achieves the desired objective and outperforms strong competitors in the literature.","542":"Microsimulation models that describe disease processes synthesize information from multiple sources and can be used to estimate the effects of screening and treatment on cancer incidence and mortality at a population level. These models are characterized by simulation of individual event histories for an idealized population of interest. Microsimulation models are complex and invariably include parameters that are not well informed by existing data. Therefore, a key component of model development is the choice of parameter values. Microsimulation model parameter values are selected to reproduce expected or known results though the process of model calibration. Calibration may be done by perturbing model parameters one at a time or by using a search algorithm. As an alternative, we propose a Bayesian method to calibrate microsimulation models that uses Markov chain Monte Carlo. We show that this approach converges to the target distribution and use a simulation study to demonstrate its finite-sample performance. Although computationally intensive, this approach has several advantages over previously proposed methods, including the use of statistical criteria to select parameter values, simultaneous calibration of multiple parameters to multiple data sources, incorporation of information via prior distributions, description of parameter identifiability, and the ability to obtain interval estimates of model parameters. We develop a microsimulation model for colorectal cancer and use our proposed method to calibrate model parameters. The microsimulation model provides a good fit to the calibration data. We find evidence that some parameters are identified primarily through prior distributions. Our results underscore the need to incorporate multiple sources of variability (i.e., due to calibration data, unknown parameters, and estimated parameters and predicted values) when calibrating and applying microsimulation models.","543":"We introduce methods for signal and associated variability estimation based on hierarchical nonparametric smoothing with application to the Sleep Heart Health Study (SHHS). SHHS is the largest electroencephalographic (EEG) collection of sleep-related data, which contains, at each visit, two quasi-continuous EEG signals for each subject. The signal features extracted from EEG data are then used in second level analyses to investigate the relation between health, behavioral, or biometric outcomes and sleep. Using subject specific signals estimated with known variability in a second level regression becomes a nonstandard measurement error problem. We propose and implement methods that take into account cross-sectional and longitudinal measurement error. The research presented here forms the basis for EEG signal processing for the SHHS.","544":"Nonparametric varying-coefficient models are commonly used for analysis of data measured repeatedly over time, including longitudinal and functional responses data. While many procedures have been developed for estimating the varying-coefficients, the problem of variable selection for such models has not been addressed. In this article, we present a regularized estimation procedure for variable selection that combines basis function approximations and the smoothly clipped absolute deviation (SCAD) penalty. The proposed procedure simultaneously selects significant variables with time-varying effects and estimates the nonzero smooth coefficient functions. Under suitable conditions, we have established the theoretical properties of our procedure, including consistency in variable selection and the oracle property in estimation. Here the oracle property means that the asymptotic distribution of an estimated coefficient function is the same as that when it is known a priori which variables are in the model. The method is illustrated with simulations and two real data examples, one for identifying risk factors in the study of AIDS and one using microarray time-course gene expression data to identify the transcription factors related to the yeast cell cycle process.","545":"This article focuses on variable selection for partially linear models when the covariates are measured with additive errors. We propose two classes of variable selection procedures, penalized least squares and penalized quantile regression, using the nonconvex penalized principle. The first procedure corrects the bias in the loss function caused by the measurement error by applying the so-called correction-for-attenuation approach, whereas the second procedure corrects the bias by using orthogonal regression. The sampling properties for the two procedures are investigated. The rate of convergence and the asymptotic normality of the resulting estimates are established. We further demonstrate that, with proper choices of the penalty functions and the regularization parameter, the resulting estimates perform asymptotically as well as an oracle procedure (Fan and Li 2001). Choice of smoothing parameters is also discussed. Finite sample performance of the proposed variable selection procedures is assessed by Monte Carlo simulation studies. We further illustrate the proposed procedures by an application.","546":null,"547":"In close elections, the losing side has an incentive to obtain evidence that the election result is incorrect. Sometimes this evidence comes in the form of court testimony from a sample of invalid voters, and this testimony is used to adjust vote totals (Borders v King County, 2005; Belcher v Mayor of Ann Arbor, 1978). However, while courts may be reluctant to make explicit findings about out-of-sample data (e.g. invalid voters that do not testify), when samples are used to adjust vote totals, the court is implicitly making findings about this out-of-sample data. In this paper, we show that the practice of adjusting vote totals on the basis of potentially unrepresentative samples can lead to incorrectly voided election results. More generally, we show that given the difficulties of sampling and non-response in this context, even when frame error is minimal and if voter testimony is accurate, such testimony has limited power to detect incorrect election results without precinct level polarization or the acceptance of large Type I error rates. Therefore in U.S. election disputes, even high quality post-vote vote-choice data will often not be sufficient to resolve contested elections without modeling assumptions (whether or not these assumptions are acknowledged).","548":"Multiplicative regression model or accelerated failure time model, which becomes linear regression model after logarithmic transformation, is useful in analyzing data with positive responses, such as stock prices or life times, that are particularly common in economic\/financial or biomedical studies. Least squares or least absolute deviation are among the most widely used criterions in statistical estimation for linear regression model. However, in many practical applications, especially in treating, for example, stock price data, the size of relative error, rather than that of error itself, is the central concern of the practitioners. This paper offers an alternative to the traditional estimation methods by considering minimizing the least absolute relative errors for multiplicative regression models. We prove consistency and asymptotic normality and provide an inference approach via random weighting. We also specify the error distribution, with which the proposed least absolute relative errors estimation is efficient. Supportive evidence is shown in simulation studies. Application is illustrated in an analysis of stock returns in Hong Kong Stock Exchange.","549":"We present a mixture cure model with the survival time of the \"uncured\" group coming from a class of linear transformation models, which is an extension of the proportional odds model. This class of model, first proposed by Dabrowska and Doksum (1988), which we term \"generalized proportional odds model,\" is well suited for the mixture cure model setting due to a clear separation between long-term and short-term effects. A standard expectation-maximization algorithm can be employed to locate the nonparametric maximum likelihood estimators, which are shown to be consistent and semiparametric efficient. However, there are difficulties in the M-step due to the nonparametric component. We overcome these difficulties by proposing two different algorithms. The first is to employ an majorize-minimize (MM) algorithm in the M-step instead of the usual Newton-Raphson method, and the other is based on an alternative form to express the model as a proportional hazards frailty model. The two new algorithms are compared in a simulation study with an existing estimating equation approach by Lu and Ying (2004). The MM algorithm provides both computational stability and efficiency. A case study of leukemia data is conducted to illustrate the proposed procedures.","550":null,"551":"We propose a novel alternative to case-control sampling for the estimation of individual-level risk in spatial epidemiology. Our approach uses weighted estimating equations to estimate regression parameters in the intensity function of an inhomogeneous spatial point process, when information on risk-factors is available at the individual level for cases, but only at a spatially aggregated level for the population at risk. We develop data-driven methods to select the weights used in the estimating equations and show through simulation that the choice of weights can have a major impact on efficiency of estimation. We develop a formal test to detect non-Poisson behavior in the underlying point process and assess the performance of the test using simulations of Poisson and Poisson cluster point processes. We apply our methods to data on the spatial distribution of childhood meningococcal disease cases in Merseyside, U.K. between 1981 and 2007.","552":"Temperature control for a large data center is both important and expensive. On the one hand, many of the components produce a great deal of heat, and on the other hand, many of the components require temperatures below a fairly low threshold for reliable operation. A statistical framework is proposed within which the behavior of a large cooling system can be modeled and forecast under both steady state and perturbations. This framework is based upon an extension of multivariate Gaussian autoregressive hidden Markov models (HMMs). The estimated parameters of the fitted model provide useful summaries of the overall behavior of and relationships within the cooling system. Predictions under system perturbations are useful for assessing potential changes and improvements to be made to the system. Many data centers have far more cooling capacity than necessary under sensible circumstances, thus resulting in energy inefficiencies. Using this model, predictions for system behavior after a particular component of the cooling system is shut down or reduced in cooling power can be generated. Steady-state predictions are also useful for facility monitors. System traces outside control boundaries flag a change in behavior to examine. The proposed model is fit to data from a group of air conditioners within an enterprise data center from the IT industry. The fitted model is examined, and a particular unit is found to be underutilized. Predictions generated for the system under the removal of that unit appear very reasonable. Steady-state system behavior also is predicted well.","553":"A long-standing problem in clinical research is distinguishing drug treated subjects that respond due to specific effects of the drug from those that respond to non-specific (or placebo) effects of the treatment. Linear mixed effect models are commonly used to model longitudinal clinical trial data. In this paper we present a solution to the problem of identifying placebo responders using an optimal partitioning methodology for linear mixed effects models. Since individual outcomes in a longitudinal study correspond to curves, the optimal partitioning methodology produces a set of prototypical outcome profiles. The optimal partitioning methodology can accommodate both continuous and discrete covariates. The proposed partitioning strategy is compared and contrasted with the growth mixture modelling approach. The methodology is applied to a two-phase depression clinical trial where subjects in a first phase were treated openly for 12 weeks with fluoxetine followed by a double blind discontinuation phase where responders to treatment in the first phase were randomized to either stay on fluoxetine or switched to a placebo. The optimal partitioning methodology is applied to the first phase to identify prototypical outcome profiles. Using time to relapse in the second phase of the study, a survival analysis is performed on the partitioned data. The optimal partitioning results identify prototypical profiles that distinguish whether subjects relapse depending on whether or not they stay on the drug or are randomized to a placebo.","554":"In this manuscript we are concerned with functional imaging of the colon to assess the kinetics of microbicide lubricants. The overarching goal is to understand the distribution of the lubricants in the colon. Such information is crucial for understanding the potential impact of the microbicide on HIV viral transmission. The experiment was conducted by imaging a radiolabeled lubricant distributed in the subject's colon. The tracer imaging was conducted via single photon emission computed tomography (SPECT), a non-invasive, in-vivo functional imaging technique. We develop a novel principal curve algorithm to construct a three dimensional curve through the colon images. The developed algorithm is tested and debugged on several difficult two-dimensional images of familiar curves where the original principal curve algorithm does not apply. The final curve fit to the colon data is compared with experimental sigmoidoscope collection.","555":"We analyze data collected in a somatic embryogenesis experiment carried out on Zea mays at Iowa State University. The main objective of the study was to identify the set of genes in maize that actively participate in embryo development. Embryo tissue was sampled and analyzed at various time periods and under different mediums and light conditions. As is the case in many microarray experiments, the operator scanned each slide multiple times to find the slide-specific 'optimal' laser and sensor settings. The multiple readings of each slide are repeated measurements on different scales with differing censoring; they cannot be considered to be replicate measurements in the traditional sense. Yet it has been shown that the choice of reading can have an impact on genetic inference. We propose a hierarchical modeling approach to estimating gene expression that combines all available readings on each spot and accounts for censoring in the observed values. We assess the statistical properties of the proposed expression estimates using a simulation experiment. As expected, combining all available scans using an approach with good statistical properties results in expression estimates with noticeably lower bias and root mean squared error relative to other approaches that have been proposed in the literature. Inferences drawn from the somatic embryogenesis experiment, which motivated this work changed drastically when data were analyzed using the standard approaches or using the methodology we propose.","556":"In medical imaging analysis and computer vision, there is a growing interest in analyzing various manifold-valued data including 3D rotations, planar shapes, oriented or directed directions, the Grassmann manifold, deformation field, symmetric positive definite (SPD) matrices and medial shape representations (m-rep) of subcortical structures. Particularly, the scientific interests of most population studies focus on establishing the associations between a set of covariates (e.g., diagnostic status, age, and gender) and manifold-valued data for characterizing brain structure and shape differences, thus requiring a regression modeling framework for manifold-valued data. The aim of this paper is to develop an intrinsic regression model for the analysis of manifold-valued data as responses in a Riemannian manifold and their association with a set of covariates, such as age and gender, in Euclidean space. Because manifold-valued data do not form a vector space, directly applying classical multivariate regression may be inadequate in establishing the relationship between manifold-valued data and covariates of interest, such as age and gender, in real applications. Our intrinsic regression model, which is a semiparametric model, uses a link function to map from the Euclidean space of covariates to the Riemannian manifold of manifold data. We develop an estimation procedure to calculate an intrinsic least square estimator and establish its limiting distribution. We develop score statistics to test linear hypotheses on unknown parameters. We apply our methods to the detection of the difference in the morphological changes of the left and right hippocampi between schizophrenia patients and healthy controls using medial shape description.","557":"Differential equation (DE) models are widely used in many scientific fields that include engineering, physics and biomedical sciences. The so-called \"forward problem\", the problem of simulations and predictions of state variables for given parameter values in the DE models, has been extensively studied by mathematicians, physicists, engineers and other scientists. However, the \"inverse problem\", the problem of parameter estimation based on the measurements of output variables, has not been well explored using modern statistical methods, although some least squares-based approaches have been proposed and studied. In this paper, we propose parameter estimation methods for ordinary differential equation models (ODE) based on the local smoothing approach and a pseudo-least squares (PsLS) principle under a framework of measurement error in regression models. The asymptotic properties of the proposed PsLS estimator are established. We also compare the PsLS method to the corresponding SIMEX method and evaluate their finite sample performances via simulation studies. We illustrate the proposed approach using an application example from an HIV dynamic study.","558":"This article considers a methodology for flexibly characterizing the relationship between a response and multiple predictors. Goals are (1) to estimate the conditional response distribution addressing the distributional changes across the predictor space, and (2) to identify important predictors for the response distribution change both within local regions and globally. We first introduce the probit stick-breaking process (PSBP) as a prior for an uncountable collection of predictor-dependent random distributions and propose a PSBP mixture (PSBPM) of normal regressions for modeling the conditional distributions. A global variable selection structure is incorporated to discard unimportant predictors, while allowing estimation of posterior inclusion probabilities. Local variable selection is conducted relying on the conditional distribution estimates at different predictor points. An efficient stochastic search sampling algorithm is proposed for posterior computation. The methods are illustrated through simulation and applied to an epidemiologic study.","559":"A sensitivity analysis displays the increase in uncertainty that attends an inference when a key assumption is relaxed. In matched observational studies of treatment effects, a key assumption in some analyses is that subjects matched for observed covariates are comparable, and this assumption is relaxed by positing a relevant covariate that was not observed and not controlled by matching. What properties would such an unobserved covariate need to have to materially alter the inference about treatment effects? For ease of calculation and reporting, it is convenient that the sensitivity analysis be of low dimension, perhaps indexed by a scalar sensitivity parameter, but for interpretation in specific contexts, a higher dimensional analysis may be of greater relevance. An amplification of a sensitivity analysis is defined as a map from each point in a low dimensional sensitivity analysis to a set of points, perhaps a 'curve,' in a higher dimensional sensitivity analysis such that the possible inferences are the same for all points in the set. Possessing an amplification, an investigator may calculate and report the low dimensional analysis, yet have available the interpretations of the higher dimensional analysis.","560":"","561":"Stochastic noise, susceptibility artifacts, magnetic field and radiofrequency inhomogeneities, and other noise components in magnetic resonance images (MRIs) can introduce serious bias into any measurements made with those images. We formally introduce three regression models including a Rician regression model and two associated normal models to characterize stochastic noise in various magnetic resonance imaging modalities, including diffusion-weighted imaging (DWI) and functional MRI (fMRI). Estimation algorithms are introduced to maximize the likelihood function of the three regression models. We also develop a diagnostic procedure for systematically exploring MR images to identify noise components other than simple stochastic noise, and to detect discrepancies between the fitted regression models and MRI data. The diagnostic procedure includes goodness-of-fit statistics, measures of influence, and tools for graphical display. The goodness-of-fit statistics can assess the key assumptions of the three regression models, whereas measures of influence can isolate outliers caused by certain noise components, including motion artifacts. The tools for graphical display permit graphical visualization of the values for the goodness-of-fit statistic and influence measures. Finally, we conduct simulation studies to evaluate performance of these methods, and we analyze a real dataset to illustrate how our diagnostic procedure localizes subtle image artifacts by detecting intravoxel variability that is not captured by the regression models.","562":"In this paper, we propose a computationally efficient approach -space(Sparse PArtial Correlation Estimation)- for selecting non-zero partial correlations under the high-dimension-low-sample-size setting. This method assumes the overall sparsity of the partial correlation matrix and employs sparse regression techniques for model fitting. We illustrate the performance of space by extensive simulation studies. It is shown that space performs well in both non-zero partial correlation selection and the identification of hub variables, and also outperforms two existing methods. We then apply space to a microarray breast cancer data set and identify a set of hub genes which may provide important insights on genetic regulatory networks. Finally, we prove that, under a set of suitable assumptions, the proposed procedure is asymptotically consistent in terms of model selection and parameter estimation.","563":"Kinetic analysis is used to extract metabolic information from dynamic positron emission tomography (PET) uptake data. The theory of indicator dilutions, developed in the seminal work of Meier and Zierler (1954), provides a probabilistic framework for representation of PET tracer uptake data in terms of a convolution between an arterial input function and a tissue residue. The residue is a scaled survival function associated with tracer residence in the tissue. Nonparametric inference for the residue, a deconvolution problem, provides a novel approach to kinetic analysis-critically one that is not reliant on specific compartmental modeling assumptions. A practical computational technique based on regularized cubic B-spline approximation of the residence time distribution is proposed. Nonparametric residue analysis allows formal statistical evaluation of specific parametric models to be considered. This analysis needs to properly account for the increased flexibility of the nonparametric estimator. The methodology is illustrated using data from a series of cerebral studies with PET and fluorodeoxyglucose (FDG) in normal subjects. Comparisons are made between key functionals of the residue, tracer flux, flow, etc., resulting from a parametric (the standard two-compartment of Phelps et al. 1979) and a nonparametric analysis. Strong statistical evidence against the compartment model is found. Primarily these differences relate to the representation of the early temporal structure of the tracer residence-largely a function of the vascular supply network. There are convincing physiological arguments against the representations implied by the compartmental approach but this is the first time that a rigorous statistical confirmation using PET data has been reported. The compartmental analysis produces suspect values for flow but, notably, the impact on the metabolic flux, though statistically significant, is limited to deviations on the order of 3%-4%. The general advantage of the nonparametric residue analysis is the ability to provide a valid kinetic quantitation in the context of studies where there may be heterogeneity or other uncertainty about the accuracy of a compartmental model approximation of the tissue residue.","564":"Ecologists use the relative abundance of fossil pollen in sediments to estimate how tree species abundances change over space and time. To predict historical forest composition and quantify the available information, we build a Bayesian hierarchical model of forest composition in central New England, USA, based on pollen in a network of ponds. The critical relationships between abundances of taxa in the pollen record and abundances as actual vegetation are estimated for the modern and colonial periods, for which both pollen and direct vegetation data are available, based on a latent multivariate spatial process representing forest composition. For time periods in the past with only pollen data, we use the estimated model parameters to constrain predictions about the latent spatio-temporal process conditional on the pollen data. We develop an innovative graphical assessment of feature significance to help to infer which spatial patterns are reliably estimated. The model allows us to estimate the spatial distribution and relative abundances of tree species over the last 2500 years, with an assessment of uncertainty, and to draw inference about how these patterns have changed over time. Cross-validation suggests that our feature significance approach can reliably indicate certain large-scale spatial features for many taxa, but that features on scales smaller than 50 km are difficult to distinguish, as are large-scale features for some taxa. We also use the model to quantitatively investigate ecological hypotheses, including covariate effects on taxa abundances and questions about pollen dispersal characteristics. The critical advantages of our modeling approach over current ecological analyses are the explicit spatio-temporal representation, quantification of abundance on the scale of trees rather than pollen, and uncertainty characterization.","565":"Motivated by a problem encountered in the analysis of cell cycle gene expression data, this article deals with the estimation of parameters subject to order restrictions on a unit circle. A normal eukaryotic cell cycle has four major phases during cell division, and a cell cycle gene has its peak expression (phase angle) during the phase that may correspond to its biological function. Because the phases are ordered along a circle, the phase angles of cell cycle genes are ordered unknown parameters on a unit circle. The problem of interest is to estimate the phase angles using the information regarding the order among them. We address this problem by developing a circular version of the well-known isotonic regression for Euclidean data. Because of the underlying geometry, the standard pool adjacent violator algorithm (PAVA) cannot be used for deriving the circular isotonic regression estimator (CIRE). However, PAVA can be modified to obtain a computationally efficient algorithm for deriving the CIRE. We illustrate the CIRE by estimating the phase angles of some of well-known cell cycle genes using the unrestricted estimators obtained in the literature.","566":"We propose, develop, and implement a fully Bayesian inferential approach for the Cox model when the log hazard function contains unknown smooth functions of the variables measured with error. Our approach is to model nonparametrically both the log-baseline hazard and the smooth components of the log-hazard functions using low-rank penalized splines. Careful implementation of the Bayesian inferential machinery is shown to produce remarkably better results than the naive approach. Our methodology was motivated by and applied to the study of progression time to chronic kidney disease as a function of baseline kidney function and applied to the Atherosclerosis Risk in Communities study, a large epidemiological cohort study. This article has supplementary material online.","567":"Right-censored time-to-event data are often observed from a cohort of prevalent cases that are subject to length-biased sampling. Informative right censoring of data from the prevalent cohort within the population often makes it difficult to model risk factors on the unbiased failure times for the general population, because the observed failure times are length biased. In this paper, we consider two classes of flexible semiparametric models: the transformation models and the accelerated failure time models, to assess covariate effects on the population failure times by modeling the length-biased times. We develop unbiased estimating equation approaches to obtain the consistent estimators of the regression coefficients. Large sample properties for the estimators are derived. The methods are confirmed through simulations and illustrated by application to data from a study of a prevalent cohort of dementia patients.","568":null,"569":"We describe a class of log-linear models for the detection of interactions in high-dimensional genomic data. This class of models leads to a Bayesian model selection algorithm that can be applied to data that have been reduced to contingency tables using ranks of observations within subjects, and discretization of these ranks within gene\/network components. Many normalization issues associated with the analysis of genomic data are thereby avoided. A prior density based on Ewens' sampling distribution is used to restrict the number of interacting components assigned high posterior probability, and the calculation of posterior model probabilities is expedited by approximations based on the likelihood ratio statistic. Simulation studies are used to evaluate the efficiency of the resulting algorithm for known interaction structures. Finally, the algorithm is validated in a microarray study for which it was possible to obtain biological confirmation of detected interactions.","570":"The distribution of genetic variation among populations is conveniently measured by Wright's F(ST), which is a scaled variance taking on values in [0,1]. For certain types of genetic markers, and for single-nucleotide polymorphisms (SNPs) in particular, it is reasonable to presume that allelic differences at most loci are selectively neutral. For such loci, the distribution of genetic variation among populations is determined by the size of local populations, the pattern and rate of migration among those populations, and the rate of mutation. Because the demographic parameters (population sizes and migration rates) are common across all autosomal loci, locus-specific estimates of F(ST) will depart from a common distribution only for loci with unusually high or low rates of mutation or for loci that are closely associated with genomic regions having a relationship with fitness. Thus, loci that are statistical outliers showing significantly more among-population differentiation than others may mark genomic regions subject to diversifying selection among the sample populations. Similarly, statistical outliers showing significantly less differentiation among populations than others may mark genomic regions subject to stabilizing selection across the sample populations. We propose several Bayesian hierarchical models to estimate locus-specific effects on F(ST), and we apply these models to single nucleotide polymorphism data from the HapMap project. Because loci that are physically associated with one another are likely to show similar patterns of variation, we introduce conditional autoregressive models to incorporate the local correlation among loci for high-resolution genomic data. We estimate the posterior distributions of model parameters using Markov chain Monte Carlo (MCMC) simulations. Model comparison using several criteria, including DIC and LPML, reveals that a model with locus- and population-specific effects is superior to other models for the data used in the analysis. To detect statistical outliers we propose an approach that measures divergence between the posterior distributions of locus-specific effects and the common F(ST) with the Kullback-Leibler divergence measure. We calibrate this measure by comparing values with those produced from the divergence between a biased and a fair coin. We conduct a simulation study to illustrate the performance of our approach for detecting loci subject to stabilizing\/divergent selection, and we apply the proposed models to low- and high-resolution SNP data from the HapMap project. Model comparison using DIC and LPML reveals that CAR models are superior to alternative models for the high resolution data. For both low and high resolution data, we identify statistical outliers that are associated with known genes.","571":"In studies of the accuracy of diagnostic tests, it is common that both the diagnostic test itself and the reference test are imperfect. This is the case for the microsatellite instability test, which is routinely used as a prescreening procedure to identify individuals with Lynch syndrome, the most common hereditary colorectal cancer syndrome. The microsatellite instability test is known to have imperfect sensitivity and specificity. Meanwhile, the reference test, mutation analysis, is also imperfect. We evaluate this test via a random effects meta-analysis of 17 studies. Study-specific random effects account for between-study heterogeneity in mutation prevalence, test sensitivities and specificities under a nonlinear mixed effects model and a Bayesian hierarchical model. Using model selection techniques, we explore a range of random effects models to identify a best-fitting model. We also evaluate sensitivity to the conditional independence assumption between the microsatellite instability test and the mutation analysis by allowing for correlation between them. Finally, we use simulations to illustrate the importance of including appropriate random effects and the impact of overfitting, underfitting, and misfitting on model performance. Our approach can be used to estimate the accuracy of two imperfect diagnostic tests from a meta-analysis of multiple studies or a multicenter study when the prevalence of disease, test sensitivities and\/or specificities may be heterogeneous among studies or centers.","572":"We study several theoretical properties of Jeffreys's prior for binomial regression models. We show that Jeffreys's prior is symmetric and unimodal for a class of binomial regression models. We characterize the tail behavior of Jeffreys's prior by comparing it with the multivariate t and normal distributions under the commonly used logistic, probit, and complementary log-log regression models. We also show that the prior and posterior normalizing constants under Jeffreys's prior are linear transformation-invariant in the covariates. We further establish an interesting theoretical connection between the Bayes information criterion and the induced dimension penalty term using Jeffreys's prior for binomial regression models with general links in variable selection problems. Moreover, we develop an importance sampling algorithm for carrying out prior and posterior computations under Jeffreys's prior. We analyze a real data set to illustrate the proposed methodology.","573":"Case-control association studies often aim to investigate the role of genes and gene-environment interactions in terms of the underlying haplotypes (i.e., the combinations of alleles at multiple genetic loci along chromosomal regions). The goal of this article is to develop robust but efficient approaches to the estimation of disease odds-ratio parameters associated with haplotypes and haplotype-environment interactions. We consider \"shrinkage\" estimation techniques that can adaptively relax the model assumptions of Hardy-Weinberg-Equilibrium and gene-environment independence required by recently proposed efficient \"retrospective\" methods. Our proposal involves first development of a novel retrospective approach to the analysis of case-control data, one that is robust to the nature of the gene-environment distribution in the underlying population. Next, it involves shrinkage of the robust retrospective estimator toward a more precise, but model-dependent, retrospective estimator using novel empirical Bayes and penalized regression techniques. Methods for variance estimation are proposed based on asymptotic theories. Simulations and two data examples illustrate both the robustness and efficiency of the proposed methods.","574":"We develop a mixed model to capture the complex stochastic nature of tobacco abuse and dependence. This model describes transition processes among addiction and nonaddiction stages. An important innovation of our model is allowing an unobserved cure state, or permanent quitting, in contrast to transient quitting. This distinction is necessary to model data from situations where censoring prevents unambiguous determination that a person has been \"cured.\" Moreover, the processes that describe transient and permanent quitting are likely to be different and have different policy-making implications. For example, when analyzing factors that influence smoking and can be targeted by interventions, it is more important to target those factors that are associated with permanent quitting rather than transient quitting.We apply our methodology to the Alpha-Tocopherol, Beta-Carotene Cancer Prevention (ATBC) study, a large (29,133 participants) longitudinal cohort study. While ATBC was designed as a cancer prevention study, it contains unique information about the smoking status of each participant during every 4-month period of the study. These data are used to model smoking cessation patterns using a discrete-time stochastic mixed-effects model with three states: smoking, transient cessation, and permanent cessation (absorbent state). Random participant-specific transition probabilities among these states are used to account for participant-to-participant heterogeneity. Another important innovation in our article is to design computationally practical methods for dealing with the size of the dataset and complexity of the models. This is achieved using the marginal likelihood obtained by integrating over the Beta distribution of random effects.","575":"Understanding human sexual behaviors is essential for the effective prevention of sexually transmitted infections. Analysis of longitudinally measured sexual behavioral data, however, is often complicated by zero-inflation of event counts, nonlinear time trend, time-varying covariates, and informative dropouts. Ignoring these complicating factors could undermine the validity of the study findings. In this paper, we put forth a unified joint modeling structure that accommodates these features of the data. Specifically, we propose a pair of simultaneous models for the zero-inflated event counts: Each of these models contains an auto-regressive structure for the accommodation of the effect of recent event history, and a nonparametric component for the modeling of nonlinear time effect. Informative dropout and time varying covariates are modeled explicitly in the process. Model fitting and parameter estimation are carried out in a Bayesian paradigm by the use of a Markov Chain Monte Carlo (MCMC) method. Analytical results showed that adolescent sexual behaviors tended to evolve nonlinearly over time and they were strongly influenced by the day-to-day variations in mood and sexual interests. These findings suggest that adolescent sex is to a large extent driven by intrinsic factors rather than being compelled by circumstances, thus highlighting the need of education on self protective measures against infection risks.","576":"Dynamic treatment regimes are time-varying treatments that individualize sequences of treatments to the patient. The construction of dynamic treatment regimes is challenging because a patient will be eligible for some treatment components only if he has not responded (or has responded) to other treatment components. In addition there are usually a number of potentially useful treatment components and combinations thereof. In this article, we propose new methodology for identifying promising components and screening out negligible ones. First, we define causal factorial effects for treatment components that may be applied sequentially to a patient. Second we propose experimental designs that can be used to study the treatment components. Surprisingly, modifications can be made to (fractional) factorial designs - more commonly found in the engineering statistics literature -for screening in this setting. Furthermore we provide an analysis model that can be used to screen the factorial effects. We demonstrate the proposed methodology using examples motivated in the literature and also via a simulation study.","577":"Estimation of longitudinal data covariance structure poses significant challenges because the data are usually collected at irregular time points. A viable semiparametric model for covariance matrices was proposed in Fan, Huang and Li (2007) that allows one to estimate the variance function nonparametrically and to estimate the correlation function parametrically via aggregating information from irregular and sparse data points within each subject. However, the asymptotic properties of their quasi-maximum likelihood estimator (QMLE) of parameters in the covariance model are largely unknown. In the current work, we address this problem in the context of more general models for the conditional mean function including parametric, nonparametric, or semi-parametric. We also consider the possibility of rough mean regression function and introduce the difference-based method to reduce biases in the context of varying-coefficient partially linear mean regression models. This provides a more robust estimator of the covariance function under a wider range of situations. Under some technical conditions, consistency and asymptotic normality are obtained for the QMLE of the parameters in the correlation function. Simulation studies and a real data example are used to illustrate the proposed approach.","578":"There has been a recent surge of interest in modeling and methods for analyzing recurrent events data with risk of termination dependent on the history of the recurrent events. To aid the future users in understanding the implications of modeling assumptions and modeling properties, we review the state of the art statistical methods and present novel theoretical properties, identifiability results and practical consequences of key modeling assumptions of several fully specified stochastic models. After introducing stochastic models with noninformative termination process, we focus on a class of models which allows both negative and positive association between the risk of termination and the rate of recurrent events via a frailty variable. We also discuss the relationship as well as the major differences between these models in terms of their motivations and physical interpretations. We discuss associated Bayesian methods based on Markov chain Monte Carlo tools, and novel model diagnostic tools to perform inference based on fully specified models. We demonstrate the usefulness of current methodology through an analysis of a data set from a clinical trial. In conclusion, we explore possible future extensions and limitations of the methodology.","579":"Motivated by the need to understand and predict early pregnancy loss using hormonal indicators of pregnancy health, this paper proposes a semiparametric Bayes approach for assessing the relationship between functional predictors and a response. A multivariate adaptive spline model is used to describe the functional predictors, and a generalized linear model with a random intercept describes the response. Through specifying the random intercept to follow a Dirichlet process jointly with the random spline coefficients, we obtain a procedure that clusters trajectories according to shape and according to the parameters of the response model for each cluster. This very flexible method allows for the incorporation of covariates in the models for both the response and the trajectory. We apply the method to post-ovulatory progesterone data from the Early Pregnancy Study and find that the model successfully predicts early pregnancy loss.","580":"In this paper we define a hierarchical Bayesian model for microarray expression data collected from several studies and use it to identify genes that show differential expression between two conditions. Key features include shrinkage across both genes and studies, and flexible modeling that allows for interactions between platforms and the estimated effect, as well as concordant and discordant differential expression across studies. We evaluated the performance of our model in a comprehensive fashion, using both artificial data, and a \"split-study\" validation approach that provides an agnostic assessment of the model's behavior not only under the null hypothesis, but also under a realistic alternative. The simulation results from the artificial data demonstrate the advantages of the Bayesian model. The 1 - AUC values for the Bayesian model are roughly half of the corresponding values for a direct combination of t- and SAM-statistics. Furthermore, the simulations provide guidelines for when the Bayesian model is most likely to be useful. Most noticeably, in small studies the Bayesian model generally outperforms other methods when evaluated by AUC, FDR, and MDR across a range of simulation parameters, and this difference diminishes for larger sample sizes in the individual studies. The split-study validation illustrates appropriate shrinkage of the Bayesian model in the absence of platform-, sample-, and annotation-differences that otherwise complicate experimental data analyses. Finally, we fit our model to four breast cancer studies employing different technologies (cDNA and Affymetrix) to estimate differential expression in estrogen receptor positive tumors versus negative ones. Software and data for reproducing our analysis are publicly available.","581":"The expanded disability status scale (EDSS) is an ordinal score that measures progression in multiple sclerosis (MS). Progression is defined as reaching EDSS of a certain level (absolute progression) or increasing of one point of EDSS (relative progression). Survival methods for time to progression are not adequate for such data since they do not exploit the EDSS level at the end of follow-up. Instead, we suggest a Markov transitional model applicable for repeated categorical or ordinal data. This approach enables derivation of covariate-specific survival curves, obtained after estimation of the regression coefficients and manipulations of the resulting transition matrix. Large sample theory and resampling methods are employed to derive pointwise confidence intervals, which perform well in simulation. Methods for generating survival curves for time to EDSS of a certain level, time to increase of EDSS of at least one point, and time to two consecutive visits with EDSS greater than three are described explicitly. The regression models described are easily implemented using standard software packages. Survival curves are obtained from the regression results using packages that support simple matrix calculation. We present and demonstrate our method on data collected at the Partners MS center in Boston, MA. We apply our approach to progression defined by time to two consecutive visits with EDSS greater than three, and calculate crude (without covariates) and covariate-specific curves.","582":"A fundamental assumption usually made in causal inference is that of no interference between individuals (or units); that is, the potential outcomes of one individual are assumed to be unaffected by the treatment assignment of other individuals. However, in many settings, this assumption obviously does not hold. For example, in the dependent happenings of infectious diseases, whether one person becomes infected depends on who else in the population is vaccinated. In this article, we consider a population of groups of individuals where interference is possible between individuals within the same group. We propose estimands for direct, indirect, total, and overall causal effects of treatment strategies in this setting. Relations among the estimands are established; for example, the total causal effect is shown to equal the sum of direct and indirect causal effects. Using an experimental design with a two-stage randomization procedure (first at the group level, then at the individual level within groups), unbiased estimators of the proposed estimands are presented. Variances of the estimators are also developed. The methodology is illustrated in two different settings where interference is likely: assessing causal effects of housing vouchers and of vaccines.","583":"A primary focus of an increasing number of scientific studies is to determine whether two exposures interact in the effect that they produce on an outcome of interest. Interaction is commonly assessed by fitting regression models in which the linear predictor includes the product between those exposures. When the main interest lies in the interaction, this approach is not entirely satisfactory because it is prone to (possibly severe) bias when the main exposure effects or the association between outcome and extraneous factors are misspecified. In this article, we therefore consider conditional mean models with identity or log link which postulate the statistical interaction in terms of a finite-dimensional parameter, but which are otherwise unspecified. We show that estimation of the interaction parameter is often not feasible in this model because it would require nonparametric estimation of auxiliary conditional expectations given high-dimensional variables. We thus consider 'multiply robust estimation' under a union model that assumes at least one of several working submodels holds. Our approach is novel in that it makes use of information on the joint distribution of the exposures conditional on the extraneous factors in making inferences about the interaction parameter of interest. In the special case of a randomized trial or a family-based genetic study in which the joint exposure distribution is known by design or by Mendelian inheritance, the resulting multiply robust procedure leads to asymptotically distribution-free tests of the null hypothesis of no interaction on an additive scale. We illustrate the methods via simulation and the analysis of a randomized follow-up study.","584":"We describe studies in molecular profiling and biological pathway analysis that use sparse latent factor and regression models for microarray gene expression data. We discuss breast cancer applications and key aspects of the modeling and computational methodology. Our case studies aim to investigate and characterize heterogeneity of structure related to specific oncogenic pathways, as well as links between aggregate patterns in gene expression profiles and clinical biomarkers. Based on the metaphor of statistically derived \"factors\" as representing biological \"subpathway\" structure, we explore the decomposition of fitted sparse factor models into pathway subcomponents and investigate how these components overlay multiple aspects of known biological activity. Our methodology is based on sparsity modeling of multivariate regression, ANOVA, and latent factor models, as well as a class of models that combines all components. Hierarchical sparsity priors address questions of dimension reduction and multiple comparisons, as well as scalability of the methodology. The models include practically relevant non-Gaussian\/nonparametric components for latent structure, underlying often quite complex non-Gaussianity in multivariate expression patterns. Model search and fitting are addressed through stochastic simulation and evolutionary stochastic search methods that are exemplified in the oncogenic pathway studies. Supplementary supporting material provides more details of the applications, as well as examples of the use of freely available software tools for implementing the methodology.","585":"","586":"In 2003 Thompson and colleagues reported that daily use of finasteride reduced the prevalence of prostate cancer by 25% compared to placebo. These results were based on the double-blind randomized Prostate Cancer Prevention Trial (PCPT) which followed 18,882 men with no prior or current indications of prostate cancer annually for seven years. Enthusiasm for the risk reduction afforded by the chemopreventative agent and adoption of its use in clinical practice, however, was severely dampened by the additional finding in the trial of an increased absolute number of high-grade (Gleason score &gt;\/= 7) cancers on the finasteride arm. The question arose as to whether this finding truly implied that finasteride increased the risk of more severe prostate cancer or was a study artifact due to a series of possible post-randomization selection biases, including differences among treatment arms in patient characteristics of cancer cases, differences in biopsy verification of cancer status due to increased sensitivity of prostate-specific antigen under finasteride, differential grading by biopsy due to prostate volume reduction by finasteride, and nonignorable drop-out. Via a causal inference approach implementing inverse probability weighted estimating equations, this analysis addresses the question of whether finasteride caused more severe prostate cancer by estimating the mean treatment difference in prostate cancer severity between finasteride and placebo for the principal stratum of participants who would have developed prostate cancer regardless of treatment assignment. We perform sensitivity analyses that sequentially adjust for the numerous potential post-randomization biases conjectured in the PCPT.","587":"The pretest-posttest study design is commonly used in medical and social science research to assess the effect of a treatment or an intervention. Recently, interest has been rising in developing inference procedures that improve efficiency while relaxing assumptions used in the pretest-posttest data analysis, especially when the posttest measurement might be missing. In this article we propose a semiparametric estimation procedure based on empirical likelihood (EL) that incorporates the common baseline covariate information to improve efficiency. The proposed method also yields an asymptotically unbiased estimate of the response distribution. Thus functions of the response distribution, such as the median, can be estimated straightforwardly, and the EL method can provide a more appealing estimate of the treatment effect for skewed data. We show that, compared with existing methods, the proposed EL estimator has appealing theoretical properties, especially when the working model for the underlying relationship between the pretest and posttest measurements is misspecified. A series of simulation studies demonstrates that the EL-based estimator outperforms its competitors when the working model is misspecified and the data are missing at random. We illustrate the methods by analyzing data from an AIDS clinical trial (ACTG 175).","588":"Repeated adhesion frequency assay is the only published method for measuring the kinetic rates of cell adhesion. Cell adhesion plays an important role in many physiological and pathological processes. Traditional analysis of adhesion frequency experiments assumes that the adhesion test cycles are independent Bernoulli trials. This assumption can often be violated in practice. Motivated by the analysis of repeated adhesion tests, a binary time series model incorporating random effects is developed in this paper. A goodness-of-fit statistic is introduced to assess the adequacy of distribution assumptions on the dependent binary data with random effects. The asymptotic distribution of the goodness-of-fit statistic is derived and its finite-sample performance is examined via a simulation study. Application of the proposed methodology to real data from a T-cell experiment reveals some interesting information, including the dependency between repeated adhesion tests.","589":"Economic theory assigns a central role to risk preferences. This article develops a measure of relative risk tolerance using responses to hypothetical income gambles in the Health and Retirement Study. In contrast to most survey measures that produce an ordinal metric, this article shows how to construct a cardinal proxy for the risk tolerance of each survey respondent. The article also shows how to account for measurement error in estimating this proxy and how to obtain consistent regression estimates despite the measurement error. The risk tolerance proxy is shown to explain differences in asset allocation across households.","590":"Preterm birth, defined as delivery before 37 completed weeks' gestation, is a leading cause of infant morbidity and mortality. Identifying factors related to preterm delivery is an important goal of public health professionals who wish to identify etiologic pathways to target for prevention. Validation studies are often conducted in nutritional epidemiology in order to study measurement error in instruments that are generally less invasive or less expensive than \"gold standard\" instruments. Data from such studies are then used in adjusting estimates based on the full study sample. However, measurement error in nutritional epidemiology has recently been shown to be complicated by correlated error structures in the study-wide and validation instruments. Investigators of a study of preterm birth and dietary intake designed a validation study to assess measurement error in a food frequency questionnaire (FFQ) administered during pregnancy and with the secondary goal of assessing whether a single administration of the FFQ could be used to describe intake over the relatively short pregnancy period, in which energy intake typically increases. Here, we describe a likelihood-based method via Markov Chain Monte Carlo to estimate the regression coefficients in a generalized linear model relating preterm birth to covariates, where one of the covariates is measured with error and the multivariate measurement error model has correlated errors among contemporaneous instruments (i.e. FFQs, 24-hour recalls, and\/or biomarkers). Because of constraints on the covariance parameters in our likelihood, identifiability for all the variance and covariance parameters is not guaranteed and, therefore, we derive the necessary and suficient conditions to identify the variance and covariance parameters under our measurement error model and assumptions. We investigate the sensitivity of our likelihood-based model to distributional assumptions placed on the true folate intake by employing semi-parametric Bayesian methods through the mixture of Dirichlet process priors framework. We exemplify our methods in a recent prospective cohort study of risk factors for preterm birth. We use long-term folate as our error-prone predictor of interest, the food-frequency questionnaire (FFQ) and 24-hour recall as two biased instruments, and serum folate biomarker as the unbiased instrument. We found that folate intake, as measured by the FFQ, led to a conservative estimate of the estimated odds ratio of preterm birth (0.76) when compared to the odds ratio estimate from our likelihood-based approach, which adjusts for the measurement error (0.63). We found that our parametric model led to similar conclusions to the semi-parametric Bayesian model.","591":"Genomic alterations have been linked to the development and progression of cancer. The technique of comparative genomic hybridization (CGH) yields data consisting of fluorescence intensity ratios of test and reference DNA samples. The intensity ratios provide information about the number of copies in DNA. Practical issues such as the contamination of tumor cells in tissue specimens and normalization errors necessitate the use of statistics for learning about the genomic alterations from array CGH data. As increasing amounts of array CGH data become available, there is a growing need for automated algorithms for characterizing genomic profiles. Specifically, there is a need for algorithms that can identify gains and losses in the number of copies based on statistical considerations, rather than merely detect trends in the data.We adopt a Bayesian approach, relying on the hidden Markov model to account for the inherent dependence in the intensity ratios. Posterior inferences are made about gains and losses in copy number. Localized amplifications (associated with oncogene mutations) and deletions (associated with mutations of tumor suppressors) are identified using posterior probabilities. Global trends such as extended regions of altered copy number are detected. Because the posterior distribution is analytically intractable, we implement a Metropolis-within-Gibbs algorithm for efficient simulation-based inference. Publicly available data on pancreatic adenocarcinoma, glioblastoma multiforme, and breast cancer are analyzed, and comparisons are made with some widely used algorithms to illustrate the reliability and success of the technique.","592":"We propose a general strategy for variable selection in semiparametric regression models by penalizing appropriate estimating functions. Important applications include semiparametric linear regression with censored responses and semiparametric regression with missing predictors. Unlike the existing penalized maximum likelihood estimators, the proposed penalized estimating functions may not pertain to the derivatives of any objective functions and may be discrete in the regression coefficients. We establish a general asymptotic theory for penalized estimating functions and present suitable numerical algorithms to implement the proposed estimators. In addition, we develop a resampling technique to estimate the variances of the estimated regression coefficients when the asymptotic variances cannot be evaluated directly. Simulation studies demonstrate that the proposed methods perform well in variable selection and variance estimation. We illustrate our methods using data from the Paul Coverdell Stroke Registry.","593":"Clinical management of individuals found to harbor a mutation at a known disease-susceptibility gene depends on accurate assessment of mutation-specific disease risk. For missense mutations (MMs)-mutations that lead to a single amino acid change in the protein coded by the gene-this poses a particularly challenging problem. Because it is not possible to predict the structural and functional changes to the protein product for a given amino acid substitution, and because functional assays are often not available, disease association must be inferred from data on individuals with the mutation. Inference is complicated by small sample sizes and by sampling mechanisms that bias toward individuals at high familial risk of disease. We propose a Bayesian hierarchical model to classify the disease association of MMs given pedigree data collected in the high-risk setting. The model's structure allows simultaneous characterization of multiple MMs. It uses a group of pedigrees identified through probands tested positive for known disease associated mutations and a group of test-negative pedigrees, both obtained from the same clinic, to calibrate classification and control for potential ascertainment bias. We apply this model to study MMs of breast-ovarian susceptibility genes BRCA1 and BRCA2, using data collected at the Duke University Medical Center in Durham, North Carolina.","594":"Phenotypic characterization of rare disease genes poses a significant statistical challenge, but the need to do so is clear. Clinical management of patients carrying a disease gene depends crucially on an accurate characterization of the genetically predisposed disease, including its likelihood of occurrence among mutation carriers, natural history, and response to treatment. We propose a formal yet practical method for controlling for bias due to ignoring ascertainment, defined as the sampling mechanism, when quantifying the association between genotype and disease using data on high-risk families. The approach is more statistically efficient than conditioning on the variables used in sampling. In it, the likelihood is adjusted by a factor that is a function of sampling weights in strata defined by those variables. It requires that these variables and the sampling probabilities in the strata they define either are known or can be estimated. The latter requires a second, population-based dataset. As an example, we derive ascertainment-corrected estimates of penetrance for the breast cancer susceptibility genes BRCA1 and BRCA2. The Bayesian analysis that we use incorporates a modified segregation model and prior data on penetrance derived from the literature. Markov chain Monte Carlo methods are used for inference.","595":"We propose Bayesian parametric and semiparametric partially linear regression methods to analyze the outcome-dependent follow-up data when the random time of a follow-up measurement of an individual depends on the history of both observed longitudinal outcomes and previous measurement times. We begin with the investigation of the simplifying assumptions of Lipsitz, Fitzmaurice, Ibrahim, Gelber, and Lipshultz, and present a new model for analyzing such data by allowing subject-specific correlations for the longitudinal response and by introducing a subject-specific latent variable to accommodate the association between the longitudinal measurements and the follow-up times. An extensive simulation study shows that our Bayesian partially linear regression method facilitates accurate estimation of the true regression line and the regression parameters. We illustrate our new methodology using data from a longitudinal observational study.","596":"Affymetrix's SNP (single-nucleotide polymorphism) genotyping chips have increased the scope and decreased the cost of gene-mapping studies. Because each SNP is queried by multiple DNA probes, the chips present interesting challenges in genotype calling. Traditional clustering methods distinguish the three genotypes of an SNP fairly well given a large enough sample of unrelated individuals or a training sample of known genotypes. This article describes our attempt to improve genotype calling by constructing Gaussian mixture models with empirically derived priors. The priors stabilize parameter estimation and borrow information collectively gathered on tens of thousands of SNPs. When data from related family members are available, our models capture the correlations in signals between relatives. With these advantages in mind, we apply the models to Affymetrix probe intensity data on 10,000 SNPs gathered on 63 genotyped individuals spread over eight pedigrees. We integrate the genotype-calling model with pedigree analysis and examine a sequence of symmetry hypotheses involving the correlated probe signals. The symmetry hypotheses raise novel mathematical issues of parameterization. Using the Bayesian information criterion, we select the best combination of symmetry assumptions. Compared to Affymetrix's software, our model leads to a reduction in no-calls with little sacrifice in overall calling accuracy.","597":"Alteration of gene methylation patterns has been reported to be involved in the early onsets of many human malignancies. Many exogenous risk factors, such as cigarette smoke, dietary additives, chemical exposures, radiation, and biologic agents including viral infection, are involved in the methylation pathways of cancers. We propose a multidimensional selective item response regression model to describe and test how a risk factor may alter molecular pathways involving aberrant methylation of multiple genes in oncogenesis. Our modeling framework is built on an item response model for multivariate dichotomous responses of high dimension, such as aberrant methylation of multiple tumor-suppressor genes, but we allow risk factors such as SV40 viral infection to alter the distribution of the latent factors that subsequently affect the outcome of cancer. We postulate empirical identification conditions under our model formulation. Moreover, we do not prespecify the links between the multiple dichotomous methylation responses and the latent factors, but rather conduct specification searches with a genetic algorithm to discover the links. Parameter estimation through maximum likelihood and specification searches in models with multidimensional latent factors for multivariate binary responses have become practical only recently, due to modern statistical computing development. We illustrate our proposal with the biological finding that simultaneous methylation of multiple tumor-suppressor genes is associated with the presence of SV40 viral sequences and with the cancer status of lymphoma\/leukemia.We are able to test whether the data are consistent with the causal hypothesis that SV40 induces aberrant methylation of multiple genes in its oncogenic pathways. At the same time, we are able to evaluate the role of SV40 in the methylation pathway and to determine whether the methylation pathway is responsible for the development of leukemia\/lymphoma.","598":"We are often interested in estimating sensitivity and specificity of a group of raters or a set of new diagnostic tests in situations in which gold standard evaluation is expensive or invasive. Numerous authors have proposed latent modeling approaches for estimating diagnostic error without a gold standard. Albert and Dodd showed that, when modeling without a gold standard, estimates of diagnostic error can be biased when the dependence structure between tests is misspecified. In addition, they showed that choosing between different models for this dependence structure is difficult in most practical situations. While these results caution against using these latent class models, the difficulties of obtaining gold standard verification remain a practical reality. We extend two classes of models to provide a compromise that collects gold standard information on a subset of subjects but incorporates information from both the verified and nonverified subjects during estimation. We examine the robustness of diagnostic error estimation with this approach and show that choosing between competing models is easier in this context. In our analytic work and simulations, we consider situations in which verification is completely at random as well as settings in which the probability of verification depends on the actual test results. We apply our methodological work to a study designed to estimate the diagnostic error of digital radiography for gastric cancer.","599":"In epidemiology, it is often of interest to assess how individuals with different trajectories over time in an environmental exposure or biomarker differ with respect to a continuous response. For ease in interpretation and presentation of results, epidemiologists typically categorize predictors prior to analysis. To extend this approach to time-varying predictors, one can cluster individuals by their predictor trajectory, with the cluster index included as a predictor in a regression model for the response. This article develops a semiparametric Bayes approach, which avoids assuming a pre-specified number of clusters and allows the response to vary nonparametrically over predictor clusters. This methodology is motivated by interest in relating trajectories in weight gain during pregnancy to the distribution of birth weight adjusted for gestational age at delivery. In this setting, the proposed approach allows the tails of the birth weight density to vary flexibly over weight gain clusters.","600":"We analyze experimental survey data, with a random split into respondents who get an open-ended question on the amount of total family consumption (with follow-up unfolding brackets of the form \"Is consumption $X or more?\" for those who answer \"don't know\" or \"refuse\") and respondents who are immediately directed to unfolding brackets. In both cases, the entry point of the unfolding bracket sequence is randomized. Allowing for any type of selection into answering the open-ended or bracket questions, a nonparametric test is developed for errors in the answers to the first bracket question that are different from the usual reporting errors that will also affect open-ended answers. Two types of errors are considered explicitly: anchoring and yea-saying. Data are collected in the 1995 wave of the Assets and Health Dynamics survey, which is representative of the population in the United States that is 70 years and older. We reject the joint hypothesis of no anchoring and no yea-saying. Once yea-saying is taken into account, we find no evidence of anchoring at the entry point.","601":"In this paper we propose a Bayesian natural history model for disease progression based on the joint modeling of longitudinal biomarker levels, age at clinical detection of disease and disease status at diagnosis. We establish a link between the longitudinal responses and the natural history of the disease by using an underlying latent disease process which describes the onset of the disease and models the transition to an advanced stage of the disease as dependent on the biomarker levels. We apply our model to the data from the Baltimore Longitudinal Study of Aging on prostate specific antigen (PSA) to investigate the natural history of prostate cancer.","602":"We present a unified approach to nonparametric comparisons of receiver operating characteristic (ROC) curves for a paired design with clustered data. Treating empirical ROC curves as stochastic processes, their asymptotic joint distribution is derived in the presence of both between-marker and within-subject correlations. A Monte Carlo method is developed to approximate their joint distribution without involving nonparametric density estimation. The developed theory is applied to derive new inferential procedures for comparing weighted areas under the ROC curves, confidence bands for the difference function of ROC curves, confidence intervals for the set of specificities at which one diagnostic test is more sensitive than the other, and multiple comparison procedures for comparing more than two diagnostic markers. Our methods demonstrate satisfactory small-sample performance in simulations. We illustrate our methods using clustered data from a glaucoma study and repeated-measurement data from a startle response study.","603":"In a prospective cohort study, information on clinical parameters, tests and molecular markers is often collected. Such information is useful to predict patient prognosis and to select patients for targeted therapy. We propose a new graphical approach, the positive predictive value (PPV) curve, to quantify the predictive accuracy of prognostic markers measured on a continuous scale with censored failure time outcome. The proposed method highlights the need to consider both predictive values and the marker distribution in the population when evaluating a marker, and it provides a common scale for comparing different markers. We consider both semiparametric and nonparametric based estimating procedures. In addition, we provide asymptotic distribution theory and resampling based procedures for making statistical inference. We illustrate our approach with numerical studies and datasets from the Seattle Heart Failure Study.","604":"This article describes a class of heteroscedastic generalized linear regression models in which a subset of the regression parameters are rescaled nonparametrically, and develops efficient semiparametric inferences for the parametric components of the models. Such models provide a means to adapt for heterogeneity in the data due to varying exposures, varying levels of aggregation, and so on. The class of models considered includes generalized partially linear models and nonparametrically scaled link function models as special cases. We present an algorithm to estimate the scale function nonparametrically, and obtain asymptotic distribution theory for regression parameter estimates. In particular, we establish that the asymptotic covariance of the semiparametric estimator for the parametric part of the model achieves the semiparametric lower bound. We also describe bootstrap-based goodness-of-scale test. We illustrate the methodology with simulations, published data, and data from collaborative research on ultrasound safety.","605":"In multicenter studies, one often needs to make inference about a population survival curve based on multiple, possibly heterogeneous survival data from individual centers. We investigate a flexible Bayesian method for estimating a population survival curve based on a semiparametric multiresolution hazard model that can incorporate covariates and account for center heterogeneity. The method yields a smooth estimate of the survival curve for \"multiple resolutions\" or time scales of interest. The Bayesian model used has the capability to accommodate general forms of censoring and a priori smoothness assumptions. We develop a model checking and diagnostic technique based on the posterior predictive distribution and use it to identify departures from the model assumptions. The hazard estimator is used to analyze data from 110 centers that participated in a multicenter randomized clinical trial to evaluate tamoxifen in the treatment of early stage breast cancer. Of particular interest are the estimates of center heterogeneity in the baseline hazard curves and in the treatment effects, after adjustment for a few key clinical covariates. Our analysis suggests that the treatment effect estimates are rather robust, even for a collection of small trial centers, despite variations in center characteristics.","606":"The largest randomized, double-blind, placebo-controlled chemoprevention trial, the National Surgical Adjuvant Breast and Bowel Project's Breast Cancer Prevention Trial (NSABP-BCPT), evaluated the efficacy of tamoxifen in the prevention of breast cancer among women at high risk of developing the disease. The trial has reported a reduction of breast cancer incidence for the tamoxifen group. However, the effect of tamoxifen on the time to diagnosis of the disease over the six-year follow-up of the trial has not been fully explored in literature. We propose a flexible semiparametric model to assess the effects of tamoxifen on the incidence of breast cancer as well as time to the diagnosis of the disease, separately, in the framework of a cure-rate model. We used an estimating equation approach to estimating the unknown parameters, and assessed the semiparametric model assumption with a test based on the area between two survival curves. On the NSABP-BCPT data, we found that the treatment of tamoxifen has a substantial effect in the reduction of invasive breast cancer events in estrogen receptor (ER)-positive tumors, but has no effect on ER-negative tumors. Among women who were diagnosed to have ER-positive breast cancer during the study follow-up, there was little difference in terms of time to diagnosis between the two arms. However, tamoxifen may advance the time to breast cancer diagnosis for ER-negative breast cancer, while the incidence of ER-negative tumors is similar in the two treatment arms.","607":"A rapidly aging population, such as the United States today, is characterized by the increased prevalence of chronic impairment. Robust estimation of disability-free life expectancy (DFLE), or healthy life expectancy, is essential for examining whether additional years of life are spent in good health and whether life expectancy is increasing faster than the decline of disability rates. Over 30 years since its publication, Sullivan's method remains the most widely used method to estimate DFLE. Therefore, it is surprising to note that Sullivan did not provide any formal justification of his method. Debates in the literature have centered around the properties of Sullivan's method and have yielded conflicting results regarding the assumptions required for Sullivan's method. In this article we establish a statistical foundation of Sullivan's method. We prove that, under stationarity assumptions, Sullivan's estimator is unbiased and consistent. This resolves the debate in the literature, which has generally concluded that additional assumptions are necessary. We also show that the standard variance estimator is consistent and approximately unbiased. Finally, we demonstrate that Sullivan's method can be extended to estimate DFLE without stationarity assumptions. Such an extension is possible whenever a cohort life table and either consecutive cross-sectional disability surveys or a longitudinal survey are available. Our empirical analysis of the 1907 and 1912 U.S. birth cohorts suggests that while mortality rates remain approximately stationary, disability rates decline during this time period.","608":"With rapid improvements in medical treatment and health care, many datasets dealing with time to relapse or death now reveal a substantial portion of patients who are cured (i.e., who never experience the event). Extended survival models called cure rate models account for the probability of a subject being cured and can be broadly classified into the classical mixture models of Berkson and Gage (BG type) or the stochastic tumor models pioneered by Yakovlev and extended to a hierarchical framework by Chen, Ibrahim, and Sinha (YCIS type). Recent developments in Bayesian hierarchical cure models have evoked significant interest regarding relationships and preferences between these two classes of models. Our present work proposes a unifying class of cure rate models that facilitates flexible hierarchical model-building while including both existing cure model classes as special cases. This unifying class enables robust modeling by accounting for uncertainty in underlying mechanisms leading to cure. Issues such as regressing on the cure fraction and propriety of the associated posterior distributions under different modeling assumptions are also discussed. Finally, we offer a simulation study and also illustrate with two datasets (on melanoma and breast cancer) that reveal our framework's ability to distinguish among underlying mechanisms that lead to relapse and cure.","609":"Improving efficiency for regression coefficients and predicting trajectories of individuals are two important aspects in analysis of longitudinal data. Both involve estimation of the covariance function. Yet, challenges arise in estimating the covariance function of longitudinal data collected at irregular time points. A class of semiparametric models for the covariance function is proposed by imposing a parametric correlation structure while allowing a nonparametric variance function. A kernel estimator is developed for the estimation of the nonparametric variance function. Two methods, a quasi-likelihood approach and a minimum generalized variance method, are proposed for estimating parameters in the correlation structure. We introduce a semiparametric varying coefficient partially linear model for longitudinal data and propose an estimation procedure for model coefficients by using a profile weighted least squares approach. Sampling properties of the proposed estimation procedures are studied and asymptotic normality of the resulting estimators is established. Finite sample performance of the proposed procedures is assessed by Monte Carlo simulation studies. The proposed methodology is illustrated by an analysis of a real data example.","610":"In some randomized studies, researchers are interested in determining the effect of treatment assignment on outcomes that may exist only in a subset chosen after randomization. For example, in preventative human immunodeficiency virus (HIV) vaccine efficacy trials, it is of interest to determine whether randomization to vaccine affects postinfection outcomes that may be right-censored. Such outcomes in these trials include time from infection diagnosis to initiation of antiretroviral therapy and time from infection diagnosis to acquired immune deficiency syndrome. Here we present sensitivity analysis methods for making causal comparisons on these postinfection outcomes. We focus on estimating the survival causal effect, defined as the difference between probabilities of not yet experiencing the event in the vaccine and placebo arms, conditional on being infected regardless of treatment assignment. This group is referred to as the always-infected principal stratum. Our key assumption is monotonicity-that subjects randomized to the vaccine arm who become infected would have been infected had they been randomized to placebo. We propose nonparametric, semiparametric, and parametric methods for estimating the survival causal effect. We apply these methods to the first Phase III preventative HIV vaccine trial, VaxGen's trial of AIDSVAX B\/B.","611":"Likelihood approaches for large irregularly spaced spatial datasets are often very difficult, if not infeasible, to implement due to computational limitations. Even when we can assume normality, exact calculations of the likelihood for a Gaussian spatial process observed at n locations requires O(n(3)) operations. We present a version of Whittle's approximation to the Gaussian log likelihood for spatial regular lattices with missing values and for irregularly spaced datasets. This method requires O(nlog(2)n) operations and does not involve calculating determinants. We present simulations and theoretical results to show the benefits and the performance of the spatial likelihood approximation method presented here for spatial irregularly spaced datasets and lattices with missing values. We apply these methods to estimate the spatial structure of sea surface temperatures (SST) using satellite data with missing values.","612":"Animal carcinogenicity studies, such as those conducted by the U.S. National Toxicology Program (NTP), focus on detecting trends in tumor rates across dose groups. Over time, the NTP has compiled vast amounts of data on tumors in control animals. Currently, this information is used informally, without the benefit of statistical tests for carcinogenicity that directly incorporate historical data on control animals. This article proposes a survival-adjusted test for detecting dose-related trends in tumor incidence rates, which incorporates data on historical control rates and formally accounts for variation in these rates among studies. An extensive simulation, based on a wide range of realistic situations, demonstrates that the proposed test performs well in comparison to the current NTP test, which does not incorporate historical control data. In particular, our test can aid in interpreting the occurrence of a few tumors in treated animals that are rarely seen in controls. One such example, which motivates our work, concerns the analysis of histiocytic sarcoma in the NTP's 2-year cancer bioassay of benzophenone. Whereas the occurrence of three histiocytic sarcomas in female rats was not significant according to the current NTP testing procedure (P = 0.074), it was highly significant (P = 0.004) when control data from six recent historical studies were included and our test was applied to the combined data.","613":"The assessment of air pollution regulatory programs designed to improve ground level ozone concentrations is a topic of considerable interest to environmental managers. To aid this assessment, it is necessary to model the space-time behavior of ozone for predicting summaries of ozone across spatial domains of interest and for the detection of long-term trends at monitoring sites. These trends, adjusted for the effects of meteorological variables, are needed for determining the effectiveness of pollution control programs in terms of their magnitude and uncertainties across space. This paper proposes a space-time model for daily 8-hour maximum ozone levels to provide input to regulatory activities: detection, evaluation, and analysis of spatial patterns of ozone summaries and temporal trends. The model is applied to analyzing data from the state of Ohio which has been chosen because it contains a mix of urban, suburban, and rural ozone monitoring sites in several large cities separated by large rural areas. The proposed space-time model is auto-regressive and incorporates the most important meteorological variables observed at a collection of ozone monitoring sites as well as at several weather stations where ozone levels have not been observed. This problem of misalignment of ozone and meteorological data is overcome by spatial modeling of the latter. In so doing we adopt an approach based on the successive daily increments in meteorological variables. With regard to modeling, the increment (or change-in-meteorology) process proves more attractive than working directly with the meteorology process, without sacrificing any desired inference. The full model is specified within a Bayesian framework and is fitted using MCMC techniques. Hence, full inference with regard to model unknowns is available as well as for predictions in time and space, evaluation of annual summaries and assessment of trends.","614":"Large-scale inference for random spatial surfaces over a region using spatial process models has been well studied. Under such models, local analysis of the surface (e.g., gradients at given points) has received recent attention. A more ambitious objective is to move from points to curves, to attempt to assign a meaningful gradient to a curve. For a point, if the gradient in a particular direction is large (positive or negative), then the surface is rapidly increasing or decreasing in that direction. For a curve, if the gradients in the direction orthogonal to the curve tend to be large, then the curve tracks a path through the region where the surface is rapidly changing. In the literature, learning about where the surface exhibits rapid change is called wombling, and a curve such as we have described is called a wombling boundary. Existing wombling methods have focused mostly on identifying points and then connecting these points using an ad hoc algorithm to create curvilinear wombling boundaries. Such methods are not easily incorporated into a statistical modeling setting. The contribution of this article is to formalize the notion of a curvilinear wombling boundary in a vector analytic framework using parametric curves and to develop a comprehensive statistical framework for curvilinear boundary analysis based on spatial process models for point-referenced data. For a given curve that may represent a natural feature (e.g., a mountain, a river, or a political boundary), we address the issue of testing or assessing whether it is a wombling boundary. Our approach is applicable to both spatial response surfaces and, often more appropriately, spatial residual surfaces. We illustrate our methodology with a simulation study, a weather dataset for the state of Colorado, and a species presence\/absence dataset from Connecticut.","615":"We present a case study illustrating the challenges of analyzing accelerometer data taken from a sample of children participating in an intervention study designed to increase physical activity. An accelerometer is a small device worn on the hip that records the minute-by-minute activity levels of the child throughout the day for each day it is worn. The resulting data are irregular functions characterized by many peaks representing short bursts of intense activity. We model these data using the wavelet-based functional mixed model. This approach incorporates multiple fixed effect and random effect functions of arbitrary form, the estimates of which are adaptively regularized using wavelet shrinkage. The method yields posterior samples for all functional quantities of the model, which can be used to perform various types of Bayesian inference and prediction. In our case study, a high proportion of the daily activity profiles are incomplete, i.e. have some portion of the profile missing, so cannot be directly modeled using the previously described method. We present a new method for stochastically imputing the missing data that allows us to incorporate these incomplete profiles in our analysis. Our approach borrows strength from both the observed measurements within the incomplete profiles and from other profiles, from the same child as well as other children with similar covariate levels, while appropriately propagating the uncertainty of the imputation throughout all subsequent inference. We apply this method to our case study, revealing some interesting insights into children's activity patterns. We point out some strengths and limitations of using this approach to analyze accelerometer data.","616":"An easy-to-implement global procedure for testing the four assumptions of the linear model is proposed. The test can be viewed as a Neyman smooth test and it only relies on the standardized residual vector. If the global procedure indicates a violation of at least one of the assumptions, the components of the global test statistic can be utilized to gain insights into which assumptions have been violated. The procedure can also be used in conjunction with associated deletion statistics to detect unusual observations. Simulation results are presented indicating the sensitivity of the procedure in detecting model violations under a variety of situations, and its performance is compared with three potential competitors, including a procedure based on the Box-Cox power transformation. The procedure is demonstrated by applying it to a new car mileage data set and a water salinity data set that has been used previously to illustrate model diagnostics.","617":"The effects of vaccine on postinfection outcomes, such as disease, death, and secondary transmission to others, are important scientific and public health aspects of prophylactic vaccination. As a result, evaluation of many vaccine effects condition on being infected. Conditioning on an event that occurs posttreatment (in our case, infection subsequent to assignment to vaccine or control) can result in selection bias. Moreover, because the set of individuals who would become infected if vaccinated is likely not identical to the set of those who would become infected if given control, comparisons that condition on infection do not have a causal interpretation. In this article we consider identifiability and estimation of causal vaccine effects on binary postinfection outcomes. Using the principal stratification framework, we define a postinfection causal vaccine efficacy estimand in individuals who would be infected regardless of treatment assignment. The estimand is shown to be not identifiable under the standard assumptions of the stable unit treatment value, monotonicity, and independence of treatment assignment. Thus selection models are proposed that identify the causal estimand. Closed-form maximum likelihood estimators (MLEs) are then derived under these models, including those assuming maximum possible levels of positive and negative selection bias. These results show the relations between the MLE of the causal estimand and two commonly used estimators for vaccine effects on postinfection outcomes. For example, the usual intent-to-treat estimator is shown to be an upper bound on the postinfection causal vaccine effect provided that the magnitude of protection against infection is not too large. The methods are used to evaluate postinfection vaccine effects in a clinical trial of a rotavirus vaccine candidate and in a field study of a pertussis vaccine. Our results show that pertussis vaccination has a significant causal effect in reducing disease severity.","618":"This paper considers the problem of estimating the dispersion parameter in a Gaussian model which is intermediate between a model where the mean parameter is fully known (fixed) and a model where the mean parameter is completely unknown. One of the goals is to understand the implications of the two-step process of first selecting a model among a finite number of sub-models, and then estimating a parameter of interest after the model selection, but using the same sample data. The estimators are classified into global, two-step, and weighted-type estimators. While the global-type estimators ignore the model space structure, the two-step estimators explore the structure adaptively and can be related to pre-test estimators, and the weighted estimators are motivated by the Bayesian approach. Their performances are compared theoretically and through simulations using their risk functions based on a scale invariant quadratic loss function. It is shown that in the variance estimation problem efficiency gains arise by exploiting the sub-model structure through the use of two-step and weighted estimators, especially when the number of competing sub-models is few; but that this advantage may deteriorate or be lost altogether for some two-step estimators as the number of sub-models increases or as the distance between them decreases. Furthermore, it is demonstrated that weighted estimators, arising from properly chosen priors, outperform two-step estimators when there are many competing sub-models or when the sub-models are close to each other, whereas two-step estimators are preferred when the sub-models are highly distinguishable. The results have implications regarding model averaging and model selection issues.","619":"An estimator for the load share parameters in an equal load-share model is derived based on observing k-component parallel systems of identical components that have a continuous distribution function F (.) and failure rate r(.). In an equal load share model, after the first of k components fails, failure rates for the remaining components change from r(t) to gamma(1)r(t), then to gamma(2)r(t) after the next failure, and so on. On the basis of observations on n independent and identical systems, a semiparametric estimator of the component baseline cumulative hazard function R = - log(1 - F) is presented, and its asymptotic limit process is established to be a Gaussian process. The effect of estimation of the load-share parameters is considered in the derivation of the limiting process. Potential applications can be found in diverse areas, including materials testing, software reliability and power plant safety assessment.","620":"Recurrent event data are commonly encountered in longitudinal follow-up studies related to biomedical science, econometrics, reliability, and demography. In many studies, recurrent events serve as important measurements for evaluating disease progression, health deterioration, or insurance risk. When analyzing recurrent event data, an independent censoring condition is typically required for the construction of statistical methods. In some situations, however, the terminating time for observing recurrent events could be correlated with the recurrent event process, thus violating the assumption of independent censoring. In this article, we consider joint modeling of a recurrent event process and a failure time in which a common subject-specific latent variable is used to model the association between the intensity of the recurrent event process and the hazard of the failure time. The proposed joint model is flexible in that no parametric assumptions on the distributions of censoring times and latent variables are made, and under the model, informative censoring is allowed for observing both the recurrent events and failure times. We propose a \"borrow-strength estimation procedure\" by first estimating the value of the latent variable from recurrent event data, then using the estimated value in the failure time model. Some interesting implications and trajectories of the proposed model are presented. Properties of the regression parameter estimates and the estimated baseline cumulative hazard functions are also studied.","621":"We consider studies for evaluating the short-term effect of a treatment of interest on a time-to-event outcome. The studies we consider are only partially controlled in the following sense: (1) Subjects' exposure to the treatment of interest can vary over time, but this exposure is not directly controlled by the study; (2) subjects' follow-up time is not directly controlled by the study; and (3) the study directly controls another factor that can affect subjects' exposure to the treatment of interest as well as subjects' follow-up time. When factors 1 and 2 are both present in the study, evaluating the treatment of interest using standard methods, including instrumental variables, does not generally estimate treatment effects. We develop the methodology for estimating the effect of treatment 1 in this setting of partially controlled studies under explicit assumptions using the framework for principal stratification for causal inference. We illustrate our methods by a study to evaluate the efficacy of the Baltimore Needle Exchange Program to reduce the risk of human immunodeficiency virus (HIV) transmission, using data on distance of the program's sites from the subjects.","622":"This article considers the utility of the bounded cumulative hazard model in cure rate estimation, which is an appealing alternative to the widely used two-component mixture model. This approach has the following distinct advantages: (1) It allows for a natural way to extend the proportional hazards regression model, leading to a wide class of extended hazard regression models. (2) In some settings the model can be interpreted in terms of biologically meaningful parameters. (3) The model structure is particularly suitable for semiparametric and Bayesian methods of statistical inference. Notwithstanding the fact that the model has been around for less than a decade, a large body of theoretical results and applications has been reported to date. This review article is intended to give a big picture of these modeling techniques and associated statistical problems. These issues are discussed in the context of survival data in cancer.","623":"\"This article uses a unique set of pooled cross-sectional and time series data to examine the annual rate of U.S. immigration during 1972-1991 from 60 source countries. One distinguishing feature of the article is that it breaks out and cross-classifies various classes of immigrants--numerically limited versus numerically exempt and new immigrant versus adjustment of status. A second distinguishing feature is that it utilizes a unique vector of variables relating to the presence and characteristics of various social programs in source countries. The models developed here emphasize the importance of both differential economic advantage and the ease with which a prospective migrant can transfer skills to the U.S. labor market. Hausman-Taylor instrumental variable estimates of the coefficients indicate that in addition to other factors, social programs in source countries are significant determinants of immigration to the USA.\" Data are from the Immigration and Naturalization Service's Public Use Files.","624":"The relationship between body weight and mortality is examined using U.S. data from the National Health and Nutrition Examination Survey I (NHANES I) Epidemiologic Follow-up Study concerning 13,242 individuals, the emphasis being on identifying the body mass index associated with the lowest levels of mortality. Factors such as smoking status, sex, race, and age are taken into consideration. The results suggest that only the interaction between race and body mass index is significant.","625":null,"626":"Multivariate matching with doses of treatment differs from the treatment-control matching in three ways. First, pairs must not only balance covariates, but also must differ markedly in dose. Second, any two subjects may be paired, so that the matching is nonbipartite, and different algorithms are required. Finally, a propensity score with doses must be used in place of the conventional propensity score. We illustrate multivariate matching with doses using pilot data from a media campaign against drug abuse. The media campaign is intended to change attitudes and intentions related to illegal drugs, and the evaluation compares stated intentions among ostensibly comparable teens who reported markedly different exposures to the media campaign.","627":"A dynamic treatment regime is a list of rules for how the level of treatment will be tailored through time to an individual's changing severity. In general, individuals who receive the highest level of treatment are the individuals with the greatest severity and need for treatment. Thus there is planned selection of the treatment dose. In addition to the planned selection mandated by the treatment rules, the use of staff judgment results in unplanned selection of the treatment level. Given observational longitudinal data or data in which there is unplanned selection, of the treatment level, the methodology proposed here allows the estimation of a mean response to a dynamic treatment regime under the assumption of sequential randomization.","628":"Recurrent event data are frequently encountered in longitudinal follow-up studies. In statistical literature, noninformative censoring is typically assumed when statistical methods and theory are developed for analyzing recurrent event data. In many applications, however, the observation of recurrent events could be terminated by informative dropouts or failure events, and it is unrealistic to assume that the censoring mechanism is independent of the recurrent event process. In this article we consider recurrent events of the same type and allow the censoring mechanism to be possibly informative. The occurrence of recurrent events is modeled by a subject-specific nonstationary Poisson process via a latent variable. A multiplicative intensity model is used as the underlying model for nonparametric estimation of the cumulative rate function. The multiplicative intensity model is also extended to a regression model by taking the covariate information into account. Statistical methods and theory are developed for estimation of the cumulative rate function and regression parameters. As a major feature of this article, we treat the distributions of both the censoring and latent variables as nuisance parameters. We avoid modeling and estimating the nuisance parameters by proper procedures. An analysis of the AIDS Link to Intravenous Experiences cohort data is presented to illustrate the proposed methods.","629":"Recurrent event data are frequently encountered in studies with longitudinal designs. Let the recurrence time be the time between two successive recurrent events. Recurrence times can be treated as a type of correlated survival data in statistical analysis. In general, because of the ordinal nature of recurrence times, statistical methods that are appropriate for standard correlated survival data in marginal models may not be applicable to recurrence time data. Specifically, for estimating the marginal survival function, the Kaplan-Meier estimator derived from the pooled recurrence times serves as a consistent estimator for standard correlated survival data but not for recurrence time data. In this article we consider the problem of how to estimate the marginal survival function in nonparametric models. A class of nonparametric estimators is introduced. The appropriateness of the estimators is confirmed by statistical theory and simulations. Simulation and analysis from schizophrenia data are presented to illustrate the estimators' performance.","630":"\"This article presents a multivariate hazard model for survival data that are clustered at two hierarchical levels.... We apply the model to an analysis of the covariates of child survival using survey data from northeast Brazil collected via a hierarchically clustered sampling scheme. We find that family and community frailty effects are fairly small in magnitude but are of importance because they alter the results in a systematic pattern.\"","631":"The authors describe a random-effects fertility model based upon the assumption that the menstrual cycle viability probability varies from couple to couple according to a beta distribution.  An EM algorithm is used to fit the model.  The proposed estimating procedure is fully expandable to allow covariate effects on the beta variate.  The method can be applied generally whenever dependency among Bernoulli trials is induced by a susceptibility state and the outcomes can be observed only in the aggregate.  Based upon data from a cohort of 221 couples with no known fertility problems who were attempting pregnancy, cycle viability was found to be heterogeneous among couples.  Stratification on the presence or absence of prenatal exposure of the woman to her mother's cigarette smoking revealed a statistically significant difference in the two-cycle viability distributions.  Differences are discussed in the interpretation of the beta model compared to the marginal approach based upon generalized estimating equations.","632":"This article presents a general review of the major trends in the conceptualization, development, and success of case-control methods for the study of disease causation and prevention. \"Recent work on nested case-control, case-cohort, and two-stage case control designs demonstrates the continuing impact of statistical thinking on epidemiology. The influence of R. A. Fisher's work on these developments is mentioned wherever possible. His objections to the drawing of causal conclusions from observational data on cigarette smoking and lung cancer are used to introduce the problems of measurement error and confounding bias.\"","633":"\"In this article we evaluate the accuracy and bias of projections of total population and population by age group for census tracts in three counties in Florida.  We use [U.S. census] data from 1970 and 1980 and several simple extrapolation techniques to produce projections for 1990; we then compare these projections with 1990 census counts and evaluate the differences.  For the total sample, we find mean absolute errors of 17%-20% for the three most accurate techniques for projecting total population and find no indication of overall bias.  For individual age groups, mean absolute errors range from 20%-29%.\"  This is a revised version of a paper presented at the 1993 Annual Meeting of the Population Association of America.","634":"\"This article presents and implements a new method for making stochastic population forecasts that provide consistent probability intervals. We blend mathematical demography and statistical time series methods to estimate stochastic models of fertility and mortality based on U.S. data back to 1900 and then use the theory of random-matrix products to forecast various demographic measures and their associated probability intervals to the year 2065.  Our expected total population sizes agree quite closely with the Census medium projections, and our 95 percent probability intervals are close to the Census high and low scenarios.  But Census intervals in 2065 for ages 65+ are nearly three times as broad as ours, and for 85+ are nearly twice as broad. In contrast, our intervals for the total dependency and youth dependency ratios are more than twice as broad as theirs, and our ratio for the elderly dependency ratio is 12 times as great as theirs.  These items have major implications for policy, and these contrasting indications of uncertainty clearly show the limitations of the conventional scenario-based methods.\"","635":"\"In this article we describe a logistic regression modeling approach for nonresponse in the [U.S.] Post-Enumeration Survey (PES) that has desirable theoretical properties and that has performed well in practice.... In the 1990 PES, interviews were not obtained from approximately 1.2% of households in the sample, and approximately 2.1% of the individuals in interviewed households were considered unresolved after follow-up....The missing binary enumeration statuses for these unresolved cases were replaced with probabilities estimated under a statistical model that incorporated covariate information observed for these cases.  This article describes an approach to modeling missing binary outcomes when there are a large number of covariates.\"","636":"\"A central assumption in the standard capture-recapture approach to the estimation of the size of a closed population is the homogeneity of the 'capture' probabilities.  In this article we develop an approach that allows for varying susceptibility to capture through individual parameters using a variant of the Rasch model from psychological measurement situations. Our approach requires an additional recapture.  In the context of census undercount estimation, this requirement amounts to the use of a second independent sample or alternative data source to be matched with census and Post-Enumeration Survey (PES) data.... We illustrate [our] models and their estimation using data from a 1988 dress-rehearsal study for the 1990 census conducted by the U.S. Bureau of the Census, which explored the use of administrative data as a supplement to the PES.  The article includes a discussion of extensions and related models.\"","637":"\"We show how conditional logistic regression can be used to estimate the probability of being enumerated in a census and apply the model to the 1990 Post-Enumeration Survey (PES) in the United States.... We discuss some special problems caused by the fact that the PES sample area is open to migration between the captures.  We also consider the effect of data errors in estimation.  We characterize hard-to-enumerate populations and give some tentative estimates of correlation bias.\"","638":"\"The 1990 [U.S.] Post-Enumeration Survey (PES) stratified the population into 1,392 subpopulations called post-strata based on location, race, tenure, sex and age, in the hope that these subpopulations were homogeneous in relation to factors affecting the Census coverage....With block-level data from the PES for sites around Detroit and Texas, we are able to examine empirically the extent to which this hope was realized.  Using various measures, we find that between-block variation in erroneous enumeration and gross omission rates is about the same magnitude as, and largely in addition to, the corresponding between-post-stratum variation.\" Comments by Joseph L.  Schafer and Donald Ylvisaker and a rejoinder by the authors are included (pp. 1,125-9).","639":"\"The 1990 [U.S.] census and Post-Enumeration Survey produced census and dual system estimates (DSE) of population by domain, together with an estimated sampling covariance matrix of the DSE.  Estimates of the bias of the DSE were derived from various PES evaluation programs.  Of the three sources, the unadjusted census is the least variable but is believed to be the most biased, the DSE is less biased but more variable, and the bias estimates may be regarded as unbiased but are the most variable.  This article addresses methods for combining the census, the DSE, and bias estimates obtained from the evaluation programs to produce accurate estimates of population shares, as measured by weighted squared- or absolute-error loss functions applied to estimated population shares of domains.\"","640":"\"Although techniques for calculating mean survival time from current-status data are well known, their use in multiple regression models is somewhat troublesome.  Using data on current breast-feeding behavior, this article considers a number of techniques that have been suggested in the literature, including parametric, nonparametric, and semiparametric models as well as the application of standard schedules.  Models are tested in both proportional-odds and proportional-hazards frameworks....I fit [the] models to current status data on breast-feeding from the Demographic and Health Survey (DHS) in six countries:  two African (Mali and Ondo State, Nigeria), two Asian (Indonesia and Sri Lanka), and two Latin American (Colombia and Peru).\"","641":"\"Population estimates from the 1990 Post-Enumeration Survey (PES), used to measure decennial census undercount, were obtained from dual system estimates (DSE's) that assumed independence within strata defined by age-race-sex-geography and other variables.  We make this independence assumption for females, but develop methods to avoid the independence assumption for males within strata by using national level sex ratios from demographic analysis (DA).... We consider several...alternative DSE's, and use DA results for 1990 to apply them to data from the 1990 U.S. census and PES.\"","642":"\"In July 1991 the [U.S.] Census Bureau recommended to its parent agency, the Department of Commerce, that the 1990 census be adjusted for undercount.  The Secretary of Commerce decided not to adjust, however.  Those decisions relied at least partly on the Census Bureau's analyses of the accuracy of the census and of the proposed undercount adjustments based on the Post-Enumeration Survey (PES).... This article describes the total error analysis and loss function analysis of the Census Bureau.  In its decision not to adjust the census, the Department of Commerce cited different criteria than aggregate loss functions.  Those criteria are identified and discussed.\"","643":"\"This article presents estimates of net coverage of the national population in the 1990 [U.S.] census, based on the method of demographic analysis.  The general techniques of demographic analysis as an analytic tool for coverage measurement are discussed, including use of the demographic accounting equation, data components, and strengths and limitations of the method.  Patterns of coverage displayed by the 1990 estimates are described, along with similarities or differences from comparable demographic estimates for previous censuses....A final section presents the results of the first statistical assessment of the uncertainty in the demographic coverage estimates for 1990.\"  Comments by Clifford C. Clogg and Christine L. Himes (pp. 1,072-4) and Jeffrey S. Passel (pp. 1,074-7) and a rejoinder by the authors (pp. 1,077-9) are included.","644":"The author assesses the 1990 Post-Enumeration Survey, which was \"designed to produce Census tabulation of [U.S.] states and local areas corrected for the undercount or overcount of population....[He] discusses the process that produced the census adjustment estimates [as well as] the work aimed at improving the estimates.... The article then presents some of the principal results....\"","645":"Recently developed methods for power analysis expand the options available for study design. We demonstrate how easily the methods can be applied by (1) reviewing their formulation and (2) describing their application in the preparation of a particular grant proposal. The focus is a complex but ubiquitous setting: repeated measures in a longitudinal study. Describing the development of the research proposal allows demonstrating the steps needed to conduct an effective power analysis. Discussion of the example also highlights issues that typically must be considered in designing a study. First, we discuss the motivation for using detailed power calculations, focusing on multivariate methods in particular. Second, we survey available methods for the general linear multivariate model (GLMM) with Gaussian errors and recommend those based on F approximations. The treatment includes coverage of the multivariate and univariate approaches to repeated measures, MANOVA, ANOVA, multivariate regression, and univariate regression. Third, we describe the design of the power analysis for the example, a longitudinal study of a child's intellectual performance as a function of mother's estimated verbal intelligence. Fourth, we present the results of the power calculations. Fifth, we evaluate the tradeoffs in using reduced designs and tests to simplify power calculations. Finally, we discuss the benefits and costs of power analysis in the practice of statistics. We make three recommendations: Align the design and hypothesis of the power analysis with the planned data analysis, as best as practical.Embed any power analysis in a defensible sensitivity analysis.Have the extent of the power analysis reflect the ethical, scientific, and monetary costs. We conclude that power analysis catalyzes the interaction of statisticians and subject matter specialists. Using the recent advances for power analysis in linear models can further invigorate the interaction.","646":"\"This article revises the Coale-Trussell method for analyzing data from the World Fertility Survey by proposing and testing alternative log-linear and log-multiplicative models.  The models, in one form or another, represent the structural constraint underlying the Coale-Trussell method on the variation in the age pattern of human fertility.  With a Poisson distribution assumption for the number of births, several parameters of the models are simultaneously estimated via maximum likelihood.  It is shown that the new approach can be adopted whenever fertility limitation is compared across multiple populations or subpopulations.\"","647":"\"The [U.S.] Current Population Survey (CPS) reinterview sample consists of two subsamples:  (a) a sample of CPS households is reinterviewed and the discrepancies between the reinterview responses and the original interview responses are reconciled for the purpose of obtaining more accurate responses..., and (b) a sample of CPS households, nonoverlapping with sample (a), is reinterviewed 'independently' of the original interview for the purpose of estimating simple response variance (SRV).  In this article a model and estimation procedure are proposed for obtaining estimates of SRV from subsample (a) as well as the customary estimates of SRV from subsample (b).... Data from the CPS reinterview program for both subsamples (a) and (b) are analyzed both (1) to illustrate the methodology and (2) to check the validity of the CPS reinterview data.  Our results indicate that data from subsample (a) are not consistent with the data from subsample (b) and provide convincing evidence that errors in subsample (a) are the source of the inconsistency.\"","648":"\"This article proposes new methods for modeling household fertility decisions....Specifically, we model the trivariate distribution of wife's stated desire for additional children, husband's stated desire for additional children, and subsequent fertility.  In the model, the stated desire of the husband (wife) is viewed as an indicator of the husband's (wife's) latent disposition toward subsequent fertility.  The husband's (wife's) disposition is allowed to depend on the wife's (husband's) disposition.  The two dispositions are then combined to generate the couple's propensity for subsequent fertility.  We show how such models can be estimated and tested and how the parameters can be used to assess the relative influence of each partner on the propensity.\"  The model is tested using U.S. data from the Princeton Fertility Study for the 1950s and 1960s.  The results indicate that \"both husband's disposition score and wife's disposition score affect the propensity score, and, under some additional assumptions, that husbands and wives have equal relative influence on the propensity.\"","649":"\"We describe a methodology for estimating the accuracy of dual systems estimates (DSE's) of population, census estimates of population, and estimates of undercount in the census.  The DSE's are based on the census and a post-enumeration survey (PES).  We apply the methodology to the 1988 dress rehearsal census of St. Louis and east-central Missouri and we discuss its applicability to the 1990 [U.S.] census and PES.  The methodology is based on decompositions of the total (or net) error into components, such as sampling error, matching error, and other nonsampling errors.  Limited information about the accuracy of certain components of error, notably failure of assumptions in the 'capture-recapture' model, but others as well, lead us to offer tentative estimates of the errors of the census, DSE, and undercount estimates for 1988.  Improved estimates are anticipated for 1990.\"  Comments are included by Eugene P. Ericksen and Joseph B. Kadane (pp. 855-7) and Kenneth W. Wachter and Terence P. Speed (pp. 858-61), as well as a rejoinder by Mulry and Spencer (pp. 861-3).","650":"The authors \"consider the problem of adjusting provisional time series using a bivariate structural model with correlated measurement errors. Maximum likelihood estimators and a minimum mean squared error adjustment procedure are derived for a provisional and final series containing common trend and seasonal components.  The model also includes measurement errors common to both series and errors that are specific to the provisional series. [The authors] illustrate the technique by using provisional data to forecast ischemic heart disease mortality.\"","651":"\"This article shows that undercount adjustment [of the 1990 U.S. census] will probably reallocate one [in] three House seats across the states. The adjustment's impact may depend on the method used and the assumptions underlying undercount estimates.  Using regression analysis to reduce sampling error in undercount estimates from dual-systems analysis, however, eliminates sensitivity to all but the most extreme changes in assumptions.  Generally, adjustment will more likely affect large states than small ones, and large states with proportionately many urban Black and Hispanic residents will likely gain seats at the expense of large states with few such residents.\"","652":"\"We provide and illustrate methods for evaluating across-the-board ratio estimation and synthetic estimation, two techniques that might be used for improving population estimates for small areas.  The methods emphasize determination of a break-even accuracy of knowledge concerning externally obtained population totals, which marks the point at which improvement occurs.\"  The techniques are illustrated using 1980 U.S. census data.","653":"\"This article investigates the application of the three-parameter, Coale-McNeil marriage model and some related hyperparameterized specifications to data on the first marriage patterns of American women.  Because the model is parametric, it can be used to estimate the parameters of the marriage process for cohorts that have yet to complete their first marriage experience. Empirical evidence from three surveys is reported on the ability of the model to replicate and project observed marriage behavior.  The results indicate that the model can be a useful tool for analyzing cohort marriage data and that recent cohorts are showing relatively strong proclivities to both delay and forego marriage.  Consistent with earlier work, the results also indicate that education is a powerful covariate of the timing of first marriage and that race is a powerful covariate of its incidence.\"  Data are from the U.S. Current Population Survey for 1976 and 1985 and Cycle III of the National Survey of Family Growth for 1982.","654":"\"The Office of the Actuary, U.S. Social Security Administration, produces alternative forecasts of mortality to reflect uncertainty about the future.... In this article we identify the components and assumptions of the official forecasts and approximate them by stochastic parametric models.  We estimate parameters of the models from past data, derive statistical intervals for the forecasts, and compare them with the official high-low intervals.  We use the models to evaluate the forecasts rather than to develop different predictions of the future.  Analysis of data from 1972 to 1985 shows that the official intervals for mortality forecasts for males or females aged 45-70 have approximately a 95% chance of including the true mortality rate in any year.  For other ages the chances are much less than 95%.\"","655":"\"The base period of a population forecast is the time period from which historical data are collected for the purpose of forecasting future population values.  The length of the base period is one of the fundamental decisions made in preparing population forecasts, yet very few studies have investigated the effects of this decision on population forecast errors.  In this article the relationship between the length of the base period and population forecast errors is analyzed, using three simple forecasting techniques and data from 1900 to 1980 for states in the United States.  It is found that increasing the length of the base period up to 10 years improves forecast accuracy, but that further increases generally have little additional effect.  The only exception to this finding is long-range forecasts of rapidly growing states, in which a longer base period substantially improves forecast accuracy for two of the forecasting techniques.\"","656":"\"This article tests assumptions invoked in the demographic literature to estimate the population distribution of fecundability from data on waiting times to first conception.  In continuous time, the key assumption is that waiting times are realizations from a mixture of exponentials distribution.  In discrete time, the key assumption is that waiting times are realizations from a mixture of geometrics distribution.  The [U.S.] Hutterite data analyzed by Sheps (1965) are consistent with this assumption.  Various models, however, have one representation in mixture of exponentials form.  A fundamental identification problem plagues the conventional estimation procedure.  Our analysis calls into question the conventional practice of checking model specification by using goodness-of-fit tests.  The practical importance of the identification problem in duration models is demonstrated.\"","657":"\"Two models, the U.S. census model and the latent-class model, are compared in their application to evaluating measurements of ethnicity. Although the census approach assumes that the response categories of a questionnaire item correspond to groups in the population, the latent-class approach seeks to assess whether any set of response categories can represent observed ethnic heterogeneity.  Data collected using the 1990 census Hispanic-origin question and other instruments for measuring ethnicity suggest that the latent-class approach is superior whenever the response categories are not known to be valid.  In particular, using the latent-class model, this article rejects the census model's assumption of a single dimension of meaning underlying responses to the Hispanic-origin question.\"","658":"A number of alternative models are used to examine the relationship of survival among breast-cancer patients to the time since diagnosis and to the stage of the disease at diagnosis.  The data concern 2,495 women aged 55-64 diagnosed with breast cancer in the San Francisco Bay area of California.  In particular, the authors examine the extent to which the bad fit of simple models for breast-cancer survival is due to measurement error in the covariates.","659":"\"In 1980, several cities and states sued the U.S. Census Bureau to correct census results.  This correction would adjust for the differential undercounting of Blacks and Hispanics, especially in cities.  In this article, the authors, each of whom testified for New York City and State in their joint lawsuit against the Census Bureau, describe the likely pattern of the undercount and present a method to adjust for it.\"  The authors describe available methods for data adjustment and introduce a regression-based composite method of adjustment, which is used to estimate the undercounts for 66 areas.  \"As expected, we find that the highest undercount rates are in large cities, and the lowest are in states and state remainders with small percentages of Blacks and Hispanics.  Next, we analyze how sensitive our estimates are to changes in data and modeling assumptions.  We find that these changes do not affect the estimates very much.  Our conclusion is that regardless of whether we use one of the simple methods or the composite method and regardless of how we vary the assumptions of the composite method, an adjustment reliably reduces population shares in states with few minorities and increases the shares of large cities.\"","660":"\"This article demonstrates the value of microdata for understanding the effect of wages on life cycle fertility dynamics. Conventional estimates of neoclassical economic fertility models obtained from linear aggregate time series regressions are widely criticized for being nonrobust when adjusted for serial correlation.  Moreover, the forecasting power of these aggregative neoclassical models has been shown to be inferior when compared with conventional time series models that assign no role to wages.  This article demonstrates that, when neoclassical models of fertility are estimated on microdata using methods that incorporate key demographic restrictions and when they are properly aggregated, they have considerable forecasting power.\"  Data are from the 1981 Swedish Fertility Survey.","661":"Empirical Bayes methods are used to estimate the extent of the undercount at the local level in the 1980 U.S. census.  \"Grouping of like subareas from areas such as states, counties, and so on into strata is a useful way of reducing the variance of undercount estimators.  By modeling the subareas within a stratum to have a common mean and variances inversely proportional to their census counts, and by taking into account sampling of the areas (e.g., by dual-system estimation), empirical Bayes estimators that compromise between the (weighted) stratum average and the sample value can be constructed.  The amount of compromise is shown to depend on the relative importance of stratum variance to sampling variance.  These estimators are evaluated at the state level (51 states, including Washington, D.C.) and stratified on race\/ethnicity (3 strata) using data from the 1980 postenumeration survey (PEP 3-8, for the noninstitutional population).\"","662":"\"In the first half of the article, a broad account of content and procedures is given.  In conduct of individual surveys, the achievements of the World Fertility Survey were based on thoroughness rather than technical superiority. The later aspects of the program, including analysis, archiving, and data dissemination, were more innovative and represent models of excellence for similar future enquiries.  In overall terms, the program is judged to be an expensive success.  In the second half of the article, two methodological issues are discussed in more detail:  the collection of retrospective birth histories and the translation of survey instruments into local languages.\"  Comments by Anthony G. Turner (pp. 768-9), Kweku T. de Graft-Johnson (pp. 769-70), Burton Singer (pp. 771-2), and Joel E. Cohen (pp. 772-4) are appended.","663":"1 solution to the dimensionality problem raised by projection of individual age-specific fertility rates is the use of parametric curves to approximate the annual age-specific rates and a multivariate time series model to forecast the curve parameters.  Such a method reduces the number of time series to be modeled for women 14-45 years of age from 32 to 40 (the number of curve parameters). In addition, the curves force even longterm fertility projections to exhibit the same smooth distribution across age as historical data.  The data base used to illustrate this approach was age-specific fertility rates for US white women in 1921-84.  An important advantage of this model is that it permits investigation of the interactions among the total fertility rate, the mean age of childbearing, and the standard deviation of age at childbearing.  In the analysis of this particular data base, the contemporaneous relationship between the mean and standard deviation of age at childbearing was the only significant relationship.  The addition of bias forecasts to the forecast gamma curve improves forecast accuracy, especially 1-2 years ahead.  The most recent US Census Bureau projections have combined a time series model with longterm projections based on demographic judgment.  These official projections yielded a slightly higher ultimate mean age and slightly lower standard deviation than those resulting from the model described in this paper.","664":"\"The geographic mapping of age-standardized, cause-specific death rates is a powerful tool for identifying possible etiologic factors, because the spatial distribution of mortality risks can be examined for correlations with the spatial distribution of disease-specific risk factors.  This article presents a two-stage empirical Bayes procedure for calculating age-standardized cancer death rates, for use in mapping, which are adjusted for the stochasticity of rates in small area populations.  Using the adjusted rates helps isolate and identify spatial patterns in the rates.  The model is applied to sex-specific data on U.S. county cancer mortality in the white population for 15 cancer sites for three decades:  1950-1959, 1960-1969, and 1970-1979.  Selected results are presented as maps of county death rates for white males.\"","665":"Methods of determining distance covered in migration are discussed.  \"Two approaches for doing so are simply to ask movers how far they moved or to infer distance from localities of origin and destination. The former has been used in Health Interview Surveys, and the latter is applied to Current Population Surveys; both are national surveys conducted by the U.S. Census Bureau.  The two approaches appear to produce consistent results and offer ways of increasing comparability of data and research findings on geographical mobility.\"","666":"\"A mixed model is proposed for the analysis of geographic variability in mortality rates.  In addition to demographic parameters and random geographic parameters, the model includes additional random-effects parameters to adjust for extra-Poisson variability.  The model uses a gamma-Poisson distribution with a random scale parameter having an inverse gamma prior.  An empirical Bayes approach is used to estimate relative risks for geographic regions and annual rates for demographic groups within each region.  Lung cancer in Missouri is used to motivate and illustrate the procedure.\"","667":"\"This article deals with the forecast accuracy and bias of population projections for 2,971 counties in the United States.  It uses three different projection techniques and data from 1950, 1960, 1970, and 1980 to make two sets of 10-year projections and one set of 20-year projections.  These projections are compared with census counts to determine forecast errors.  The size, direction, and distribution of forecast errors are analyzed by size of place, rate of growth, and length of projection horizon.  A number of consistent patterns are noted, and an extension of the empirical results to the production of confidence intervals for population projections is considered.\"  A comment by Paul M. Beaumont and Andrew M. Isserman is included (pp. 1,004-9) together with a rejoinder by the author (pp.  1,009-12).  This is a revised version of a paper presented at the 1986 Annual Meeting of the Population Association of America (see Population Index, Vol. 52, No. 3, Fall 1986, p. 456).","668":"\"Three widely used classes of methods for forecasting national populations are reviewed:  demographic accounting\/cohort-component methods for long-range projections, statistical time series methods for short-range forecasts, and structural modeling methods for the simulation and forecasting of the effects of policy changes.  In each case, the major characteristics, strengths, and weaknesses of the methods are described.  Factors that place intrinsic limits on the accuracy of population forecasts are articulated.  Promising lines of additional research by statisticians and demographers are identified for each class of methods and for population forecasting generally.\"","669":"\"A model for birth forecasting based on prediction of the so-called 'birth order probabilities' is constructed.  The relation between this model and recent models of fertility prediction is derived.  Birth forecasts with approximate probability limits for the U.S. for the period 1983-1997 are generated.  The performance of the proposed model in predicting future fertility is tested by fitting time series models to part of the available series (1917-1982) and ultimately generating birth forecasts for the remainder of the period, then comparing these forecasts with the actual data.\"  The accuracy of the fertility forecasts made are compared with those made by other methods.","670":"The authors examine how sensitive the estimates of heterogeneity in the mortality risks in a population are to the choices of two types of function, \"one describing the age-specific rate of increase of mortality risks for individuals and the other describing the distribution of mortality risks across individuals.\"  U.S. data from published Medicare mortality rates for the period 1968-1978 are used to analyze total mortality among the aged.  \"In addition, national vital statistics data for the period 1950-1977 were used to analyze adult lung cancer mortality.  For these data, the estimates of structural parameters were less sensitive to reasonable choices of the heterogeneity distribution (gamma vs. inverse Gaussian) than to reasonable choices of the hazard rate function (Gompertz vs. Weibull).\"","671":"\"Alternative models are presented for representing coverage error in surveys and censuses of human populations.  The models are related to the capture-recapture models used in wildlife applications and to the dual-system models employed in the vital events literature.  Estimation methodologies are discussed for one of the coverage error models.\"  After a discussion of the theory underlying the methodology, \"distinctions are made between two kinds of error:  (a) sampling error and (b) error associated with the model.  An example involving data from the 1980 U.S. census is presented.  The problem of adjusting census and survey data for coverage error is also discussed.\"","672":"\"The housing unit (HU) method is used by public and private agencies throughout the United States to make local population estimates.  This article describes many of the different types of data and techniques that can be used in applying the HU method, and it discusses the strengths and weaknesses of each. Empirical evidence from four different states is provided, comparing the accuracy of HU population estimates with the accuracy of other commonly used estimation techniques.  Several conclusions are drawn regarding the usefulness of the HU method for local population estimation.\"","673":"This study examines the possibility that estimation of the effect of breast-feeding on infant survival is affected by selection bias, in that children who are healthier at birth may be more likely to be breast-fed.  Data are from the 1976 Malaysian Family Life Survey. \"Ordinary logit models for breast-feeding and survival are estimated, and the results suggest that selection is indeed present.  For example, children of higher birth weight appear to be more likely to be breast-fed and likely to survive.  In addition, weight at birth and the duration of breast-feeding appear to be linked.\"  Using birth weight as an indicator for the child's health, the authors conclude that \"the direct influence of breast-feeding on survival remains of overwhelming importance even after corrections for selection bias are made.\"","674":"The authors first note that current official U.S. population estimates and projections are based on the assumption that certain characteristics of the institutionalized population remain constant between censuses.  The article \"examines the empirical validity of this assumption by using data from the decennial censuses for 1940-1980 and, in light of substantial decade to decade changes in the age patterns of the institutional proportions for sex- and race-specific populations, seeks to develop alternative methods.\"  As part of these alternative methods, \"parametric curves are fit to the age-specific institutional proportions for each population for each decade.  A study of the observed historical variation in the parameters of these curves then leads to some suggestions about how their shapes can be estimated between censuses and projected beyond the latest available census to provide more accurate estimates and projections of the civilian noninstitutional population.\"  This is a revised version of a paper originally presented at the 1984 Annual Meeting of the Population Association of America (see Population Index, Vol. 50, No. 3, Fall 1984, p.  439).","675":"\"This article reports progress on the development of a population projection process that emphasizes model selection over demographic accounting.  Transparent multiregional\/multistate population projections that rely on parameterized model schedules are illustrated [using data primarily from a number of developed countries, particularly Sweden], together with simple techniques that extrapolate the recent trends exhibited by the parameters of such schedules.\"  The author notes that \"the parameterized schedules condense the amount of demographic information, expressing it in a language and variables that are more readily understood by the users of the projections.  In addition, they permit a concise specification of the expected temporal patterns of variation among these variables, and they allow a disaggregated focus on demographic change that otherwise would not be feasible.\"","676":"\"Two proportional hazards models for cohort fertility evaluation are constructed.  Time-dependent covariates describe sources of heterogeneity between and within women regarding fertility characteristics.  In the first model, U.S. birth rates specific to maternal age, race, parity, and birth cohort are used as underlying hazard rates.  Covariate effects are estimated by maximizing the full likelihood.  In the second model, covariate effects are estimated via Cox regression with stratified underlying hazard rates regarded as unknown nuisance parameters.\"  The authors illustrate the models \"with an evaluation of the fertility histories of the wives of workers at a manufacturing plant with potential for hazardous exposure.  Adjustments to the U.S. birth rates for maternal age and parity zero experience are required with the first approach.  Then, despite differences in the model-specific estimation procedures, the point estimates of the exposure effect and the estimated standard errors from the two models are practically equivalent.\"","677":"A univariate time-series model is presented for projecting age- or duration-specific data for recent cohorts.  The model is also capable of taking period effects into account, is illustrated using Dutch data on marital fertility rates.","678":"\"Errors in population forecasts arise from errors in the jump-off population and errors in the predictions of future vital rates.  The propagation of these errors through the linear (Leslie) growth model is studied, and prediction intervals for future population are developed.  For U.S. national forecasts, the prediction intervals are compared with the U.S. Census Bureau's high-low intervals.\"  In order to assess the accuracy of the predictions of vital rates, the authors \"derive the predictions from a parametric statistical model and estimate the extent of model misspecification and errors in parameter estimates.  Subjective, expert opinion, so important in real forecasting, is incorporated with the technique of mixed estimation.  A robust regression model is used to assess the effects of model misspecification.\"","679":"\"The limitations of available migration data preclude a time-series approach of modeling interstate migration [in the United States].  The method presented here combines aspects of the demographic and economic approaches to forecasting migration in a manner compatible with existing data.  Migration rates are modeled to change in response to changes in economic conditions.  When applied to resently constructed data on migration based on income tax returns and then compared to standard demographic projections, the demographic-economic approach has a 20% lower total error in forecasting net migration by state for cohorts of labor-force age.\"","680":"The authors present the case for the housing unit (HU) method as a technique for estimating the population of states and local areas in the United States.  They \"evaluate population estimates produced by the housing unit method and by three other commonly used methods:  component II, ratio correlation, and administrative records.  Basing [the] analysis on 1980 census data from 67 counties in Florida and testing for precision, bias, and the distribution of errors, [they find that] application of the HU method performs at least as well as the more highly acclaimed methods of local population estimation.\"","681":"\"This article estimates levels of childhood mortality on the basis of new data derived from a nationally representative sample of manuscripts of the 1900 U.S. census.  The data are responses to census questions on numbers of children ever born and numbers surviving.  The results for a subsample corresponding to the small death registration area (DRA) in 1900\/02 validate the procedures used.\"  The findings suggest that the 1900-1902 DRA life tables seriously overestimate child mortality among blacks.  \"Evidence also indicates that child mortality was declining at a moderate pace in the late 19th century, but that little decline was occurring among blacks.  The results suggest the need for revising accounts of American black demographic history, including birth rates.  They also imply that 20th-century progress in narrowing black-white mortality differentials has been smaller than is commonly believed.\"","682":"\"The individual household's fertility response to an experienced child death, the replacement probability, is estimated for Brazil using a maximum likelihood estimation model.... The model allows the distribution of births, deaths, and replacement births to vary with included exogenous variables.  The replacement probabilities estimated for Brazil were between .4 and .9 and varied systematically with women's education, rural-urban residence, and household electrification.  Increased development was shown to be correlated with a lower number of surviving children.\"","683":"A review of past population projection errors is presented as a means for constructing confidence intervals for future projections.  The author first defines a statistic to measure projection errors independent of the size of population and the length of the projection period.  A sample of U.S. and U.N. projections is used to show that the distributions of components of the error statistic are relatively stable.  This information is then used to construct confidence intervals for the U.S. population up to the year 2000.","684":"\"This paper discusses the problem of modeling demographic variables for the purpose of forecasting.\"  Two empirical model selection procedures, a time series approach and a sequential testing procedure, are applied to suggest final-form forecasting equations for an Australian births series, namely, first nuptial confinements.  The models are compared with the method used to construct the Australian government's IMPACT demographic module.  Comments by Joseph B. Kadane, Ronald Lee, Roderick J. A. Little, John F. Long, and Kenneth F. Wallis are included, together with a rejoinder by the author.","685":"The distribution of births by month exhibits a seasonal pattern in most populations.  The monthly marital fertility rate for an area of Bangladesh provides a good example of the seasonal periodicity.  Seasonal patterns of measures of reproduction in a population of married women are considered.  Equations are developed that predict the seasonal patterns of these alternative measures under the assumption that the fertility rate (R) follows a trigonometric curve.  This is followed by an empirical analysis of the measures in a Bangladesh population that has a pronounced seasonal fertility.  The investigation is intended both to validate the theoretical framework developed in the 1st part of the paper as well as to determine whether seasonal variation in actual populations is sufficiently large to affect the alternative measures significantly.  4 measures are considered:  pregnancy prevalence (PP)--the proportion of married women who are pregnant at the survey date; mean open birth interval (MOI)--the time from the last live birth to the date of the survey for parous women and from the time of marriage to the date of the survey for nulliparous women; mean closed interval-birth (MCIB)--the mean interval between the last 2 live births for married women who have a birth in the period immediately preceding the survey date; and mean closed interval-woman (MCIW)--the mean interval between the last 2 live births for women who have had at least 2 children by the time of the survey.  It is assumed that the seasonal pattern of the fertility rate of a population follows a cosine curve and that there is no trend in annual fertility from year to year.  The lag and relative variability of the other measures are considered in comparison with the fertility rate curve.   The predictions from this theoretical effort, when compared with observed patterns and trigonometric regression results for each measure in data from Bangladesh, are shown to be quite accurate.  The figure and regression results show that R, PP, and MOI have definite seasonal periodicity, but MCIB and MCIW do not display any seasonal patterns.  If there is a secular trend in fertility in addition to seasonality, these relationships between the seasonal patterns of the measures may no longer hold.  There is a disadvantage to using closed interval measures, for they are unable to detect effects of limiting of childbearing in a population since they are based only on information from women who have births.","686":"In nomination sampling, the largest values from several independent random samples (nominees) are rank ordered, and an estimate of the population median is formed by interpolating between 2 of these order statistics.  The resulting estimate compares favorably to the sample median of a simple random sample from the same population.  When historical data sets retain only extreme values, nomination sampling may offer the only practical way to estimate the population median.  The approach may also be useful when potential survey respondents will only participate if they can actively influence the selection of cases for analysis.","687":"","688":"","689":"","690":"","691":"","692":"","693":"","694":"","695":"In order to produce estimates of the number of women having abortions during a 12-month period in the conterminous United States, the randomized response technique was used in the 1973 National Survey of Family Growth.  The model applied used 2 unrelated questions in separate half-samples, with a coin as the randomizing device.  The randomized response technique resulted in a higher estimate for the number of women with abortions that has previously been obtained through direct questions or reporting systems.  The overall estimated proportion who had abortions among women who had been married or who had their own children in the household is 3.0% with a standardized error of 0.8 percentage points;  however, there is a wide variation in the half-sample estimates of abortion.  Differences between the 2 half-samples led to an examination of possible measurement error.  3 types of errors in measurement which may affect the estimate based on the randomized response technique are: 1) error in the answer to the sensitive question on abortion under the randomized response conditions;  2) error in the answer to the innocuous question under randomized response conditions;  and 3) error in the answer to the innocuous question when asked directly.  Comparisons between data from the different sources for currently married women suggest that all differences are not due to measurement error and that a large number of women had an unreported and\/or illegal abortion in 1973.  Although the randomized response models have been in use for at least 10 years, there continues to be a need for work on the field administration and subsequent analysis of these models.","696":"","697":"Time series analysis of fertility can improve demographic forecasts.  The optimal forecast and its variance for births to an age-structured population subject to serially correlated random fertility are developed.  The general case in which the fertility process had arbitrary autoregressive structure is dealt with and then the 4 special cases of white noise, 1st-order autoregressive, 2nd-order autoregressive, and random walk are considered.  Consequently, it is determined that the predictions and their variances are highly sensitive to the autoregressive structure of fertility and, therefore, if stochastic models are to be used for prediction, they must emphasize this aspect of  the problem.  Preliminary empirical efforts to model the time series of U.S. fertility from 1917 to 1972 proved unsuccessful, but it is obvious that at least a 2nd-order autoregressive scheme is require d.  The analysis proveded should be helpful in: 1) any application of the procedures requires a successful parameterization of the fertility process;  2) fertility variations could be decomposed into the effects of nuptiality and marital fertility and then births and marriages could be jointly predicted;  and 3) the simplifying approximations should be dropped and each age-specific rate could be analyzed and predicted.","698":"","699":"","700":"","701":"The authors present 2 methods for the approximation of a representative schedule recording first marriage frequencies by age.  Both treatments are mathematically complex.  One method achieves a very close approximation with a simple closed form frequency function, which is the limiting distribution of the convolution of an infinite number of exponentially distributed components.  The other method achieves an equal approximation by the convolution of a normal distribution of age of entry into a marriageable state and as few as 3 exponentially distributed delays.  This latter convolution provides a feasible model of nuptiality, a model receiving surprising empirical support.","702":"","703":"A numerical investigation using a flexible simulation model to establish interval analysis as an index for changing natality patterns.  Such an index should reflect parity distribution, the age at which women start reproduction, and the spacing of their births.  The simulated statistical results illustrate the truncation effect that reflects a negative correlation between parity and the length of closed and open intervals in a birth or marriage cohort.  Truncation is related to the duration of marriage at survey, but this duration interacts with other assumptions.  Holding duration constant does not ensure that the data on intervals will reflect postulated changes in the distributions.  For complete birth orders, this analysis does reflect patterns of child spacing.  However, it ignores changes in the parity distribution, whether produced by deliberate limitation of family size or by the onset of secondary sterility.  This difficulty is not overcome by life table analysis except under highly restrictive assumptions.  It is doubtful whether the current emphasis on securing such data is justified.  Further investigation is needed to provide a better basis for the definition and analysis of interval data if they are to be used.","704":"A statistical study was made of sex ratios and mortality in Ceylon, Pakistan, and India.  Contrary to general expectations, female mortality was higher than male mortality.  A greater divergence in the sex ratio was found with increased age.  Female emigration and abnormal sex ratios at birth are discounted as explanations of the phenomenon.  It is considered that underenumeration of females in the census and higher female mortality rates, especially during the reproductive years and childhood, are responsible for the inverted sex ratio.  The projected sex ratios for these countries are not reflected in the model life tables derived from international experience.","705":"","706":"A survey technique for improving the reliability of responses to sensitive interview questions is described.  The technique permits the respondentto answer \"yes\" or \"no\" to a question without the interviewer knowing what informationis being conveyed by the respondent.  The privacy of the interviewee is protected by randomizing his response.  For example if all members of a population belong either to group A or to group B and the investigator wants to determine the proportion of group A individuals in the population, this information can be elicited by using the following procedures.  Before each interview, the respondent is provided with a spinner marked with a point A and a point B.  The spinner is marked off in such a way that the spinner's marker will stop at point A with a probability of p and at point B with a probability of 1-p.  When the interviewer asks the sensitive question concerning group membership, the respondent spins the spinner out of the sight of the interviewer.  The marker will either stop at point A or point B.  The respondent then indicates whether or not he belongs to the group to which the marker is pointing.  The respondent does not tell the interviewer where the marker is pointing.  Assuming that these responses are truthful, it is then possible to determine maximum likelihood estimates of the true proportion of As in the population.  The formula for calculating these estimates is provided.  Both the degree of truthfulness which can be expected from the respondents and the sample size required for given levels of precision depends on the parameter p.","707":"","708":"","709":"","710":"","711":"","712":"","713":"","714":"","715":"","716":"","717":"","718":"","719":"","720":"","721":"","722":"","723":"","724":"","725":"","726":"","727":"","728":"","729":"","730":"","731":"","732":"","733":"","734":"","735":"","736":"","737":"","738":"","739":"","740":"","741":"","742":"","743":"","744":"","745":"","746":"","747":"","748":"","749":"","750":"","751":"","752":""},"articletitle":{"0":"Prioritizing Autism Risk Genes using Personalized Graphical Models Estimated from Single Cell RNA-seq Data.","1":"A multiple-testing procedure for high-dimensional mediation hypotheses.","2":"Elucidating age and sex-dependent association between frontal EEG asymmetry and depression: An application of multiple imputation in functional regression.","3":"Brain Regions Identified as Being Associated with Verbal Reasoning through the Use of Imaging Regression via Internal Variation.","4":"Bayesian Factor Analysis for Inference on Interactions.","5":"Bayesian Copula Density Deconvolution for Zero-Inflated Data in Nutritional Epidemiology.","6":"Optimal Permutation Recovery in Permuted Monotone Matrix Model.","7":"Learning Individualized Treatment Rules for Multiple-Domain Latent Outcomes.","8":"Discussion of \"Exponential-family Embedding with Application to Cell Developmental Trajectories for Single-cell RNA-seq Data\".","9":"Bayesian Semiparametric Longitudinal Drift-Diffusion Mixed Models for Tone Learning in Adults.","10":"Regression Models and Multivariate Life Tables.","11":"High-Dimensional Precision Medicine From Patient-Derived Xenografts.","12":"Targeted Inference Involving High-Dimensional Data Using Nuisance Penalized Regression.","13":"Statistical Inference for High-Dimensional Models via Recursive Online-Score Estimation.","14":"Complier stochastic direct effects: identification and robust estimation.","15":"Discussion of Kallus (2020) and Mo et al (2020).","16":"A penalized regression framework for building polygenic risk models based on summary statistics from genome-wide association studies and incorporating external information.","17":"Global and Simultaneous Hypothesis Testing for High-Dimensional Logistic Regression Models.","18":"Auto-G-Computation of Causal Effects on a Network.","19":"Exponential-Family Embedding With Application to Cell Developmental Trajectories for Single-Cell RNA-Seq Data.","20":"Gene-Based Association Testing of Dichotomous Traits With Generalized Functional Linear Mixed Models Using Extended Pedigrees: Applications to Age-Related Macular Degeneration.","21":"Nonbifurcating Phylogenetic Tree Inference via the Adaptive LASSO.","22":"Bayesian Joint Modeling of Multiple Brain Functional Networks.","23":"Recurrent Events Analysis With Data Collected at Informative Clinical Visits in Electronic Health Records.","24":"Bayesian Structure Learning in Multi-layered Genomic Networks.","25":"Personalized Policy Learning using Longitudinal Mobile Health Data.","26":"Rejoinder: Learning Optimal Distributionally Robust Individualized Treatment Rules.","27":"Learning Optimal Distributionally Robust Individualized Treatment Rules.","28":"A Distributed and Integrated Method of Moments for High-Dimensional Correlated Data Analysis.","29":"Robust Q-learning.","30":"Machine intelligence for individualized decision making under a counterfactual world: A rejoinder.","31":"Improved doubly robust estimation in learning optimal individualized treatment rules.","32":"A semiparametric instrumental variable approach to optimal treatment regimes under endogeneity.","33":"Off-Policy Estimation of Long-Term Average Outcomes with Applications to Mobile Health.","34":"Estimation and Validation of Ratio-based Conditional Average Treatment Effects Using Observational Data.","35":"Statistical Inference for Online Decision-Making: In a Contextual Bandit Setting.","36":"Optimal individualized decision rules using instrumental variable methods.","37":"Covariance-based sample selection for heterogeneous data: Applications to gene expression and autism risk gene detection.","38":"Bayesian Scalar on Image Regression With Nonignorable Nonresponse.","39":"Nonparametric Bayesian Instrumental Variable Analysis: Evaluating Heterogeneous Effects of Coronary Arterial Access Site Strategies.","40":"MODEL-FREE FORWARD SCREENING VIA CUMULATIVE DIVERGENCE.","41":"Evaluation of the health impacts of the 1990 Clean Air Act Amendments using causal inference and machine learning.","42":"L2RM: Low-rank Linear Regression Models for High-dimensional Matrix Responses.","43":"A semiparametric kernel independence test with application to mutational signatures.","44":"A Gibbs sampler for a class of random convex polytopes.","45":"A Bayesian Hierarchical CACE Model Accounting for Incomplete Noncompliance With Application to a Meta-analysis of Epidural Analgesia on Cesarean Section.","46":"IFAA: Robust Association Identification and Inference for Absolute Abundance in Microbiome Analyses.","47":"A Sparse Random Projection-based Test for Overall Qualitative Treatment Effects.","48":"D-CCA: A Decomposition-based Canonical Correlation Analysis for High-Dimensional Datasets.","49":"Ball Covariance: A Generic Measure of Dependence in Banach Space.","50":"Confidence Intervals for Sparse Penalized Regression with Random Designs.","51":"Adaptive Huber Regression.","52":"Combining Multiple Observational Data Sources to Estimate Causal Effects.","53":"Template Independent Component Analysis: Targeted and Reliable Estimation of Subject-level Brain Networks using Big Data Population Priors.","54":"A Bayesian General Linear Modeling Approach to Cortical Surface fMRI Data Analysis.","55":"Genetic Variant Set-Based Tests Using the Generalized Berk-Jones Statistic with Application to a Genome-Wide Association Study of Breast Cancer.","56":"A Geometric Variational Approach to Bayesian Inference.","57":"Matched Learning for Optimizing Individualized Treatment Strategies Using Electronic Health Records.","58":"Efficiently inferring the demographic history of many populations with allele count data.","59":"Modeling Between-Study Heterogeneity for Improved Replicability in Gene Signature Selection and Clinical Prediction.","60":"Bayesian Nonparametric Policy Search with Application to Periodontal Recall Intervals.","61":"ICeD-T Provides Accurate Estimates of Immune Cell Abundance in Tumor Samples by Allowing for Aberrant Gene Expression Patterns.","62":"Cauchy combination test: a powerful test with analytic p-value calculation under arbitrary dependency structures.","63":"Modeling Bronchiolitis Incidence Proportions in the Presence of Spatio-Temporal Uncertainty.","64":"Sensitivity Analysis for Unmeasured Confounding in Meta-Analyses.","65":"Quantile Function on Scalar Regression Analysis for Distributional Data.","66":"Likelihood ratio tests for a large directed acyclic graph.","67":"Regression Analysis of Doubly Truncated Data.","68":"Estimating Dynamic Treatment Regimes in Mobile Health Using V-learning.","69":"Robust Clustering with Subpopulation-specific Deviations.","70":"On High-Dimensional Constrained Maximum Likelihood Inference.","71":"Mapping Tumor-Specific Expression QTLs in Impure Tumor Samples.","72":"RANK: Large-Scale Inference with Graphical Nonlinear Knockoffs.","73":"Simultaneous Estimation and Variable Selection for Interval-Censored Data with Broken Adaptive Ridge Regression.","74":"Value of Information: Sensitivity Analysis and Research Design in Bayesian Evidence Synthesis.","75":"Robust Bayesian inference via coarsening.","76":"A Spatio-Temporal Modeling Framework for Surveillance Data of Multiple Infectious Pathogens with Small Laboratory Validation Sets.","77":"Efficient Signal Inclusion With Genomic Applications.","78":"Proper Inference for Value Function in High-Dimensional Q-Learning for Dynamic Treatment Regimes.","79":"Robust Score Tests With Missing Data in Genomics Studies.","80":"Optimal Sparse Linear Prediction for Block-missing Multi-modality Data without Imputation.","81":"Distribution on Warp Maps for Alignment of Open and Closed Curves.","82":"Mixed-Effect Time-Varying Network Model and Application in Brain Connectivity Analysis.","83":"Multi-Armed Angle-Based Direct Learning for Estimating Optimal Individualized Treatment Rules With Various Outcomes.","84":"Parsimonious Model Averaging With a Diverging Number of Parameters.","85":"A Bayesian Latent Class Model to Predict Kidney Obstruction in the Absence of Gold Standard.","86":"Empirical Frequency Band Analysis of Nonstationary Time Series.","87":"Testing and Estimation of Social Network Dependence With Time to Event Data.","88":"Simultaneous Covariance Inference for Multimodal Integrative Analysis.","89":"Optimal Designs of Two-Phase Studies.","90":"Improved small-sample estimation of nonlinear cross-validated prediction metrics.","91":"IPAD: Stable Interpretable Forecasting with Knockoffs Inference.","92":"Functional Horseshoe Priors for Subspace Shrinkage.","93":"Comparing and weighting imperfect models using D-probabilities.","94":"Fine-scale spatiotemporal air pollution analysis using mobile monitors on Google Street View vehicles.","95":"A Bayesian Hierarchical Summary Receiver Operating Characteristic Model for Network Meta-analysis of Diagnostic Tests.","96":"On the Use of the Lasso for Instrumental Variables Estimation with Some Invalid Instruments.","97":"A Generic Sure Independence Screening Procedure.","98":"Diagnosing Glaucoma Progression with Visual Field Data Using a Spatiotemporal Boundary Detection Method.","99":"Estimating and Testing Vaccine Sieve Effects Using Machine Learning.","100":"Automatic Detection and Uncertainty Quantification of Landmarks on Elastic Curves.","101":"Semiparametric Regression Analysis of Multiple Right- and Interval-Censored Events.","102":"A Geometric Perspective on the Power of Principal Component Association Tests in Multiple Phenotype Studies.","103":"Inverse probability weighted estimation of risk under representative interventions in observational studies.","104":"Bayesian Semiparametric Estimation of Cancer-specific Age-at-onset Penetrance with Application to Li-Fraumeni Syndrome.","105":"High-Dimensional Posterior Consistency in Bayesian Vector Autoregressive Models.","106":"Joint Indirect Standardization when Only Marginal Distributions are Observed in the Index Population.","107":"Functional Data Analysis of Dynamic PET Data.","108":"Graphical Model Selection for Gaussian Conditional Random Fields in the Presence of Latent Variables.","109":"Fully Bayesian analysis of RNA-seq counts for the detection of gene expression heterosis.","110":"Bayesian Model Search for Nonstationary Periodic Time Series.","111":"Bayesian Semiparametric Functional Mixed Models for Serially Correlated Functional Data, with Application to Glaucoma Data.","112":"Group SLOPE - adaptive selection of groups of predictors.","113":"Censoring Unbiased Regression Trees and Ensembles.","114":"Bayesian Hierarchical Varying-sparsity Regression Models with Application to Cancer Proteogenomics.","115":"Panel Data Analysis via Mechanistic Models.","116":"Adaptive Bayesian Time-Frequency Analysis of Multivariate Time Series.","117":"Accurate and Efficient P-value Calculation via Gaussian Approximation: a Novel Monte-Carlo Method.","118":"Weighted NPMLE for the Subdistribution of a Competing Risk.","119":"FSEM: Functional Structural Equation Models for Twin Functional Data.","120":"Bayesian Approximate Kernel Regression with Variable Selection.","121":"Optimal Penalized Function-on-Function Regression under a Reproducing Kernel Hilbert Space Framework.","122":"Interpretable Dynamic Treatment Regimes.","123":"A Powerful Bayesian Test for Equality of Means in High Dimensions.","124":"A Massive Data Framework for M-Estimators with Cubic-Rate.","125":"A Bayesian Machine Learning Approach for Optimizing Dynamic Treatment Regimes.","126":"Combined Hypothesis Testing on Graphs with Applications to Gene Set Enrichment Analysis.","127":"Parameter Estimation and Variable Selection for Big Systems of Linear Ordinary Differential Equations: A Matrix-Based Approach.","128":"Discussion of PENCOMP.","129":"Optimal Sparse Singular Value Decomposition for High-Dimensional High-Order Data.","130":"Downstream Effects of Upstream Causes.","131":"Detecting strong signals in gene perturbation experiments: An adaptive approach with power guarantee and FDR control.","132":"Inferring Brain Signals Synchronicity from a Sample of EEG Readings.","133":"FarmTest: Factor-adjusted robust multiple testing with approximate false discovery control.","134":"Sparse Sliced Inverse Regression Via Lasso.","135":"Prediction Accuracy Measures for a Nonlinear Model and for Right-Censored Time-to-Event Data.","136":"Robust Variable and Interaction Selection for Logistic Regression and General Index Models.","137":"Toward computerized efficient estimation in infinite-dimensional models.","138":"Nonparametric Bayes Models of Fiber Curves Connecting Brain Regions.","139":"PUlasso: High-Dimensional Variable Selection With Presence-Only Data.","140":"Nonparametric Maximum Likelihood Estimators of Time-Dependent Accuracy Measures for Survival Outcome Under Two-Stage Sampling Designs.","141":"Controlling the FDR in imperfect matches to an incomplete database.","142":"EDGE EXCHANGEABLE MODELS FOR INTERACTION NETWORKS.","143":"Assessing Time-Varying Causal Effect Moderation in Mobile Health.","144":"Quantile-Optimal Treatment Regimes.","145":"Modeling motor learning using heteroskedastic functional principal components analysis.","146":"On the Null Distribution of Bayes Factors in Linear Regression.","147":"Modeling Heterogeneity in Healthcare Utilization Using Massive Medical Claims Data.","148":"Residuals and Diagnostics for Ordinal Regression Models: A Surrogate Approach.","149":"A Tailored Multivariate Mixture Model for Detecting Proteins of Concordant Change Among Virulent Strains of Clostridium Perfringens.","150":"Comment.","151":"Nonparametric Adjustment for Measurement Error in Time-to-Event Data: Application to Risk Prediction Models.","152":"Efficient Estimation for Semiparametric Structural Equation Models With Censored Data.","153":"Optimal Subsampling for Large Sample Logistic Regression.","154":"Parametric-rate inference for one-sided differentiable parameters.","155":"Bayesian Semiparametric Multivariate Density Deconvolution.","156":"Meta-Analysis of Mid-p-Values: Some New Results based on the Convex Order.","157":"On Inverse Probability Weighting for Nonmonotone Missing at Random Data.","158":"Error Variance Estimation in Ultrahigh-Dimensional Additive Models.","159":"Learning Optimal Personalized Treatment Rules in Consideration of Benefit and Risk: with an Application to Treating Type 2 Diabetes Patients with Insulin Therapies.","160":"Mixture models with a prior on the number of components.","161":"Embracing the Blessing of Dimensionality in Factor Models.","162":"Multiple Testing of Submatrices of a Precision Matrix with Applications to Identification of Between Pathway Interactions.","163":"Efficient Semiparametric Inference Under Two-Phase Sampling, With Applications to Genetic Association Studies.","164":"Comment.","165":"The Hamming Ball Sampler.","166":"Bayesian Nonparametric Ordination for the Analysis of Microbial Communities.","167":"Conditional Spectral Analysis of Replicated Multiple Time Series with Application to Nocturnal Physiology.","168":"Restoration of Monotonicity Respecting in Dynamic Regression.","169":"Linear Model Selection when Covariates Contain Errors.","170":"PLMET: A Novel Pseudolikelihood-Based EM Test for Homogeneity in Generalilzed Exponential Tilt Mixture Models.","171":"Sparse simultaneous signal detection for identifying genetically controlled disease genes.","172":"Statistical Significance and the Dichotomization of Evidence: The Relevance of the ASA Statement on Statistical Significance and p-values for Statisticians.","173":"Testing for inequality constraints in singular models by trimming or winsorizing the variance matrix.","174":"A Bayesian Phase I\/II Trial Design for Immunotherapy.","175":"Interpretable High-Dimensional Inference Via Score Projection with an Application in Neuroimaging.","176":"Partial Identification of the Average Treatment Effect Using Instrumental Variables: Review of Methods for Binary Instruments, Treatments, and Outcomes.","177":"Marginal Bayesian Semiparametric Modeling of Mismeasured Multivariate Interval-Censored Data.","178":"Confidence regions for spatial excursion sets from repeated random field observations, with an application to climate.","179":"Bayesian Neural Networks for Selection of Drug Sensitive Genes.","180":"Maximum Rank Reproducibility: A Nonparametric Approach to Assessing Reproducibility in Replicate Experiments.","181":"Variable Selection for Skewed Model-Based Clustering: Application to the Identification of Novel Sleep Phenotypes.","182":"Weighted False Discovery Rate Control in Large-Scale Multiple Testing.","183":"Statistical Methods for Standard Membrane-Feeding Assays to Measure Transmission Blocking or Reducing Activity in Malaria.","184":"Latent Variable Poisson Models for Assessing the Regularity of Circadian Patterns over Time.","185":"Inference under Covariate-Adaptive Randomization.","186":"Tractable Bayesian variable selection: beyond normality.","187":"Polynomial accelerated solutions to a LARGE Gaussian model for imaging biofilms: in theory and finite precision.","188":"Modeling Persistent Trends in Distributions.","189":"Robust High-dimensional Volatility Matrix Estimation for High-Frequency Factor Model.","190":"On Estimation of the Hazard Function from Population-based Case-Control Studies.","191":"Semiparametric Estimation of Longitudinal Medical Cost Trajectory.","192":"Conditional modeling of longitudinal data with terminal event.","193":"A Bayesian Approach for Estimating Dynamic Functional Network Connectivity in fMRI Data.","194":"Hidden population size estimation from respondent-driven sampling: a network approach.","195":"Random Partition Distribution Indexed by Pairwise Information.","196":"Extrinsic local regression on manifold-valued data.","197":"A Functional Varying-Coefficient Single-Index Model for Functional Response Data.","198":"Generalized Scalar-on-Image Regression Models via Total Variation.","199":"MWPCR: Multiscale Weighted Principal Component Regression for High-dimensional Prediction.","200":"Evaluating Utility Measurement from Recurrent Marker Processes in the Presence of Competing Terminal Events.","201":"Latent Class Survival Models Linked by Principal Stratification to Investigate Heterogenous Survival Subgroups Among Individuals With Early-Stage Kidney Cancer.","202":"Joint scale-change models for recurrent events and failure time.","203":"Variable screening via quantile partial correlation.","204":"Residual Weighted Learning for Estimating Individualized Treatment Rules.","205":"Robust treatment comparison based on utilities of semi-competing risks in non-small-cell lung cancer.","206":"Comments on \"Personalized dose finding using outcome weighted learning\".","207":"Interactive Q-learning for Quantiles.","208":"Change-Plane Analysis for Subgroup Detection and Sample Size Calculation.","209":"The Generalized Higher Criticism for Testing SNP-Set Effects in Genetic Association Studies.","210":"Semiparametric modeling and estimation of the terminal behavior of recurrent marker processes before failure events.","211":"Defining Cancer Subtypes With Distinctive Etiologic Profiles: An Application to the Epidemiology of Melanoma.","212":"Estimation of Directed Acyclic Graphs Through Two-stage Adaptive Lasso for Gene Network Inference.","213":"Understanding the Impact of Stroke on Brain Motor Function: A Hierarchical Bayesian Approach.","214":"Ultrahigh-Dimensional Multiclass Linear Discriminant Analysis by Pairwise Sure Independence Screening.","215":"Parametrization of white matter manifold-like structures using principal surfaces.","216":"Convex Banding of the Covariance Matrix.","217":"Structured Matrix Completion with Applications to Genomic Data Integration.","218":"Estimation in the semiparametric accelerated failure time model with missing covariates: improving efficiency through augmentation.","219":"Bayesian Phase I\/II Biomarker-based Dose Finding for Precision Medicine with Molecularly Targeted Agents.","220":"Quantifying an Adherence Path-Specific Effect of Antiretroviral Therapy in the Nigeria PEPFAR Program.","221":"Comment: Extending the Latent Position Model for Networks.","222":"Functional feature construction for individualized treatment regimes.","223":"Promoting Similarity of Sparsity Structures in Integrative Analysis with Penalization.","224":"Estimation and Inference of Quantile Regression for Survival Data Under Biased Sampling.","225":"NON-LOCAL PRIORS FOR HIGH-DIMENSIONAL ESTIMATION.","226":"On the Reproducibility of Psychological Science.","227":"Set-Based Tests for the Gene-Environment Interaction in Longitudinal Studies.","228":"Nonparametric Benefit-Risk Assessment Using Marker Process in the Presence of a Terminal Event.","229":"Variable Selection in Kernel Regression Using Measurement Error Selection Likelihoods.","230":"Network Reconstruction From High-Dimensional Ordinary Differential Equations.","231":"Estimation of the Continuous and Discontinuous Leverage Effects.","232":"A Semiparametric Single-Index Risk Score Across Populations.","233":"Bayesian Nonparametric Estimation for Dynamic Treatment Regimes with Sequential Transition Times.","234":"Active Clinical Trials for Personalized Medicine.","235":"Functional CAR models for large spatially correlated functional datasets.","236":"Identification and Estimation of Causal Mechanisms in Clustered Encouragement Designs: Disentangling Bed Nets using Bayesian Principal Stratification.","237":"Comment.","238":"Comment.","239":"Probabilistic Cause-of-death Assignment using Verbal Autopsies.","240":"Efficient Estimation of the Cox Model With Auxiliary Subgroup Survival Information.","241":"A Dynamic Bayesian Model for Characterizing Cross-Neuronal Interactions During Decision-Making.","242":"Comment: Addressing the Need for Portability in Big Data Model Building and Calibration.","243":"Perturbation Detection Through Modeling of Gene Expression on a Latent Biological Pathway Network: A Bayesian hierarchical approach.","244":"Fast, Exact Bootstrap Principal Component Analysis for p &gt; 1 million.","245":"Constrained Maximum Likelihood Estimation for Model Calibration Using Summary-level Information from External Big Data Sources.","246":"Asymptotically Normal and Efficient Estimation of Covariate-Adjusted Gaussian Graphical Model.","247":"Meta-analytic framework for sparse K-means to identify disease subtypes in multiple transcriptomic studies.","248":"Large-Scale Multiple Testing of Correlations.","249":"Statistical Inference in Hidden Markov Models Using k-Segment Constraints.","250":"A Functional Approach to Deconvolve Dynamic Neuroimaging Data.","251":"Variable Selection with Prior Information for Generalized Linear Models via the Prior LASSO Method.","252":"Stepwise Signal Extraction via Marginal Likelihood.","253":"Generalizing Quantile Regression for Counting Processes with Applications to Recurrent Events.","254":"Feature Augmentation via Nonparametrics and Selection (FANS) in High-Dimensional Classification.","255":"Dirichlet-Laplace priors for optimal shrinkage.","256":"Comment.","257":"Groupwise Dimension Reduction via Envelope Method.","258":"Reinforcement Learning Trees.","259":"CONDITIONAL DISTANCE CORRELATION.","260":"The Empirical Distribution of a Large Number of Correlated Normal Variables.","261":"Comment.","262":"False discovery rate regression: an application to neural synchrony detection in primary visual cortex.","263":"A High-Dimensional Nonparametric Multivariate Test for Mean Vector.","264":"Testing and Modeling Dependencies Between a Network and Nodal Attributes.","265":"An Integrated Bayesian Nonparametric Approach for Stochastic and Variability Orders in ROC Curve Estimation: An Application to Endometriosis Diagnosis.","266":"Localized Functional Principal Component Analysis.","267":"The E-MS Algorithm: Model Selection with Incomplete Data.","268":"Comment.","269":"Bayesian Conditional Tensor Factorizations for High-Dimensional Classification.","270":"Joint Inference for Competing Risks Survival Data.","271":"Quantile Regression in the Secondary Analysis of Case-Control Data.","272":"Hierarchical Nearest-Neighbor Gaussian Process Models for Large Geostatistical Datasets.","273":"Sparse Regression Incorporating Graphical Structure among Predictors.","274":"Bayesian methods for nonignorable dropout in joint models in smoking cessation studies.","275":"Multitask Quantile Regression under the Transnormal Model.","276":"Analyzing Single-Molecule Protein Transportation Experiments via Hierarchical Hidden Markov Models.","277":"Comment.","278":"Hierarchical Feature Selection Incorporating Known and Novel Biological Information: Identifying Genomic Features Related to Prostate Cancer Recurrence.","279":"Bayesian Nonparametric Longitudinal Data Analysis.","280":"Conditional Sure Independence Screening.","281":"Fast estimation of regression parameters in a broken-stick model for longitudinal data.","282":"Hierarchical models for semi-competing risks data with application to quality of end-of-life care for pancreatic cancer.","283":"Comment: Getting into Space with a Weight Problem.","284":"Personalized Dose Finding Using Outcome Weighted Learning.","285":"Matching a Distribution by Matching Quantiles Estimation.","286":"Unifying Amplitude and Phase Analysis: A Compositional Data Approach to Functional Multivariate Mixed-Effects Modeling of Mandarin Chinese.","287":"IsoDOT Detects Differential RNA-isoform Expression\/Usage with respect to a Categorical or Continuous Covariate with High Sensitivity and Specificity.","288":null,"289":"Analysis of the Proportional Hazards Model with Sparse Longitudinal Covariates.","290":"Proper Use of Allele-Specific Expression Improves Statistical Power for cis-eQTL Mapping with RNA-Seq Data.","291":"Local dependence in random graph models: characterization, properties and statistical inference.","292":"Incomplete Categorical Data Design: Nonrandomized Response Techniques for Sensitive Questions in Surveys.","293":"SPReM: Sparse Projection Regression Model For High-dimensional Linear Regression.","294":"On an Additive Semigraphoid Model for Statistical Networks With Application to Pathway Analysis.","295":"Model-Free Feature Screening for Ultrahigh Dimensional Discriminant Analysis.","296":"Regularization Methods for High-Dimensional Instrumental Variables Regression With an Application to Genetical Genomics.","297":"Bayesian Dose-Finding in Two Treatment Cycles Based on the Joint Utility of Efficacy and Toxicity.","298":"Analysis of Sequence Data Under Multivariate Trait-Dependent Sampling.","299":"Statistical Analysis of Q-matrix Based Diagnostic Classification Models.","300":"A Two-Sample Test for Equality of Means in High Dimension.","301":"Large, Sparse Optimal Matching with Refined Covariate Balance in an Observational Study of the Health Outcomes Produced by New Surgeons.","302":"Bayesian Inference for Multivariate Meta-regression with a Partially Observed Within-Study Sample Covariance Matrix.","303":"New Statistical Learning Methods for Estimating Optimal Dynamic Treatment Regimes.","304":"Risk Classification with an Adaptive Naive Bayes Kernel Machine Model.","305":"A Flexible Bayesian Approach to Monotone Missing Data in Longitudinal Studies with Nonignorable Missingness with Application to an Acute Schizophrenia Clinical Trial.","306":"Analysis of longitudinal multivariate outcome data from couples cohort studies: application to HPV transmission dynamics.","307":"Functional and Structural Methods with Mixed Measurement Error and Misclassification in Covariates.","308":"Multivariate Meta-Analysis of Heterogeneous Studies Using Only Summary Statistics: Efficiency and Robustness.","309":"MAD Bayes for Tumor Heterogeneity - Feature Allocation with Exponential Family Sampling.","310":"Semiparametric Relative-risk Regression for Infectious Disease Transmission Data.","311":"A Unified Family of Covariate-Adjusted Response-Adaptive Designs Based on Efficiency and Ethics.","312":"Size and Shape Analysis of Error-Prone Shape Data.","313":"Homogeneity Pursuit.","314":"Bayesian Inference of Multiple Gaussian Graphical Models.","315":"A Unifying Model for Capture-Recapture and Distance Sampling Surveys of Wildlife Populations.","316":"A Dynamic Directional Model for Effective Brain Connectivity using Electrocorticographic (ECoG) Time Series.","317":"Estimating a structured covariance matrix from multi-lab measurements in high-throughput biology.","318":"Generalized species sampling priors with latent Beta reinforcements.","319":"A Simple Method for Estimating Interactions between a Treatment and a Large Number of Covariates.","320":"Score Estimating Equations from Embedded Likelihood Functions under Accelerated Failure Time Model.","321":"A Likelihood-Based Framework for Association Analysis of Allele-Specific Copy Numbers.","322":"Variable-Domain Functional Regression for Modeling ICU Data.","323":"Proportional Hazards Model with Covariate Measurement Error and Instrumental Variables.","324":"Structural pursuit over multiple undirected graphs.","325":"Biomarker Detection in Association Studies: Modeling SNPs Simultaneously via Logistic ANOVA.","326":null,"327":"A Methodology for Robust Multiproxy Paleoclimate Reconstructions and Modeling of Temperature Conditional Quantiles.","328":"Optimal Tests of Treatment Effects for the Overall Population and Two Subpopulations in Randomized Trials, using Sparse Linear Programming.","329":"Bayesian factorizations of big sparse tensors.","330":"Bayesian Partition Models for Identifying Expression Quantitative Trait Loci.","331":"Rerandomization to Balance Tiers of Covariates.","332":"Joint Estimation of Treatment and Placebo Effects in Clinical Trials with Longitudinal Blinding Assessments.","333":"Testing for nodal dependence in relational data matrices.","334":"Multi-Agent Inference in Social Networks: A Finite Population Learning Approach.","335":"An adaptive resampling test for detecting the presence of significant predictors.","336":"Generalized Ordinary Differential Equation Models.","337":"Bayesian Multiscale Modeling of Closed Curves in Point Clouds.","338":"Mechanistic Hierarchical Gaussian Processes.","339":"Causal Inference for fMRI Time Series Data with Systematic Errors of Measurement in a Balanced On\/Off Study of Social Evaluative Threat.","340":"Spatially Varying Coefficient Model for Neuroimaging Data with Jump Discontinuities.","341":"Discussion of \"Estimation and Accuracy after Model Selection\" by Brad Efron.","342":"Interaction Screening for Ultra-High Dimensional Data.","343":"The Sparse MLE for Ultra-High-Dimensional Feature Screening.","344":"Identifying Genetic Variants for Addiction via Propensity Score Adjusted Generalized Kendall's Tau.","345":"Using Data Augmentation to Facilitate Conduct of Phase I-II Clinical Trials with Delayed Outcomes.","346":"Functional Principal Component Analysis of Spatio-Temporal Point Processes with Applications in Disease Surveillance.","347":"Optimizing Sedative Dose in Preterm Infants Undergoing Treatment for Respiratory Distress Syndrome.","348":"Bayesian Generalized Low Rank Regression Models for Neuroimaging Phenotypes and Genetic Markers.","349":"Estimation and Accuracy after Model Selection.","350":"Estimation for general birth-death processes.","351":"Nonparametric Independence Screening in Sparse Ultra-High Dimensional Varying Coefficient Models.","352":"Targeted Local Support Vector Machine for Age-Dependent Classification.","353":"A Generic Path Algorithm for Regularized Statistical Estimation.","354":"Adaptive Multivariate Global Testing.","355":"Bayes variable selection in semiparametric linear models.","356":"Sparse Additive Ordinary Differential Equations for Dynamic Gene Regulatory Network Modeling.","357":"Estimating Risk with Time-to-Event Data: An Application to the Women's Health Initiative.","358":"Enriched Stick Breaking Processes for Functional Data.","359":"Variable Selection in Nonparametric Classification via Measurement Error Model Selection Likelihoods.","360":"Scale-Invariant Sparse PCA on High Dimensional Meta-elliptical Data.","361":"Kernels, Degrees of Freedom, and Power Properties of Quadratic Distance Goodness-of-Fit Tests.","362":"Modeling Bivariate Longitudinal Hormone Profiles by Hierarchical State Space Models.","363":"Bayesian Models for Multiple Outcomes in Domains with Application to the Seychelles Child Development Study.","364":"Uncertainty in Propensity Score Estimation: Bayesian Methods for Variable Selection and Model Averaged Causal Effects.","365":"A new estimation approach for combining epidemiological data from multiple sources.","366":"Feature Selection for Varying Coefficient Models With Ultrahigh Dimensional Covariates.","367":"Some Statistical Strategies for DAE-seq Data Analysis: Variable Selection and Modeling Dependencies among Observations.","368":"A locally convoluted cluster model for nucleosome positioning signals in chemical map.","369":"Frailty Models for Familial Risk with Application to Breast Cancer.","370":"Landmark Estimation of Survival and Treatment Effect in a Randomized Clinical Trial.","371":"Efficient Estimation of Semiparametric Transformation Models for Two-Phase Cohort Studies.","372":"Large sample randomization inference of causal effects in the presence of interference.","373":"Statistical Learning with Time Series Dependence: An Application to Scoring Sleep in Mice.","374":"A marginal approach to reduced-rank penalized spline smoothing with application to multilevel functional data.","375":"Case Definition and Design Sensitivity.","376":"Bayesian modeling of temporal dependence in large sparse contingency tables.","377":"Resampling Procedures for Making Inference under Nested Case-control Studies.","378":"Unified Analysis of Secondary Traits in Case-Control Association Studies.","379":"Selecting the Number of Principal Components in Functional Data.","380":"Sparse Semiparametric Nonlinear Model with Application to Chromatographic Fingerprints.","381":"Adaptive Confidence Bands for Nonparametric Regression Functions.","382":"Estimating the lifetime risk of dementia in the Canadian elderly population using cross-sectional cohort survival data.","383":"Parameter Estimation of Partial Differential Equation Models.","384":"Nonparametric Mixture of Regression Models.","385":"Latent Supervised Learning.","386":"An Integrative Bayesian Modeling Approach to Imaging Genetics.","387":"Estimation and inference concerning ordered means in analysis of covariance models with interactions.","388":"A nonparametric spatial model for periodontal data with non-random missingness.","389":"A Semiparametric Change-Point Regression Model for Longitudinal Observations.","390":"A Nonparametric Bayesian Model for Local Clustering with Application to Proteomics.","391":"A Phase I Bayesian Adaptive Design to Simultaneously Optimize Dose and Schedule Assignments Both Between and Within Patients.","392":"Sensitivity Analysis of Per-Protocol Time-to-Event Treatment Efficacy in Randomized Clinical Trials.","393":"Wavelet-Variance-Based Estimation for Composite Stochastic Processes.","394":"Simultaneous grouping pursuit and feature selection over an undirected graph.","395":"Bayesian Hierarchical Poisson Regression Models: An Application to a Driving Study with Kinematic Events.","396":"Straight to the Source: Detecting Aggregate Objects in Astronomical Images with Proper Error Control.","397":"EFFECTIVELY SELECTING A TARGET POPULATION FOR A FUTURE COMPARATIVE STUDY.","398":"Partially ordered mixed hidden Markov model for the disablement process of older adults.","399":"Auxiliary marker-assisted classification in the absence of class identifiers.","400":"Composite Partial Likelihood Estimation Under Length-Biased Sampling, With Application to a Prevalent Cohort Study of Dementia.","401":"A Study of Mexican Free-Tailed Bat Chirp Syllables: Bayesian Functional Mixed Models for Nonstationary Acoustic Time Series.","402":"Mediation and spillover effects in group-randomized trials: a case study of the 4Rs educational intervention.","403":"Bayesian Gaussian Copula Factor Models for Mixed Data.","404":"Efficient Robust Regression via Two-Stage Generalized Empirical Likelihood.","405":"Robust Variable Selection with Exponential Squared Loss.","406":"Meta-Analysis of Rare Binary Adverse Event Data.","407":"A Unified Approach to Semiparametric Transformation Models under General Biased Sampling Schemes.","408":"Measurement Error Case Series Models with Application to Infection-Cardiovascular Risk in OlderPatients on Dialysis.","409":"A Bayesian Procedure for File Linking to Analyze End-of-Life Medical Costs.","410":"Estimating Individualized Treatment Rules Using Outcome Weighted Learning.","411":"Deconvolution When Classifying Noisy Data Involving Transformations.","412":"Reconstructing Past Populations With Uncertainty From Fragmentary Data.","413":"SMART Design Issues and the Consideration of Opposing Outcomes: Discussion of \"Evaluation of Viable Dynamic Treatment Regimes in a Sequentially Randomized Trial of Advanced Prostate Cancer\" by by Wang, Rotnitzky, Lin, Millikan, and Thall.","414":"Classification via Bayesian Nonparametric Learning of Affine Subspaces.","415":"Consistent high-dimensional Bayesian variable selection via penalized credible regions.","416":"Evaluating the Effect of Early Versus Late ARV Regimen Change if Failure on an Initial Regimen: Results From the AIDS Clinical Trials Group Study A5095.","417":"Landmark Prediction of Long Term Survival Incorporating Short Term Event Time Information.","418":"Vast Portfolio Selection with Gross-exposure Constraints().","419":"Locally Adaptive Bayes Nonparametric Regression via Nested Gaussian Processes.","420":"Distribution Free Prediction Sets.","421":"Tensor Regression with Applications in Neuroimaging Data Analysis.","422":"Optimal Allocation of Gold Standard Testing under Constrained Availability: Application to Assessment of HIV Treatment Failure.","423":"Vast Volatility Matrix Estimation using High Frequency Data for Portfolio Selection.","424":"Functional Causal Mediation Analysis With an Application to Brain Connectivity.","425":"Discussion of Evaluation of Viable Dynamic Treatment Regimes in a Sequentially Randomized Trial of Advanced Prostate Cancer, by Wang et al. 2012.","426":"A Multiresolution Method for Parameter Estimation of Diffusion Processes.","427":"SURE Estimates for a Heteroscedastic Hierarchical Model.","428":"Estimating Identification Disclosure Risk Using Mixed Membership Models.","429":"Recursively Imputed Survival Trees.","430":"Quantile Regression for Analyzing Heterogeneity in Ultra-high Dimension.","431":"Evaluation of Viable Dynamic Treatment Regimes in a Sequentially Randomized Trial of Advanced Prostate Cancer.","432":"Estimating Regression Parameters in an Extended Proportional Odds Model.","433":"Instability, Sensitivity, and Degeneracy of Discrete Exponential Families.","434":"Feature Screening via Distance Correlation Learning.","435":"Nonparametric Covariate-Adjusted Association Tests Based on the Generalized Kendall's Tau().","436":"Likelihood-based selection and sharp parameter estimation.","437":"Bayesian Kernel Mixtures for Counts.","438":"Nonparametric Bayes Stochastically Ordered Latent Class Models.","439":"Correcting for Population Stratification in Genomewide Association Studies.","440":"Semiparametric Stochastic Modeling of the Rate Function in Longitudinal Studies.","441":"Robust EM Continual Reassessment Method in Oncology Dose Finding.","442":"Density Estimation in Several Populations With Uncertain Population Membership.","443":"Simplex Factor Models for Multivariate Unordered Categorical Data.","444":"Intrinsic Regression Models for Medial Representation of Subcortical Structures.","445":"Partially hidden Markov model for time-varying principal stratification in HIV prevention trials.","446":"Semiparametric Approach to a Random Effects Quantile Regression Model.","447":"Maximum Likelihood Estimations and EM Algorithms with Length-biased Data.","448":"Robust, Adaptive Functional Regression in Functional Mixed Model Framework.","449":"Nonparametric Independence Screening in Sparse Ultra-High Dimensional Additive Models.","450":"Minkowski-Weyl Priors for Models With Parameter Constraints: An Analysis of the BioCycle Study.","451":"Contrasting Evidence Within and Between Institutions That Provide Treatment in an Observational Study of Alternate Forms of Anesthesia.","452":"Inference with interference between units in an fMRI experiment of motor inhibition.","453":"Spatio-Spectral Mixed Effects Model for Functional Magnetic Resonance Imaging Data.","454":"Comment on \"Estimating False Discovery Proportion Under Arbitrary Covariance Dependence\" by Fan et al.","455":"Estimating False Discovery Proportion Under Arbitrary Covariance Dependence.","456":"Sparse Estimation of Conditional Graphical Models With Application to Gene Networks.","457":"Modeling Criminal Careers as Departures from a Unimodal Population Age-Crime Curve: The Case of Marijuana Use.","458":"Nonparametric estimation for censored mixture data with application to the Cooperative Huntington's Observational Research Trial.","459":"Rejoinder to comments on Evaluation of Viable Dynamic Treatment Regimes in a Sequentially Randomized Trial of Advanced Prostate Cancer.","460":"Bayesian Model Selection in High-Dimensional Settings.","461":"Structural Nested Cumulative Failure Time Models to Estimate the Effects of Interventions.","462":"Multiscale Community Blockmodel for Network Exploration.","463":"A Semiparametric Approach to Dimension Reduction.","464":"Nonparametric Bayes Modeling of Multivariate Categorical Data.","465":"Hard or Soft Classification? Large-margin Unified Machines.","466":"Fast and Accurate Approximation to Significance Tests in Genome-Wide Association Studies.","467":"A regularized Hotelling's T2 test for pathway analysis in proteomic studies.","468":"Linear or Nonlinear? Automatic Structure Discovery for Partially Linear Models.","469":"Adaptive Confidence Intervals for the Test Error in Classification.","470":"Comment on \"Correlated z-values and the accuracy of large-scale statistical estimates\" by Bradley Efron.","471":"Randomization-Based Inference within Principal Strata.","472":"Cocaine Dependence Treatment Data: Methods for Measurement Error Problems With Predictors Derived From Stationary Stochastic Processes.","473":"False Discovery Rate Control With Groups.","474":"Estimating the Term Structure With a Semiparametric Bayesian Hierarchical Model: An Application to Corporate Bonds.","475":"Modeling Three-Dimensional Chromosome Structures Using Gene Expression Data.","476":"Comment on \"Adaptive Confidence Intervals for the Test Error in Classification\"","477":"Meta Analysis of Functional Neuroimaging Data via Bayesian Spatial Point Processes.","478":"Testing and Estimating Shape-Constrained Nonparametric Density and Regression in the Presence of Measurement Error.","479":"Inverse regression estimation for censored data.","480":"A Rank-Based Test for Comparison of Multidimensional Outcomes.","481":"A Bayesian Shrinkage Model for Incomplete Longitudinal Binary Data with Application to the Breast Cancer Prevention Trial.","482":"Bayesian Random Segmentation Models to Identify Shared Copy Number Aberrations for Array CGH Data.","483":"Modeling Competing Infectious Pathogens from a Bayesian Perspective: Application to Influenza Studies with Incomplete Laboratory Results.","484":"Bayesian Spatial Quantile Regression.","485":"CAUSAL EFFECTS OF TREATMENTS FOR INFORMATIVE MISSING DATA DUE TO PROGRESSION\/DEATH.","486":"An Approach to the Estimation of Chronic Air Pollution Effects Using Spatio-Temporal Information.","487":"Independent Component Analysis Involving Autocorrelated Sources With an Application to Functional Magnetic Resonance Imaging.","488":"Bayesian Inference for General Gaussian Graphical Models With Application to Multivariate Lattice Data.","489":"A Statistical Framework for the Analysis of ChIP-Seq Data.","490":"Hierarchical Clustering With Prototypes via Minimax Linkage.","491":"Modeling Protein Expression and Protein Signaling Pathways.","492":"A Hierarchical Model for Quantifying Forest Variables Over Large Heterogeneous Landscapes With Uncertain Forest Areas.","493":"SparseNet: Coordinate Descent With Nonconvex Penalties.","494":"Population Value Decomposition, a Framework for the Analysis of Image Populations.","495":"Predicting Viral Infection From High-Dimensional Biomarker Trajectories.","496":"A Framework for Assessing Broad Sense Agreement Between Ordinal and Continuous Measurements.","497":"High Dimensional ODEs Coupled with Mixed-Effects Modeling Techniques for Dynamic Gene Regulatory Network Identification.","498":"Order restricted inference for multivariate binary data with application to toxicology.","499":"A Perturbation Method for Inference on Regularized Regression Estimates.","500":"Non-parametric Evaluation of Biomarker Accuracy under Nested Case-control Studies.","501":"Model-Free Feature Screening for Ultrahigh Dimensional Data.","502":"Tweedie's Formula and Selection Bias.","503":"Bayesian Modeling of MPSS Data: Gene Expression Analysis of Bovine Salmonella Infection.","504":"Weighted Distance Weighted Discrimination and Its Asymptotic Properties.","505":"Using a short screening scale for small-area estimation of mental illness prevalence for schools.","506":"Robust Model-Free Multiclass Probability Estimation.","507":"Informative Retesting.","508":"Correlated z-values and the accuracy of large-scale statistical estimates.","509":"Test of Association Between Two Ordinal Variables While Adjusting for Covariates.","510":"A framework for feature selection in clustering.","511":"Nonparametric Regression With Missing Outcomes Using Weighted Kernel Estimating Equations.","512":"On Estimation of Partially Linear Transformation Models.","513":"An Association Test for Multiple Traits Based on the Generalized Kendall's Tau.","514":"A Class of Semiparametric Mixture Cure Survival Models with Dependent Censoring.","515":"Grouping pursuit through a regularization solution surface.","516":"Generalized Functional Linear Models with Semiparametric Single-Index Interactions.","517":"Regularization Parameter Selections via Generalized Information Criterion.","518":"Hierarchical Spatial Process Models for Multiple Traits in Large Genetic Trials.","519":"Using DNA fingerprints to infer familial relationships within NHANES III households.","520":"Local Rank Inference for Varying Coefficient Models.","521":"Generalized Multilevel Functional Regression.","522":"On Consistency and Sparsity for Principal Components Analysis in High Dimensions.","523":"Nonparametric Prediction in Measurement Error Models.","524":"An Incomplete-Data Quasi-likelihood Approach to Haplotype-Based Genetic Association Studies on Related Individuals.","525":"Reduced Rank Mixed Effects Models for Spatially Correlated Hierarchical Functional Data.","526":"A Semiparametric Regression Cure Model for Interval-Censored Data.","527":"Latent Stick-Breaking Processes.","528":"Optimal Sparse Segment Identification with Application in Copy Number Variation Analysis.","529":"A Design-Adaptive Local Polynomial Estimator for the Errors-in-Variables Problem.","530":"On Nonparametric Variance Estimation for Second-Order Statistics of Inhomogeneous Spatial Point Processes With a Known Parametric Intensity Form.","531":"Empirical Bayes Estimates for Large-Scale Prediction Problems.","532":"Quantile Regression With Measurement Error.","533":"Singular Value Decomposition-based Alternative Splicing Detection.","534":"Density Estimation for Protein Conformation Angles Using a Bivariate von Mises Distribution and Bayesian Nonparametrics.","535":"How many people do you know?: Efficiently estimating personal network size.","536":"Approximate Methods for State-Space Models.","537":"Intrinsic Regression Models for Positive-Definite Matrices With Applications to Diffusion Tensor Imaging.","538":"A Class of Transformed Mean Residual Life Models With Censored Survival Data.","539":"Joint Models for the Association of Longitudinal Binary and Continuous Processes With Application to a Smoking Cessation Trial.","540":"Joint modeling of self-rated health and changes in physical functioning.","541":"On large margin hierarchical classification with multiple paths.","542":"Bayesian Calibration of Microsimulation Models.","543":"Nonparametric Signal Extraction and Measurement Error in the Analysis of Electroencephalographic Activity During Sleep.","544":"Variable Selection in Nonparametric Varying-Coefficient Models for Analysis of Repeated Measurements.","545":"Variable Selection for Partially Linear Models with Measurement Errors.","546":"Group Comparison of Eigenvalues and Eigenvectors of Diffusion Tensors.","547":"Resolving Contested Elections: The Limited Power of Post-Vote Vote-Choice Data.","548":"Least Absolute Relative Error Estimation.","549":"Semiparametric Efficient Estimation for a Class of Generalized Proportional Odds Cure Models.","550":"Global Partial Likelihood for Nonparametric Proportional Hazards Models.","551":"Estimating Individual-Level Risk in Spatial Epidemiology Using Spatially Aggregated Information on the Population at Risk.","552":"A Statistical Approach to Thermal Management of Data Centers Under Steady State and System Perturbations.","553":"Optimal Partitioning for Linear Mixed Effects Models: Applications to Identifying Placebo Responders.","554":"A Case Study in Pharmacologic Colon Imaging Using Principal Curves in Single Photon Emission Computed Tomography.","555":"Repeated Measurements on Distinct Scales With Censoring-A Bayesian Approach Applied to Microarray Analysis of Maize.","556":"Intrinsic Regression Models for Manifold-Valued Data.","557":"Parameter Estimation for Differential Equation Models Using a Framework of Measurement Error in Regression Models.","558":"Nonparametric Bayes Conditional Distribution Modeling With Variable Selection.","559":"Amplification of Sensitivity Analysis in Matched Observational Studies.","560":"Rejoinder.","561":"Regression Models for Identifying Noise Sources in Magnetic Resonance Images.","562":"Partial Correlation Estimation by Joint Sparse Regression Models.","563":"Nonparametric Residue Analysis of Dynamic PET Data With Application to Cerebral FDG Studies in Normals.","564":"Mapping Ancient Forests: Bayesian Inference for Spatio-temporal Trends in Forest Composition Using the Fossil Pollen Proxy Record.","565":"Estimation of Parameters Subject to Order Restrictions on a Circle With Application to Estimation of Phase Angles of Cell Cycle Genes.","566":"Cox Models With Smooth Functional Effect of Covariates Measured With Error.","567":"Analyzing Length-biased Data with Semiparametric Transformation and Accelerated Failure Time Models.","568":"Model Selection Criteria for Missing-Data Problems Using the EM Algorithm.","569":"Log-Linear Models for Gene Association.","570":"A Bayesian hierarchical model for analysis of SNP diversity in multilocus, multipopulation samples.","571":"Random Effects Models in a Meta-Analysis of the Accuracy of Two Diagnostic Tests Without a Gold Standard.","572":"Properties and Implementation of Jeffreys's Prior in Binomial Regression Models.","573":"Shrinkage Estimators for Robust and Efficient Inference in Haplotype-Based Case-Control Studies.","574":"Analysis of Smoking Cessation Patterns Using a Stochastic Mixed-Effects Model With a Latent Cured State.","575":"Assessing Sexual Attitudes and Behaviors of Young Women: A Joint Model with Nonlinear Time Effects, Time Varying Covariates, and Dropouts.","576":"Screening Experiments for Developing Dynamic Treatment Regimes.","577":"Semiparametric estimation of covariance matrices for longitudinal data.","578":"Current Methods for Recurrent Events Data with Dependent Termination: A Bayesian Perspective.","579":"Bayesian semiparametric joint models for functional predictors.","580":"A Bayesian model for cross-study differential gene expression.","581":"Estimating Time to Event From Longitudinal Categorical Data: An Analysis of Multiple Sclerosis Progression.","582":"Toward Causal Inference With Interference.","583":"Multiply robust inference for statistical interactions.","584":"High-Dimensional Sparse Factor Modeling: Applications in Gene Expression Genomics.","585":null,"586":"Does Finasteride Affect the Severity of Prostate Cancer? A Causal Sensitivity Analysis.","587":"Empirical Likelihood-Based Estimation of the Treatment Effect in a Pretest-Posttest Study.","588":"Binary Time Series Modeling with Application to Adhesion Frequency Experiments.","589":"Imputing Risk Tolerance From Survey Responses.","590":"Structured measurement error in nutritional epidemiology: applications in the Pregnancy, Infection, and Nutrition (PIN) Study.","591":"Bayesian Hidden Markov Modeling of Array CGH Data.","592":"Penalized Estimating Functions and Variable Selection in Semiparametric Regression Models.","593":"Classification of Missense Mutations of Disease Genes.","594":"Population-Calibrated Gene Characterization: Estimating Age at Onset Distributions Associated With Cancer Genes.","595":"Longitudinal Studies With Outcome-Dependent Follow-up: Models and Bayesian Regression.","596":"Bayesian Gaussian Mixture Models for High-Density Genotyping Arrays.","597":"Application of Multidimensional Selective Item Response Regression Model for Studying Multiple Gene Methylation in SV40 Oncogenic Pathways.","598":"On Estimating Diagnostic Accuracy From Studies With Multiple Raters and Partial Gold Standard Evaluation.","599":"Bayesian Inference on Changes in Response Densities over Predictor Clusters.","600":"A Test for Anchoring and Yea-Saying in Experimental Consumption Data.","601":"Modeling Disease Progression with Longitudinal Markers.","602":"A Unified Approach to Nonparametric Comparison of Receiver Operating Characteristic Curves for Longitudinal and Clustered Data.","603":"Time-dependent Predictive Values of Prognostic Biomarkers with Failure Time Outcome.","604":"Semiparametric Analysis of Heterogeneous Data Using Varying-Scale Generalized Linear Models.","605":"A Multiresolution Hazard Model for Multicenter Survival Studies: Application to Tamoxifen Treatment in Early Stage Breast Cancer.","606":"Inference of Tamoxifen's Effects on Prevention of Breast Cancer from a Randomized Controlled Trial.","607":"On the Estimation of Disability-Free Life Expectancy: Sullivan' Method and Its Extension.","608":"Flexible Cure Rate Modeling Under Latent Activation Schemes.","609":"Analysis of Longitudinal Data with Semiparametric Estimation of Covariance Function.","610":"Sensitivity Analyses Comparing Time-to-Event Outcomes Existing Only in a Subset Selected Postrandomization.","611":"Approximate likelihood for large irregularly spaced spatial data.","612":"Incorporating Historical Control Data When Comparing Tumor Incidence Rates.","613":"High Resolution Space-Time Ozone Modeling for Assessing Trends.","614":"Bayesian Wombling: Curvilinear Gradient Assessment Under Spatial Process Models.","615":"Using Wavelet-Based Functional Mixed Models to Characterize Population Heterogeneity in Accelerometer Profiles: A Case Study.","616":"Global Validation of Linear Model Assumptions.","617":"Causal Vaccine Effects on Binary Postinfection Outcomes.","618":"Variance Estimation in a Model with Gaussian Sub-Models.","619":"Estimating Load-Sharing Properties in a Dynamic Reliability System.","620":"Joint Modeling and Estimation for Recurrent Event Processes and Failure Time Data.","621":"Methodology for Evaluating a Partially Controlled Longitudinal Treatment Using Principal Stratification, With Application to a Needle Exchange Program.","622":"Estimating Cure Rates From Survival Data: An Alternative to Two-Component Mixture Models.","623":"The influence of social programs in source countries on various classes of U.S. immigration.","624":"Establishing the nadir of the body mass index-mortality relationship: a case study.","625":"Bayesian demography: projecting the Iraqi Kurdish population, 1977-1990.","626":"Matching With Doses in an Observational Study of a Media Campaign Against Drug Abuse.","627":"Marginal Mean Models for Dynamic Regimes.","628":"Analyzing Recurrent Event Data With Informative Censoring.","629":"Nonparametric Estimation of a Recurrent Survival Function.","630":"A nested frailty model for survival data, with an application to the study of child survival in northeast Brazil.","631":"A random-effects model for cycle viability in fertility studies.","632":"Statistics in epidemiology: the case-control study.","633":"An evaluation of population projection errors for census tracts.","634":"Stochastic population forecasts for the United States:  beyond high, medium, and low.","635":"Hierarchical logistic regression models for imputation of unresolved enumeration status in undercount estimation.","636":"A three-sample multiple-recapture approach to census population estimation with heterogeneous catchability.","637":"Estimating heterogeneity in the probabilities of enumeration for dual-system estimation.","638":"Assessing between-block heterogeneity within the post-strata of the 1990 Post-Enumeration Survey.","639":"Combining census, dual-system, and evaluation study data to estimate population shares.","640":"Regression analysis of current-status data:  an application to breast-feeding.","641":"Using information from demographic analysis in post-enumeration survey estimation.","642":"Accuracy of the 1990 census and undercount adjustments.","643":"Estimation of population coverage in the 1990 United States census based on demographic analysis.","644":"The 1990 Post-Enumeration Survey:  operations and results.","645":"Power Calculations for General Linear Multivariate Models Including Repeated Measures Applications.","646":"Age patterns of marital fertility:  revising the Coale-Trussell method.","647":"On the quality of reinterview data with application to the Current Population Survey.","648":"Modeling household fertility decisions:  a nonlinear simultaneous probit model.","649":"Total error in PES estimates of population.","650":"Adjustment of provisional mortality series:  the dynamic linear model with structured measurement errors.","651":"The effects of census undercount adjustment on congressional apportionment.","652":"Evaluation of procedures for improving population estimates for small areas.","653":"Modeling American marriage patterns.","654":"Error models for official mortality forecasts.","655":"The relationship between the length of the base period and population forecast errors.","656":"Estimating fecundability from data on waiting times to first conception.","657":"Measurement of Hispanic ethnicity in the U.S. census:  an evaluation based on latent-class analysis.","658":"Censored survival data with misclassified covariates:  a case study of breast-cancer mortality.","659":"Adjusting the 1980 census of population and housing.","660":"Forecasting aggregate period-specific birth rates:  the time series properties of a microdynamic neoclassical model of fertility.","661":"Empirical Bayes estimation of undercount in the decennial census.","662":"The World Fertility Survey:  an appraisal of methodology.","663":"Multivariate time series projections of parameterized age-specific fertility rates.","664":"Empirical Bayes procedures for stabilizing maps of U.S. cancer mortality rates.","665":"Measuring migration distances:  self-reporting and indirect methods.","666":"Mixed model for analyzing geographic variability in mortality rates.","667":"Tests of forecast accuracy and bias for county population projections.","668":"Methods for national population forecasts:  a review.","669":"Birth forecasting based on birth order probabilities, with application to U.S. data.","670":"Alternative models for the heterogeneity of mortality risks among the aged.","671":"Some coverage error models for census data.","672":"A review and evaluation of the housing unit method of population estimation.","673":"Child health, breast-feeding, and survival in Malaysia:  a random-effects logit approach.","674":"Improving the accuracy of intercensal estimates and postcensal projections of the civilian noninstitutional population:  a parameterization of institutional prevalence rates.","675":"Parameterized multistate population dynamics and projections.","676":"Survival models for fertility evaluation.","677":"A time series model for cohort data.","678":"Uncertain population forecasting.","679":"Forecasting interstate migration with limited data:  a demographic-economic approach.","680":"A comparison of population estimation methods:  housing unit versus component II, ratio correlation, and administrative records.","681":"New estimates of child mortality in the United States at the turn of the century.","682":"Reproductive response to child mortality:  a maximum likelihood estimation model.","683":"The accuracy of population projections.","684":"Modeling demographic relationships:  an analysis of forecast functions for Australian births.","685":"Seasonal patterns of fertility measures: theory and data.","686":"Estimating the population median by nomination sampling.","687":"Births time series models and structural interpretations.","688":"An adjustment of a selection bias in postpartum amenorrhea from follow-up studies.","689":"A general algorithm for estimating a Markov-generated increment-decrement life table with applications to marital-status patterns.","690":"Rejoinder [to comment by Dyn, Wahba and Wong on \"Smooth pycnophylactic interpolation for geographical regions\"].","691":"Comment [on \"Smooth pycnophylactic interpolation for geographical regions\"].","692":"Smooth pycnophylactic interpolation for geographical regions.","693":"A model-based approach to estimation for small subgroups of a population.","694":"Karhunen-Loeve analysis of historical time series with an application to plantation births in Jamaica.","695":"Randomized response technique in a National Survey.","696":"The effect of air pollution upon mortality: a consideration of distributed lag models.","697":"Forecasting births in post-transition population: stochastic renewal with serially correlated fertility.","698":"Analysis of repeated surveys using time series methods.","699":"The linear model formulation of a multitype branching process applied to population dynamics.","700":"Estimates of parameters in a probability model for first livebirth interval.","701":"The distribution by age of the frequency of first marriage in a female cohort.","702":"On future population.","703":"Truncation effect in closed and open birth interval data.","704":"Higher female than male mortality in some countries of South Asia: a digest.","705":"On the interpretation of age distributions.","706":"Randomized response: a survey technique for eliminating evasive answer bias.","707":"The case of the Indians and the teen-age widows.","708":"Bias in estimates of the U.S. nonwhite population as indicated by trends in death rates.","709":"On the best choice of sample sizes for a t-test when the ratio of variances is known.","710":"The city block as a unit for recording and analyzing urban data.","711":"Note on some errors in the evidence of periodicity in short time series.","712":"The fitting of logistic curves by means of a nomograph.","713":"On some mathematical problems arising in the development of Mendelian genetics.","714":"On estimating the mean and standard deviation of truncated normal distributions.","715":"The relation of the net reproduction rate to other fertility measures.","716":"Statistics of the Kinsey report.","717":"Beneficiary statistics under the old-age and survivors insurance program and some possible demographic studies based on these data.","718":"Applications of some significance tests for the median which are valid under very general conditions.","719":"By-product data and forecasting in unemployment insurance.","720":"Control of a general census by means of an area sampling method.","721":"A sampling study of the merits of autoregressive and reduced form transformation in regression analysis.","722":"The Monte Carlo method.","723":"The edge marking of statistical cards.","724":"Minimum X2 maximum likelihood solution in terms of a linear transform, with particular reference to bio-assay.","725":"Unemployment and migration in the depression, 1930-1935.","726":"The use of sampling in Great Britain.","727":"The uses of usefulness of binomial probability paper.","728":"The current status of state and local population estimates in the Census Bureau.","729":"A test for symmetry in contingency tables.","730":"Main effects and interactions.","731":"Commercial uses of sampling.","732":"Sampling errors in mortality and other statistics in life insurance.","733":"Recent developments in graduation and interpolation.","734":"Problems with sampling procedures for reserve valuations.","735":"On the determination of sample sizes in designing experiments.","736":"The relation of control charts to analysis of variance and chi-square tests.","737":"Estimating the resident alien population of the United States.","738":"On a population sample for Greece.","739":"The problem of plot size in large-scale yield surveys.","740":"The use of the angular transformation in biological assays.","741":"Cost-utility as a measure of the efficiency of a test.","742":"The best unbiased estimate of population standard deviation based on group ranges.","743":"A critique of a method of making actuarial estimates for a compulsory health insurance system.","744":"Problems of multiple-punching with Hollerith machines.","745":"The statistical program of the Census Bureau.","746":"The statistical sign test.","747":"The problem of non-response in sample surveys.","748":"Reproduction rates adjusted for age, parity, fecundity, and marriage.","749":"The design and analysis of methods for sampling microclimatic factors.","750":"Calculating the geometric mean from a large amount of data.","751":"Systematic sampling and its relation to other sampling designs.","752":"A method of making actuarial estimates for a compulsory health insurance system."},"doi":{"0":"","1":"","2":"","3":"","4":"","5":"","6":"","7":"","8":"","9":"","10":"","11":"","12":"","13":"","14":"","15":"","16":"","17":"","18":"","19":"","20":"","21":"","22":"","23":"","24":"","25":"","26":"","27":"","28":"","29":"","30":"","31":"","32":"","33":"","34":"","35":"","36":"","37":"","38":"","39":"","40":"","41":"","42":"","43":"","44":"","45":"","46":"","47":"","48":"","49":"","50":"","51":"","52":"","53":"","54":"","55":"","56":"","57":"","58":"","59":"","60":"","61":"","62":"","63":"","64":"","65":"","66":"","67":"","68":"","69":"","70":"","71":"","72":"","73":"","74":"","75":"","76":"","77":"","78":"","79":"","80":"","81":"","82":"","83":"","84":"","85":"","86":"","87":"","88":"","89":"","90":"","91":"","92":"","93":"","94":"","95":"","96":"","97":"","98":"","99":"","100":"","101":"","102":"","103":"","104":"","105":"","106":"","107":"","108":"","109":"","110":"","111":"","112":"","113":"","114":"","115":"","116":"","117":"","118":"","119":"","120":"","121":"","122":"","123":"","124":"","125":"","126":"","127":"","128":"","129":"","130":"","131":"","132":"","133":"","134":"","135":"","136":"","137":"","138":"","139":"","140":"","141":"","142":"","143":"","144":"","145":"","146":"","147":"","148":"","149":"","150":"","151":"","152":"","153":"","154":"","155":"","156":"","157":"","158":"","159":"","160":"","161":"","162":"","163":"","164":"","165":"","166":"","167":"","168":"","169":"","170":"","171":"","172":"","173":"","174":"","175":"","176":"","177":"","178":"","179":"","180":"","181":"","182":"","183":"","184":"","185":"","186":"","187":"","188":"","189":"","190":"","191":"","192":"","193":"","194":"","195":"","196":"","197":"","198":"","199":"","200":"","201":"","202":"","203":"","204":"","205":"","206":"","207":"","208":"","209":"","210":"","211":"","212":"","213":"","214":"","215":"","216":"","217":"","218":"","219":"","220":"","221":"","222":"","223":"","224":"","225":"","226":"","227":"","228":"","229":"","230":"","231":"","232":"","233":"","234":"","235":"","236":"","237":"","238":"","239":"","240":"","241":"","242":"","243":"","244":"","245":"","246":"","247":"","248":"","249":"","250":"","251":"","252":"","253":"","254":"","255":"","256":"","257":"","258":"","259":"","260":"","261":"","262":"","263":"","264":"","265":"","266":"","267":"","268":"","269":"","270":"","271":"","272":"","273":"","274":"","275":"","276":"","277":"","278":"","279":"","280":"","281":"","282":"","283":"","284":"","285":"","286":"","287":"","288":"","289":"","290":"","291":"","292":"","293":"","294":"","295":"","296":"","297":"","298":"","299":"","300":"","301":"","302":"","303":"","304":"","305":"","306":"","307":"","308":"","309":"","310":"","311":"","312":"","313":"","314":"","315":"","316":"","317":"","318":"","319":"","320":"","321":"","322":"","323":"","324":"","325":"","326":"","327":"","328":"","329":"","330":"","331":"","332":"","333":"","334":"","335":"","336":"","337":"","338":"","339":"","340":"","341":"","342":"","343":"","344":"","345":"","346":"","347":"","348":"","349":"","350":"","351":"","352":"","353":"","354":"","355":"","356":"","357":"","358":"","359":"","360":"","361":"","362":"","363":"","364":"","365":"","366":"","367":"","368":"","369":"","370":"","371":"","372":"","373":"","374":"","375":"","376":"","377":"","378":"","379":"","380":"","381":"","382":"","383":"","384":"","385":"","386":"","387":"","388":"","389":"","390":"","391":"","392":"","393":"","394":"","395":"","396":"","397":"","398":"","399":"","400":"","401":"","402":"","403":"","404":"","405":"","406":"","407":"","408":"","409":"","410":"","411":"","412":"","413":"","414":"","415":"","416":"","417":"","418":"","419":"","420":"","421":"","422":"","423":"","424":"","425":"","426":"","427":"","428":"","429":"","430":"","431":"","432":"","433":"","434":"","435":"","436":"","437":"","438":"","439":"","440":"","441":"","442":"","443":"","444":"","445":"","446":"","447":"","448":"","449":"","450":"","451":"","452":"","453":"","454":"","455":"","456":"","457":"","458":"","459":"","460":"","461":"","462":"","463":"","464":"","465":"","466":"","467":"","468":"","469":"","470":"","471":"","472":"","473":"","474":"","475":"","476":"","477":"","478":"","479":"","480":"","481":"","482":"","483":"","484":"","485":"","486":"","487":"","488":"","489":"","490":"","491":"","492":"","493":"","494":"","495":"","496":"","497":"","498":"","499":"","500":"","501":"","502":"","503":"","504":"","505":"","506":"","507":"","508":"","509":"","510":"","511":"","512":"","513":"","514":"","515":"","516":"","517":"","518":"","519":"","520":"","521":"","522":"","523":"","524":"","525":"","526":"","527":"","528":"","529":"","530":"","531":"","532":"","533":"","534":"","535":"","536":"","537":"","538":"","539":"","540":"","541":"","542":"","543":"","544":"","545":"","546":"","547":"","548":"","549":"","550":"","551":"","552":"","553":"","554":"","555":"","556":"","557":"","558":"","559":"","560":"","561":"","562":"","563":"","564":"","565":"","566":"","567":"","568":"","569":"","570":"","571":"","572":"","573":"","574":"","575":"","576":"","577":"","578":"","579":"","580":"","581":"","582":"","583":"","584":"","585":"","586":"","587":"","588":"","589":"","590":"","591":"","592":"","593":"","594":"","595":"","596":"","597":"","598":"","599":"","600":"","601":"","602":"","603":"","604":"","605":"","606":"","607":"","608":"","609":"","610":"","611":"","612":"","613":"","614":"","615":"","616":"","617":"","618":"","619":"","620":"","621":"","622":"","623":"","624":"","625":"","626":"","627":"","628":"","629":"","630":"","631":"","632":"","633":"","634":"","635":"","636":"","637":"","638":"","639":"","640":"","641":"","642":"","643":"","644":"","645":"","646":"","647":"","648":"","649":"","650":"","651":"","652":"","653":"","654":"","655":"","656":"","657":"","658":"","659":"","660":"","661":"","662":"","663":"","664":"","665":"","666":"","667":"","668":"","669":"","670":"","671":"","672":"","673":"","674":"","675":"","676":"","677":"","678":"","679":"","680":"","681":"","682":"","683":"","684":"","685":"","686":"","687":"","688":"","689":"","690":"","691":"","692":"","693":"","694":"","695":"","696":"","697":"","698":"","699":"","700":"","701":"","702":"","703":"","704":"","705":"","706":"","707":"","708":"","709":"","710":"","711":"","712":"","713":"","714":"","715":"","716":"","717":"","718":"","719":"","720":"","721":"","722":"","723":"","724":"","725":"","726":"","727":"","728":"","729":"","730":"","731":"","732":"","733":"","734":"","735":"","736":"","737":"","738":"","739":"","740":"","741":"","742":"","743":"","744":"","745":"","746":"","747":"","748":"","749":"","750":"","751":"","752":""},"journal_title":{"0":"Journal of the American Statistical Association","1":"Journal of the American Statistical Association","2":"Journal of the American Statistical Association","3":"Journal of the American Statistical Association","4":"Journal of the American Statistical Association","5":"Journal of the American Statistical Association","6":"Journal of the American Statistical Association","7":"Journal of the American Statistical Association","8":"Journal of the American Statistical Association","9":"Journal of the American Statistical Association","10":"Journal of the American Statistical Association","11":"Journal of the American Statistical Association","12":"Journal of the American Statistical Association","13":"Journal of the American Statistical Association","14":"Journal of the American Statistical Association","15":"Journal of the American Statistical Association","16":"Journal of the American Statistical Association","17":"Journal of the American Statistical Association","18":"Journal of the American Statistical Association","19":"Journal of the American Statistical Association","20":"Journal of the American Statistical Association","21":"Journal of the American Statistical Association","22":"Journal of the American Statistical Association","23":"Journal of the American Statistical Association","24":"Journal of the American Statistical Association","25":"Journal of the American Statistical Association","26":"Journal of the American Statistical Association","27":"Journal of the American Statistical Association","28":"Journal of the American Statistical Association","29":"Journal of the American Statistical Association","30":"Journal of the American Statistical Association","31":"Journal of the American Statistical Association","32":"Journal of the American Statistical Association","33":"Journal of the American Statistical Association","34":"Journal of the American Statistical Association","35":"Journal of the American Statistical Association","36":"Journal of the American Statistical Association","37":"Journal of the American Statistical Association","38":"Journal of the American Statistical Association","39":"Journal of the American Statistical Association","40":"Journal of the American Statistical Association","41":"Journal of the American Statistical Association","42":"Journal of the American Statistical Association","43":"Journal of the American Statistical Association","44":"Journal of the American Statistical Association","45":"Journal of the American Statistical Association","46":"Journal of the American Statistical Association","47":"Journal of the American Statistical Association","48":"Journal of the American Statistical Association","49":"Journal of the American Statistical Association","50":"Journal of the American Statistical Association","51":"Journal of the American Statistical Association","52":"Journal of the American Statistical Association","53":"Journal of the American Statistical Association","54":"Journal of the American Statistical Association","55":"Journal of the American Statistical Association","56":"Journal of the American Statistical Association","57":"Journal of the American Statistical Association","58":"Journal of the American Statistical Association","59":"Journal of the American Statistical Association","60":"Journal of the American Statistical Association","61":"Journal of the American Statistical Association","62":"Journal of the American Statistical Association","63":"Journal of the American Statistical Association","64":"Journal of the American Statistical Association","65":"Journal of the American Statistical Association","66":"Journal of the American Statistical Association","67":"Journal of the American Statistical Association","68":"Journal of the American Statistical Association","69":"Journal of the American Statistical Association","70":"Journal of the American Statistical Association","71":"Journal of the American Statistical Association","72":"Journal of the American Statistical Association","73":"Journal of the American Statistical Association","74":"Journal of the American Statistical Association","75":"Journal of the American Statistical Association","76":"Journal of the American Statistical Association","77":"Journal of the American Statistical Association","78":"Journal of the American Statistical Association","79":"Journal of the American Statistical Association","80":"Journal of the American Statistical Association","81":"Journal of the American Statistical Association","82":"Journal of the American Statistical Association","83":"Journal of the American Statistical Association","84":"Journal of the American Statistical Association","85":"Journal of the American Statistical Association","86":"Journal of the American Statistical Association","87":"Journal of the American Statistical Association","88":"Journal of the American Statistical Association","89":"Journal of the American Statistical Association","90":"Journal of the American Statistical Association","91":"Journal of the American Statistical Association","92":"Journal of the American Statistical Association","93":"Journal of the American Statistical Association","94":"Journal of the American Statistical Association","95":"Journal of the American Statistical Association","96":"Journal of the American Statistical Association","97":"Journal of the American Statistical Association","98":"Journal of the American Statistical Association","99":"Journal of the American Statistical Association","100":"Journal of the American Statistical Association","101":"Journal of the American Statistical Association","102":"Journal of the American Statistical Association","103":"Journal of the American Statistical Association","104":"Journal of the American Statistical Association","105":"Journal of the American Statistical Association","106":"Journal of the American Statistical Association","107":"Journal of the American Statistical Association","108":"Journal of the American Statistical Association","109":"Journal of the American Statistical Association","110":"Journal of the American Statistical Association","111":"Journal of the American Statistical Association","112":"Journal of the American Statistical Association","113":"Journal of the American Statistical Association","114":"Journal of the American Statistical Association","115":"Journal of the American Statistical Association","116":"Journal of the American Statistical Association","117":"Journal of the American Statistical Association","118":"Journal of the American Statistical Association","119":"Journal of the American Statistical Association","120":"Journal of the American Statistical Association","121":"Journal of the American Statistical Association","122":"Journal of the American Statistical Association","123":"Journal of the American Statistical Association","124":"Journal of the American Statistical Association","125":"Journal of the American Statistical Association","126":"Journal of the American Statistical Association","127":"Journal of the American Statistical Association","128":"Journal of the American Statistical Association","129":"Journal of the American Statistical Association","130":"Journal of the American Statistical Association","131":"Journal of the American Statistical Association","132":"Journal of the American Statistical Association","133":"Journal of the American Statistical Association","134":"Journal of the American Statistical Association","135":"Journal of the American Statistical Association","136":"Journal of the American Statistical Association","137":"Journal of the American Statistical Association","138":"Journal of the American Statistical Association","139":"Journal of the American Statistical Association","140":"Journal of the American Statistical Association","141":"Journal of the American Statistical Association","142":"Journal of the American Statistical Association","143":"Journal of the American Statistical Association","144":"Journal of the American Statistical Association","145":"Journal of the American Statistical Association","146":"Journal of the American Statistical Association","147":"Journal of the American Statistical Association","148":"Journal of the American Statistical Association","149":"Journal of the American Statistical Association","150":"Journal of the American Statistical Association","151":"Journal of the American Statistical Association","152":"Journal of the American Statistical Association","153":"Journal of the American Statistical Association","154":"Journal of the American Statistical Association","155":"Journal of the American Statistical Association","156":"Journal of the American Statistical Association","157":"Journal of the American Statistical Association","158":"Journal of the American Statistical Association","159":"Journal of the American Statistical Association","160":"Journal of the American Statistical Association","161":"Journal of the American Statistical Association","162":"Journal of the American Statistical Association","163":"Journal of the American Statistical Association","164":"Journal of the American Statistical Association","165":"Journal of the American Statistical Association","166":"Journal of the American Statistical Association","167":"Journal of the American Statistical Association","168":"Journal of the American Statistical Association","169":"Journal of the American Statistical Association","170":"Journal of the American Statistical Association","171":"Journal of the American Statistical Association","172":"Journal of the American Statistical Association","173":"Journal of the American Statistical Association","174":"Journal of the American Statistical Association","175":"Journal of the American Statistical Association","176":"Journal of the American Statistical Association","177":"Journal of the American Statistical Association","178":"Journal of the American Statistical Association","179":"Journal of the American Statistical Association","180":"Journal of the American Statistical Association","181":"Journal of the American Statistical Association","182":"Journal of the American Statistical Association","183":"Journal of the American Statistical Association","184":"Journal of the American Statistical Association","185":"Journal of the American Statistical Association","186":"Journal of the American Statistical Association","187":"Journal of the American Statistical Association","188":"Journal of the American Statistical Association","189":"Journal of the American Statistical Association","190":"Journal of the American Statistical Association","191":"Journal of the American Statistical Association","192":"Journal of the American Statistical Association","193":"Journal of the American Statistical Association","194":"Journal of the American Statistical Association","195":"Journal of the American Statistical Association","196":"Journal of the American Statistical Association","197":"Journal of the American Statistical Association","198":"Journal of the American Statistical Association","199":"Journal of the American Statistical Association","200":"Journal of the American Statistical Association","201":"Journal of the American Statistical Association","202":"Journal of the American Statistical Association","203":"Journal of the American Statistical Association","204":"Journal of the American Statistical Association","205":"Journal of the American Statistical Association","206":"Journal of the American Statistical Association","207":"Journal of the American Statistical Association","208":"Journal of the American Statistical Association","209":"Journal of the American Statistical Association","210":"Journal of the American Statistical Association","211":"Journal of the American Statistical Association","212":"Journal of the American Statistical Association","213":"Journal of the American Statistical Association","214":"Journal of the American Statistical Association","215":"Journal of the American Statistical Association","216":"Journal of the American Statistical Association","217":"Journal of the American Statistical Association","218":"Journal of the American Statistical Association","219":"Journal of the American Statistical Association","220":"Journal of the American Statistical Association","221":"Journal of the American Statistical Association","222":"Journal of the American Statistical Association","223":"Journal of the American Statistical Association","224":"Journal of the American Statistical Association","225":"Journal of the American Statistical Association","226":"Journal of the American Statistical Association","227":"Journal of the American Statistical Association","228":"Journal of the American Statistical Association","229":"Journal of the American Statistical Association","230":"Journal of the American Statistical Association","231":"Journal of the American Statistical Association","232":"Journal of the American Statistical Association","233":"Journal of the American Statistical Association","234":"Journal of the American Statistical Association","235":"Journal of the American Statistical Association","236":"Journal of the American Statistical Association","237":"Journal of the American Statistical Association","238":"Journal of the American Statistical Association","239":"Journal of the American Statistical Association","240":"Journal of the American Statistical Association","241":"Journal of the American Statistical Association","242":"Journal of the American Statistical Association","243":"Journal of the American Statistical Association","244":"Journal of the American Statistical Association","245":"Journal of the American Statistical Association","246":"Journal of the American Statistical Association","247":"Journal of the American Statistical Association","248":"Journal of the American Statistical Association","249":"Journal of the American Statistical Association","250":"Journal of the American Statistical Association","251":"Journal of the American Statistical Association","252":"Journal of the American Statistical Association","253":"Journal of the American Statistical Association","254":"Journal of the American Statistical Association","255":"Journal of the American Statistical Association","256":"Journal of the American Statistical Association","257":"Journal of the American Statistical Association","258":"Journal of the American Statistical Association","259":"Journal of the American Statistical Association","260":"Journal of the American Statistical Association","261":"Journal of the American Statistical Association","262":"Journal of the American Statistical Association","263":"Journal of the American Statistical Association","264":"Journal of the American Statistical Association","265":"Journal of the American Statistical Association","266":"Journal of the American Statistical Association","267":"Journal of the American Statistical Association","268":"Journal of the American Statistical Association","269":"Journal of the American Statistical Association","270":"Journal of the American Statistical Association","271":"Journal of the American Statistical Association","272":"Journal of the American Statistical Association","273":"Journal of the American Statistical Association","274":"Journal of the American Statistical Association","275":"Journal of the American Statistical Association","276":"Journal of the American Statistical Association","277":"Journal of the American Statistical Association","278":"Journal of the American Statistical Association","279":"Journal of the American Statistical Association","280":"Journal of the American Statistical Association","281":"Journal of the American Statistical Association","282":"Journal of the American Statistical Association","283":"Journal of the American Statistical Association","284":"Journal of the American Statistical Association","285":"Journal of the American Statistical Association","286":"Journal of the American Statistical Association","287":"Journal of the American Statistical Association","288":"Journal of the American Statistical Association","289":"Journal of the American Statistical Association","290":"Journal of the American Statistical Association","291":"Journal of the American Statistical Association","292":"Journal of the American Statistical Association","293":"Journal of the American Statistical Association","294":"Journal of the American Statistical Association","295":"Journal of the American Statistical Association","296":"Journal of the American Statistical Association","297":"Journal of the American Statistical Association","298":"Journal of the American Statistical Association","299":"Journal of the American Statistical Association","300":"Journal of the American Statistical Association","301":"Journal of the American Statistical Association","302":"Journal of the American Statistical Association","303":"Journal of the American Statistical Association","304":"Journal of the American Statistical Association","305":"Journal of the American Statistical Association","306":"Journal of the American Statistical Association","307":"Journal of the American Statistical Association","308":"Journal of the American Statistical Association","309":"Journal of the American Statistical Association","310":"Journal of the American Statistical Association","311":"Journal of the American Statistical Association","312":"Journal of the American Statistical Association","313":"Journal of the American Statistical Association","314":"Journal of the American Statistical Association","315":"Journal of the American Statistical Association","316":"Journal of the American Statistical Association","317":"Journal of the American Statistical Association","318":"Journal of the American Statistical Association","319":"Journal of the American Statistical Association","320":"Journal of the American Statistical Association","321":"Journal of the American Statistical Association","322":"Journal of the American Statistical Association","323":"Journal of the American Statistical Association","324":"Journal of the American Statistical Association","325":"Journal of the American Statistical Association","326":"Journal of the American Statistical Association","327":"Journal of the American Statistical Association","328":"Journal of the American Statistical Association","329":"Journal of the American Statistical Association","330":"Journal of the American Statistical Association","331":"Journal of the American Statistical Association","332":"Journal of the American Statistical Association","333":"Journal of the American Statistical Association","334":"Journal of the American Statistical Association","335":"Journal of the American Statistical Association","336":"Journal of the American Statistical Association","337":"Journal of the American Statistical Association","338":"Journal of the American Statistical Association","339":"Journal of the American Statistical Association","340":"Journal of the American Statistical Association","341":"Journal of the American Statistical Association","342":"Journal of the American Statistical Association","343":"Journal of the American Statistical Association","344":"Journal of the American Statistical Association","345":"Journal of the American Statistical Association","346":"Journal of the American Statistical Association","347":"Journal of the American Statistical Association","348":"Journal of the American Statistical Association","349":"Journal of the American Statistical Association","350":"Journal of the American Statistical Association","351":"Journal of the American Statistical Association","352":"Journal of the American Statistical Association","353":"Journal of the American Statistical Association","354":"Journal of the American Statistical Association","355":"Journal of the American Statistical Association","356":"Journal of the American Statistical Association","357":"Journal of the American Statistical Association","358":"Journal of the American Statistical Association","359":"Journal of the American Statistical Association","360":"Journal of the American Statistical Association","361":"Journal of the American Statistical Association","362":"Journal of the American Statistical Association","363":"Journal of the American Statistical Association","364":"Journal of the American Statistical Association","365":"Journal of the American Statistical Association","366":"Journal of the American Statistical Association","367":"Journal of the American Statistical Association","368":"Journal of the American Statistical Association","369":"Journal of the American Statistical Association","370":"Journal of the American Statistical Association","371":"Journal of the American Statistical Association","372":"Journal of the American Statistical Association","373":"Journal of the American Statistical Association","374":"Journal of the American Statistical Association","375":"Journal of the American Statistical Association","376":"Journal of the American Statistical Association","377":"Journal of the American Statistical Association","378":"Journal of the American Statistical Association","379":"Journal of the American Statistical Association","380":"Journal of the American Statistical Association","381":"Journal of the American Statistical Association","382":"Journal of the American Statistical Association","383":"Journal of the American Statistical Association","384":"Journal of the American Statistical Association","385":"Journal of the American Statistical Association","386":"Journal of the American Statistical Association","387":"Journal of the American Statistical Association","388":"Journal of the American Statistical Association","389":"Journal of the American Statistical Association","390":"Journal of the American Statistical Association","391":"Journal of the American Statistical Association","392":"Journal of the American Statistical Association","393":"Journal of the American Statistical Association","394":"Journal of the American Statistical Association","395":"Journal of the American Statistical Association","396":"Journal of the American Statistical Association","397":"Journal of the American Statistical Association","398":"Journal of the American Statistical Association","399":"Journal of the American Statistical Association","400":"Journal of the American Statistical Association","401":"Journal of the American Statistical Association","402":"Journal of the American Statistical Association","403":"Journal of the American Statistical Association","404":"Journal of the American Statistical Association","405":"Journal of the American Statistical Association","406":"Journal of the American Statistical Association","407":"Journal of the American Statistical Association","408":"Journal of the American Statistical Association","409":"Journal of the American Statistical Association","410":"Journal of the American Statistical Association","411":"Journal of the American Statistical Association","412":"Journal of the American Statistical Association","413":"Journal of the American Statistical Association","414":"Journal of the American Statistical Association","415":"Journal of the American Statistical Association","416":"Journal of the American Statistical Association","417":"Journal of the American Statistical Association","418":"Journal of the American Statistical Association","419":"Journal of the American Statistical Association","420":"Journal of the American Statistical Association","421":"Journal of the American Statistical Association","422":"Journal of the American Statistical Association","423":"Journal of the American Statistical Association","424":"Journal of the American Statistical Association","425":"Journal of the American Statistical Association","426":"Journal of the American Statistical Association","427":"Journal of the American Statistical Association","428":"Journal of the American Statistical Association","429":"Journal of the American Statistical Association","430":"Journal of the American Statistical Association","431":"Journal of the American Statistical Association","432":"Journal of the American Statistical Association","433":"Journal of the American Statistical Association","434":"Journal of the American Statistical Association","435":"Journal of the American Statistical Association","436":"Journal of the American Statistical Association","437":"Journal of the American Statistical Association","438":"Journal of the American Statistical Association","439":"Journal of the American Statistical Association","440":"Journal of the American Statistical Association","441":"Journal of the American Statistical Association","442":"Journal of the American Statistical Association","443":"Journal of the American Statistical Association","444":"Journal of the American Statistical Association","445":"Journal of the American Statistical Association","446":"Journal of the American Statistical Association","447":"Journal of the American Statistical Association","448":"Journal of the American Statistical Association","449":"Journal of the American Statistical Association","450":"Journal of the American Statistical Association","451":"Journal of the American Statistical Association","452":"Journal of the American Statistical Association","453":"Journal of the American Statistical Association","454":"Journal of the American Statistical Association","455":"Journal of the American Statistical Association","456":"Journal of the American Statistical Association","457":"Journal of the American Statistical Association","458":"Journal of the American Statistical Association","459":"Journal of the American Statistical Association","460":"Journal of the American Statistical Association","461":"Journal of the American Statistical Association","462":"Journal of the American Statistical Association","463":"Journal of the American Statistical Association","464":"Journal of the American Statistical Association","465":"Journal of the American Statistical Association","466":"Journal of the American Statistical Association","467":"Journal of the American Statistical Association","468":"Journal of the American Statistical Association","469":"Journal of the American Statistical Association","470":"Journal of the American Statistical Association","471":"Journal of the American Statistical Association","472":"Journal of the American Statistical Association","473":"Journal of the American Statistical Association","474":"Journal of the American Statistical Association","475":"Journal of the American Statistical Association","476":"Journal of the American Statistical Association","477":"Journal of the American Statistical Association","478":"Journal of the American Statistical Association","479":"Journal of the American Statistical Association","480":"Journal of the American Statistical Association","481":"Journal of the American Statistical Association","482":"Journal of the American Statistical Association","483":"Journal of the American Statistical Association","484":"Journal of the American Statistical Association","485":"Journal of the American Statistical Association","486":"Journal of the American Statistical Association","487":"Journal of the American Statistical Association","488":"Journal of the American Statistical Association","489":"Journal of the American Statistical Association","490":"Journal of the American Statistical Association","491":"Journal of the American Statistical Association","492":"Journal of the American Statistical Association","493":"Journal of the American Statistical Association","494":"Journal of the American Statistical Association","495":"Journal of the American Statistical Association","496":"Journal of the American Statistical Association","497":"Journal of the American Statistical Association","498":"Journal of the American Statistical Association","499":"Journal of the American Statistical Association","500":"Journal of the American Statistical Association","501":"Journal of the American Statistical Association","502":"Journal of the American Statistical Association","503":"Journal of the American Statistical Association","504":"Journal of the American Statistical Association","505":"Journal of the American Statistical Association","506":"Journal of the American Statistical Association","507":"Journal of the American Statistical Association","508":"Journal of the American Statistical Association","509":"Journal of the American Statistical Association","510":"Journal of the American Statistical Association","511":"Journal of the American Statistical Association","512":"Journal of the American Statistical Association","513":"Journal of the American Statistical Association","514":"Journal of the American Statistical Association","515":"Journal of the American Statistical Association","516":"Journal of the American Statistical Association","517":"Journal of the American Statistical Association","518":"Journal of the American Statistical Association","519":"Journal of the American Statistical Association","520":"Journal of the American Statistical Association","521":"Journal of the American Statistical Association","522":"Journal of the American Statistical Association","523":"Journal of the American Statistical Association","524":"Journal of the American Statistical Association","525":"Journal of the American Statistical Association","526":"Journal of the American Statistical Association","527":"Journal of the American Statistical Association","528":"Journal of the American Statistical Association","529":"Journal of the American Statistical Association","530":"Journal of the American Statistical Association","531":"Journal of the American Statistical Association","532":"Journal of the American Statistical Association","533":"Journal of the American Statistical Association","534":"Journal of the American Statistical Association","535":"Journal of the American Statistical Association","536":"Journal of the American Statistical Association","537":"Journal of the American Statistical Association","538":"Journal of the American Statistical Association","539":"Journal of the American Statistical Association","540":"Journal of the American Statistical Association","541":"Journal of the American Statistical Association","542":"Journal of the American Statistical Association","543":"Journal of the American Statistical Association","544":"Journal of the American Statistical Association","545":"Journal of the American Statistical Association","546":"Journal of the American Statistical Association","547":"Journal of the American Statistical Association","548":"Journal of the American Statistical Association","549":"Journal of the American Statistical Association","550":"Journal of the American Statistical Association","551":"Journal of the American Statistical Association","552":"Journal of the American Statistical Association","553":"Journal of the American Statistical Association","554":"Journal of the American Statistical Association","555":"Journal of the American Statistical Association","556":"Journal of the American Statistical Association","557":"Journal of the American Statistical Association","558":"Journal of the American Statistical Association","559":"Journal of the American Statistical Association","560":"Journal of the American Statistical Association","561":"Journal of the American Statistical Association","562":"Journal of the American Statistical Association","563":"Journal of the American Statistical Association","564":"Journal of the American Statistical Association","565":"Journal of the American Statistical Association","566":"Journal of the American Statistical Association","567":"Journal of the American Statistical Association","568":"Journal of the American Statistical Association","569":"Journal of the American Statistical Association","570":"Journal of the American Statistical Association","571":"Journal of the American Statistical Association","572":"Journal of the American Statistical Association","573":"Journal of the American Statistical Association","574":"Journal of the American Statistical Association","575":"Journal of the American Statistical Association","576":"Journal of the American Statistical Association","577":"Journal of the American Statistical Association","578":"Journal of the American Statistical Association","579":"Journal of the American Statistical Association","580":"Journal of the American Statistical Association","581":"Journal of the American Statistical Association","582":"Journal of the American Statistical Association","583":"Journal of the American Statistical Association","584":"Journal of the American Statistical Association","585":"Journal of the American Statistical Association","586":"Journal of the American Statistical Association","587":"Journal of the American Statistical Association","588":"Journal of the American Statistical Association","589":"Journal of the American Statistical Association","590":"Journal of the American Statistical Association","591":"Journal of the American Statistical Association","592":"Journal of the American Statistical Association","593":"Journal of the American Statistical Association","594":"Journal of the American Statistical Association","595":"Journal of the American Statistical Association","596":"Journal of the American Statistical Association","597":"Journal of the American Statistical Association","598":"Journal of the American Statistical Association","599":"Journal of the American Statistical Association","600":"Journal of the American Statistical Association","601":"Journal of the American Statistical Association","602":"Journal of the American Statistical Association","603":"Journal of the American Statistical Association","604":"Journal of the American Statistical Association","605":"Journal of the American Statistical Association","606":"Journal of the American Statistical Association","607":"Journal of the American Statistical Association","608":"Journal of the American Statistical Association","609":"Journal of the American Statistical Association","610":"Journal of the American Statistical Association","611":"Journal of the American Statistical Association","612":"Journal of the American Statistical Association","613":"Journal of the American Statistical Association","614":"Journal of the American Statistical Association","615":"Journal of the American Statistical Association","616":"Journal of the American Statistical Association","617":"Journal of the American Statistical Association","618":"Journal of the American Statistical Association","619":"Journal of the American Statistical Association","620":"Journal of the American Statistical Association","621":"Journal of the American Statistical Association","622":"Journal of the American Statistical Association","623":"Journal of the American Statistical Association","624":"Journal of the American Statistical Association","625":"Journal of the American Statistical Association","626":"Journal of the American Statistical Association","627":"Journal of the American Statistical Association","628":"Journal of the American Statistical Association","629":"Journal of the American Statistical Association","630":"Journal of the American Statistical Association","631":"Journal of the American Statistical Association","632":"Journal of the American Statistical Association","633":"Journal of the American Statistical Association","634":"Journal of the American Statistical Association","635":"Journal of the American Statistical Association","636":"Journal of the American Statistical Association","637":"Journal of the American Statistical Association","638":"Journal of the American Statistical Association","639":"Journal of the American Statistical Association","640":"Journal of the American Statistical Association","641":"Journal of the American Statistical Association","642":"Journal of the American Statistical Association","643":"Journal of the American Statistical Association","644":"Journal of the American Statistical Association","645":"Journal of the American Statistical Association","646":"Journal of the American Statistical Association","647":"Journal of the American Statistical Association","648":"Journal of the American Statistical Association","649":"Journal of the American Statistical Association","650":"Journal of the American Statistical Association","651":"Journal of the American Statistical Association","652":"Journal of the American Statistical Association","653":"Journal of the American Statistical Association","654":"Journal of the American Statistical Association","655":"Journal of the American Statistical Association","656":"Journal of the American Statistical Association","657":"Journal of the American Statistical Association","658":"Journal of the American Statistical Association","659":"Journal of the American Statistical Association","660":"Journal of the American Statistical Association","661":"Journal of the American Statistical Association","662":"Journal of the American Statistical Association","663":"Journal of the American Statistical Association","664":"Journal of the American Statistical Association","665":"Journal of the American Statistical Association","666":"Journal of the American Statistical Association","667":"Journal of the American Statistical Association","668":"Journal of the American Statistical Association","669":"Journal of the American Statistical Association","670":"Journal of the American Statistical Association","671":"Journal of the American Statistical Association","672":"Journal of the American Statistical Association","673":"Journal of the American Statistical Association","674":"Journal of the American Statistical Association","675":"Journal of the American Statistical Association","676":"Journal of the American Statistical Association","677":"Journal of the American Statistical Association","678":"Journal of the American Statistical Association","679":"Journal of the American Statistical Association","680":"Journal of the American Statistical Association","681":"Journal of the American Statistical Association","682":"Journal of the American Statistical Association","683":"Journal of the American Statistical Association","684":"Journal of the American Statistical Association","685":"Journal of the American Statistical Association","686":"Journal of the American Statistical Association","687":"Journal of the American Statistical Association","688":"Journal of the American Statistical Association","689":"Journal of the American Statistical Association","690":"Journal of the American Statistical Association","691":"Journal of the American Statistical Association","692":"Journal of the American Statistical Association","693":"Journal of the American Statistical Association","694":"Journal of the American Statistical Association","695":"Journal of the American Statistical Association","696":"Journal of the American Statistical Association","697":"Journal of the American Statistical Association","698":"Journal of the American Statistical Association","699":"Journal of the American Statistical Association","700":"Journal of the American Statistical Association","701":"Journal of the American Statistical Association","702":"Journal of the American Statistical Association","703":"Journal of the American Statistical Association","704":"Journal of the American Statistical Association","705":"Journal of the American Statistical Association","706":"Journal of the American Statistical Association","707":"Journal of the American Statistical Association","708":"Journal of the American Statistical Association","709":"Journal of the American Statistical Association","710":"Journal of the American Statistical Association","711":"Journal of the American Statistical Association","712":"Journal of the American Statistical Association","713":"Journal of the American Statistical Association","714":"Journal of the American Statistical Association","715":"Journal of the American Statistical Association","716":"Journal of the American Statistical Association","717":"Journal of the American Statistical Association","718":"Journal of the American Statistical Association","719":"Journal of the American Statistical Association","720":"Journal of the American Statistical Association","721":"Journal of the American Statistical Association","722":"Journal of the American Statistical Association","723":"Journal of the American Statistical Association","724":"Journal of the American Statistical Association","725":"Journal of the American Statistical Association","726":"Journal of the American Statistical Association","727":"Journal of the American Statistical Association","728":"Journal of the American Statistical Association","729":"Journal of the American Statistical Association","730":"Journal of the American Statistical Association","731":"Journal of the American Statistical Association","732":"Journal of the American Statistical Association","733":"Journal of the American Statistical Association","734":"Journal of the American Statistical Association","735":"Journal of the American Statistical Association","736":"Journal of the American Statistical Association","737":"Journal of the American Statistical Association","738":"Journal of the American Statistical Association","739":"Journal of the American Statistical Association","740":"Journal of the American Statistical Association","741":"Journal of the American Statistical Association","742":"Journal of the American Statistical Association","743":"Journal of the American Statistical Association","744":"Journal of the American Statistical Association","745":"Journal of the American Statistical Association","746":"Journal of the American Statistical Association","747":"Journal of the American Statistical Association","748":"Journal of the American Statistical Association","749":"Journal of the American Statistical Association","750":"Journal of the American Statistical Association","751":"Journal of the American Statistical Association","752":"Journal of the American Statistical Association"},"keyword":{"0":"Cell dependenceHurdle modelPoison-LogNormal distributionZero-inflation","1":"composite null hypothesisintersection-union testjoint significancemediation analysis","2":"electroencephalographyfunctional data analysisfunctional regressionmajor depressive disordermissing datamultiple imputation","3":"Brain ImagingInternal VariationPiecewise SmoothnessTensor RegressionVerbal Reasoning","4":"Bayesian ModelingChemical MixturesCorrelated ExposuresQuadratic regressionStatistical Interactions","5":"CopulaDensity deconvolutionMeasurement errorNutritional epidemiologyZero inflated data","6":null,"7":"Clinical trialsMachine learningMental disordersPersonalized medicineRestricted Boltzmann machine (RBM)","8":"Big DataDimension ReductionGenomicsMultivariate Analysis","9":"Auditory category\/tone learningDrift-diffusion modelsInverse Gaussian distributionsLocal clusteringLongitudinal mixed modelsPerceptual decision making","10":"Bivariate survival functionComposite outcomeCross ratioEmpirical processHazard ratesMarginal modelingMultivariate failure times","11":"BiomarkersDeep learning autoencodersMachine learningOutcome weighted learningPrecision medicineQ-learning","12":"Efficient InferenceParameter of InterestParameter of NuisanceProfile LikelihoodSelective InferenceSparsity","13":"Confidence intervalGeneralized linear modelsOnline estimationUltrahigh dimensions","14":"Mediationinstrumental variablestargeted minimum loss-based estimation","15":"Generalizabilitycovariate shiftdensity-ratio estimationefficient score","16":"Genome wide association studyLassogenetic pleiotropygenetic risk predictionpolygenic risk scoresummary statistics","17":"False discovery rateGlobal testingLarge-scale multiple testingMinimax lower bound","18":"Direct effectIndirect effectInterferenceNetworkSpillover effect","19":"Gene expressionLatent space modelsMatrix factorizationOligodendrocytesRandom dot product model","20":"Age-related macular degenerationAssociation studyComplex diseasesExtended pedigreeGeneralized functional linear mixed modelsRare variants","21":null,"22":"Brain networksDirichlet processStroop taskmultiple graphical modelsspike and slab prior","23":"Electronic health recordsInformative observationKernel smoothingProportional rate modelRecurrent event analysis","24":"Bayesian variable selectionMulti-layered Gaussian graphical modelsMulti-level data integration","25":"contextual banditsendogenous variablesgeneralized linear mixed modelindividualized decision rulepush notifications","26":"","27":"Covariate shiftsDistributionally robust optimizationGeneralizabilityIndividualized treatment rules","28":"Composite likelihoodDivide-and-conquerGeneralized method of momentsParallel computingScalable computing","29":"Cross-fittingData-adaptive techniquesDynamic treatment strategiesResidual confounding","30":"Individualized decision makingIndividualized treatment regimesMachine intelligencePartial identificationPolicy makingUnmeasured confounding","31":"Double robustnessIndividualized treatment rulePersonalized medicinePropensity score","32":"Complier optimal regimesInstrumental variableOptimal treatment regimesPrecision medicineUnmeasured confounding","33":"markov decision processpolicy evaluationreinforcement learningsequential decision making","34":"Conditional Average Treatment EffectDoubly Robust EstimationHeterogeneous Treatment EffectObservational StudyPrecision Medicine","35":"epsilon-greedyinverse propensity weighted estimatormodel misspecificationonline decision-makingstatistical inference","36":"individualized treatmentlimited resourcesunmeasured confounders","37":"Bootstrap covariance testMicroarrayMultiple testing with dependence","38":"Bayesian approachImaging dataInstrumental variableMarkov chain Monte CarloNonignorable nonresponse","39":"Dirichlet process mixture priorsHeterogeneous treatment effectsHospitalization chargesPCIRadial versus femoral arterial accessSelection bias","40":"Cumulative divergencefeature screeningforward screeninghigh dimensionalitysure screening propertyvariable selection","41":"1990 Clean Air Act AmendmentsBayesian Additive Regression TreesCounterfactual Pollution ExposuresMatching","42":"Imaging GeneticsLow RankMatrix Linear RegressionSpectral normTrace norm","43":"Excess zerosMutational signatureRosenblatt-Parzen kernel estimatorTest of independence","44":"AlgorithmsBayesian methodsCategorical data analysisSimulation","45":"Bayesian methodscausal effectmeta-analysismissing datarandomized trial","46":"Compositional dataDifferential abundance analysisHigh dimensionMicrobiome regressionZero-inflated data","47":"High-dimensional testingOptimal treatment regimePrecision medicineQualitative treatment effectsSparse random projection","48":"approximate factor modelcanonical variablecommon structuredistinctive structuresoft thresholding","49":null,"50":"confidence intervalnon-convex penaltypenalized regressionrandom designvariational inequality","51":"Adaptive Huber regressionbias and robustness tradeofffinite-sample inferenceheavy-tailed datanonasymptotic optimalityphase transition","52":"CalibrationCausal inferenceInverse probability weightingMissing confounderTwo-phase sampling","53":"","54":"Bayesian smoothingbrain imagingintegrated nested Laplace approximationspatial statisticsstochastic partial differential equation","55":"Breast cancerFGFR2 geneGene-level testGeneralized higher criticismSparse alternative","56":"Bayesian density estimationBayesian logistic regressionGradient ascent algorithmInfinite-dimensional Riemannian optimizationSquare-root density","57":"Individualized treatment rulesMachine learningMatchingObservational studiesPersonalized medicine","58":"coalescent theorydemographic inferencefrequency spectrumpopulation genetics","59":"Generalized linear mixed modelsRNA-seqmicroarraypenalized likelihoodprediction","60":"Dirichlet process priordynamic treatment regimesobservational dataperiodontal diseasepractice-based settingprecision medicinesequential optimization","61":"Bulk ExpressionDeconvolutionImmuno-OncologyImmunotherapyMicroarrayRNA-seq","62":"Cauchy distributionCorrelation matrixGlobal hypothesis testingHigh dimensional dataNon-asymptotic approximationSparse alternative","63":"Dimension reductionMeasurement errorRespiratory syncytial virusSpatial statistics","64":"BiasConfoundingMeta-analysisObservational studiesSensitivity analysis","65":"Basis FunctionsBayesian ModelingFunctional RegressionImaging GeneticsMarkov chain Monte CarloProbability Density Function","66":"Directed acyclic graphL0-regularizationgene networkhigh-dimensional inferencenonconvex minimization","67":"Confidence intervalEmpirical processL1 methodLinear programmingRank estimationResamplingU-processWilcoxon-Mann-Whitney Statistic","68":"Markov decision processesPrecision medicineReinforcement learningType 1 diabetes","69":"food frequency questionnairelatent class analysislocal partition processnutritional epidemiology","70":"(p, n)-asymptoticsBrain networksGeneralized Wilks phenomenonHigh-dimensionalityL0-regularizationSimilar tests","71":"Allele Specific ExpressionRNA-SeqTumor PurityeQTL","72":"Big dataGraphical nonlinear knockoffsHigh-dimensional nonlinear modelsLarge-scale inference and FDRPowerReproducibilityRobustness","73":null,"74":"Decision theoryResearch prioritizationUncertainty","75":"Model errorclusteringmodel misspecificationpower likelihoodrelative entropytempering","76":"Bayesian ModelingFoot and Mouth DiseaseHandInfectious DiseasesMissing Data","77":"Dimension reductionFalse negative controlFalse positive controlUltrahigh dimensionVariable screening","78":"Hard thresholdQ-learningValue function inferenceVariable selection","79":"Association testsImputationIntegrative analysisMultiple genomics platformsSemiparametric modelsSieve estimation","80":null,"81":"Functional dataPoint processesSimulated annealingStochastic curve registration","82":"Brain connectivity analysisFused lassoGeneralized linear mixed-effect modelStochastic blockmodelTime-varying network","83":"Modified matrixMulti-armed treatmentsMultivariate responses regressionPersonalized medicine","84":"Asymptotic optimalityFrequentist model averagingJackknife model averagingMallows model averagingParsimony","85":"Bayesian predictionFactor analysisGibbs samplingHierarchical probit modelJoint modelingKidney obstructionLatent class model","86":"Frequency band estimationHeart rate variabilityLocally stationarySpectrum analysis","87":"Cox modelEM algorithmSocial network dependenceTime-to-event data","88":"Extreme value distributionFalse discovery controlMinimax rate optimalityMultimodal integrative analysisMultiple testingPositron emission tomography","89":"Case-cohort designCase-control studyGeneralized linear modelsOutcome-dependent samplingProportional hazardsSemiparametric efficiency","90":"AUCcross-validationestimating equationsmachine learningpredictiontargeted minimum loss-based estimation","91":"Large-scale inference and FDRLatent factorsModel-X knockoffsPowerReproducibilityStability","92":"Bayesian shrinkageadditive modelnonparametric regressionposterior contraction","93":"Gaussian processGibbs posteriorKullback-Leibler divergenceM-openModel aggregationModel selectionNonparametric BayesPosterior probabilities","94":"Google Street View Air Quality DataKrigingMobile sensorsSpatiotemporal modelsVecchia approximation","95":"Bayesian hierarchical modelDiagnostic testsMissing dataMultiple tests comparisonNetwork meta-analysis","96":"Causal inferenceInstrumental variables estimationInvalid instrumentsLassoMendelian randomization.","97":"Ball CorrelationRankSure IndependenceVariable Screening","98":"Areal womblingBayesian methodsConditional autoregressive modelsDissimilarity metric","99":"HIVcompeting risksdependent censoringmachine learningmalariatargeted minimum loss-based estimationvaccine","100":"Markov chain Monte Carloelastic metriclandmarkslinear reconstructionshape analysis","101":"Dynamic predictionJoint modelsNonparametric likelihoodProportional hazardsRandom effectsSemiparametric efficiency","102":"Dimension reductionEigen-analysisGenome-wide Association Studies (GWAS)Omnibus testPower analysisPrincipal angleSummary statistics","103":"causal inferenceg-formulalongitudinal datamarginal structural modelssurvival analysis","104":"Li-Fraumeni syndromecancer specific age-at-onset penetrancecompeting riskfamily-wise likelihoodgamma frailty model","105":"Bayesian LassoPosterior consistencyShrinkage priorVector Autoregressive Models","106":"Causal InferenceHospital ProfilingIterative Proportional FitSurvey SamplingSynthetic Control","107":"BootstrapConstrained estimationFunction-on-scalar regressionNonparametricSplines","108":"ALSPACConditional Markov random fieldGeneticsLow-Rank plus SparseMetabolitesModel SelectionMultivariate analysis","109":"CUDAempirical Bayesgraphics processing unithierarchical modelhybrid vigornegative-binomial","110":"Bayesian spectral analysisChange-pointsReversible-jump MCMCSleep apneaUltradian sleep cycles","111":"Bayesian modelsFunctional data analysisFunctional mixed modelsFunctional regressionGlaucomaLongitudinal Functional DataNonparametric effectsSmoothing SplinesSpherical dataWavelets","112":"Asymptotic MinimaxFalse Discovery RateGroup selectionModel SelectionMultiple RegressionSLOPE","113":"","114":"Prognostic biomarkerp-splinesprecision medicinethresholdtumor heterogeneity","115":"LikelihoodLongitudinal dataNonlinear dynamicsParticle filterSequential Monte Carlo","116":"Locally Stationary ProcessModified Cholesky DecompositionNonstationary Multivariate Time SeriesPenalized SplinesReversible Jump Markov Chain Monte CarloSpectral Analysis","117":"Berk-Jones testGenome-wide association studyHigh dimensionalityHigher criticismMonte Carlo method","118":"Fine-Gray modelcumulative incidence functionnonparametric maximum likelihood estimationsemiparametric transformation modelstime-varying covariates","119":"Covariance functionFunctional structural equation modelGenetic and environmental effectsWeighted likelihood ratio test","120":"","121":"Function-on-Function regressionMinimax convergence ratePenalized least squaresRepresenter TheoremReproducing kernel Hilbert space","122":"Precision medicinedecision listsinterpretabilityresearch-practice gaptreatment regimestree-based methods","123":"Bayes factorRandom projectionRestricted most powerful Bayesian testsTesting of hypotheses","124":"Cubic rate asymptoticsM-estimatorsdivide and conquermassive data","125":"Approximate Dynamic ProgrammingBackward InductionBayesian Additive Regression TreesGibbs SamplingPotential Outcomes","126":"","127":"Complex systemEigenvalue updating algorithmHigh DimensionMatrix-based variable selectionOrdinary differential equationSeparable least squares","128":"","129":"High-dimensional high-order dataProjection and thresholdingSingular value decompositionSparsityTucker low-rank tensor","130":"g-formulamarginal structural modelspotential outcomesstructural nested models","131":null,"132":"Consensus ClusteringEEGHierarchical Mixture ModelsSpectral Clustering","133":"Factor adjustmentFalse discovery proportionHuber lossLarge-scale multiple testingRobustness","134":"","135":null,"136":"ClassificationForward screeningHigh-dimensionalQuadriatic discriminant analysisSemi-parametricStepwise selection","137":"asymptotic efficiencycanonical gradientefficient influence functionnonparametric and semiparametric modelspathwise differentiability","138":"Brain connectomicsConnectome geometryFunctional data analysisMixture modelShape analysis","139":"Majorization-minimizationNonconvexity, PU-learningRegularization","140":"Case-cohort samplingNegative predictive valueNested case-control samplingNonparametric maximum likelihood estimatorPositive predictive valueReceiver Operating Characteristics Curve (ROC curve)Two-phase study","141":"false discovery ratemultiple hypothesis testingtandem mass spectrometry","142":"edge exchangeabilityedge-labeled networkexchangeable random graphinteraction datapower law distributionscale-free networksparse network","143":"effect modificationmHealthstructural nested mean model","144":"dynamic treatment regimenonstandard asymptoticsoptimal treatment regimeprecision medicinequantile criterion","145":"Functional DataKinematic DataMotor ControlProbabilistic PCAVariance ModelingVariational Bayes","146":"p-valuescaled Bayes factorweighted sum of chi-squared random variables","147":"Healthcare UtilizationLatent Variable ModelMedicaid systemPediatric AsthmaProportional Hazards ModelSurvival Analysis","148":"goodness-of-fitlogistic odds modelmodel diagnosticsprobit model","149":"ClusteringMultivariate mixture modelPenalized estimationProteomicsRobust estimation","150":"","151":"Carrier Status PredictionFamily HistoryMismeasured CovariatesSmoothed Kaplan-Meier EstimatorSurvival Analysis","152":"Integrative analysisJoint modelingLatent variablesModel identifiabilityNonparametric maximum likelihood estimationSurvival analysis","153":"A-optimalityLogistic RegressionMassive DataOptimal SubsamplingRare Event","154":"non-regular inferencestabilized one-step estimatorvariable screening","155":"B-splinesConditional heteroscedasticityLatent factor analyzersMeasurement errorsMixture modelsMultivariate density deconvolutionRegularizationShrinkage","156":"Convex orderCyber-securityMeta-analysisNetworkp-Value","157":"BayesInverse probability weightingMissing at random","158":"Feature screeningRefitted cross-validationSparse additive modelVariance estimation","159":"Benefit Risk AnalysisHypoglycemiaMachine LearningNeyman-Pearson LemmaPersonalized Medicine","160":"Bayesianclusteringdensity estimationmodel selectionnonparametric","161":"Asymptotic normalityAuxiliary dataDivide-and-conquerFactor modelFisher informationHigh-dimensionality","162":"Between pathway interactionsGaussian graphical modelconditional dependencecovariance structurefalse discovery proportionfalse discovery ratemultiple testingprecision matrixtesting submatrices","163":"Biased samplingEM algorithmGenome sequencingResponseselective samplingSemiparametric efficiencySieve approximation","164":"","165":"BayesianDiscrete state spacesMarkov chain Monte Carlo","166":"Bayesian factor analysisDependent Dirichlet processesMicrobiome data analysisUncertainty of ordination","167":"Bayesian AnalysisCoherenceHeart Rate VariabilityMCMCMultivariate Time SeriesSleepSmoothing SplineSpectral AnalysisTensor-Product ANOVAWhittle Likelihood","168":"Adaptive interpolationAdditive complementary log-log survival modelAdditive hazards modelCensored quantile regressionMonotone functionQuantile regression","169":"Errors in covariatesLoss efficiencyMeasurement errorModel selectionSelection consistency","170":"AsymptoticsConditional likelihoodNon-regular problemPenalized likelihoodSemiparametric mixture model","171":"GWASHigher criticismIntegrative genomicseQTL","172":"","173":null,"174":"Bayesian adaptive designImmunotherapydose findingimmune responsephase I\/II trialrisk-benefit tradeoff","175":null,"176":"Average treatment effectCausal graphical modelInstrumentInstrumental variablePartial identificationSingle world intervention graph","177":"Copula functionMismeasured continuous responseMultivariate survival dataPopulation-averaged modeling","178":"","179":"Cancer Cell Line EncyclopediaNonlinear Variable SelectionOmics DataOpenMPParallel Markov Chain Monte Carlo","180":"AssociationFalse discovery rateGenomicsHigh-throughput experimentIrreproducible discovery rate","181":"InsomniaMixture ModelResearch Domain CriteriaSkewed Data","182":"Class weightsDecision weightsMultiple testing with groupsPrioritized subsetsValue to cost ratioWeighted p-value.","183":"Plasmodium falciparumassay normalizationnegative binomialstandardizationzero inflation","184":"Autoregressive processCircadian patternLongitudinal count dataPhysical activityRandom effectSerial correlationShrinkage priorStick-breaking prior","185":null,"186":"","187":"Bayesian MethodsComputationally Intensive MethodsGibbs samplingfinite precision","188":"Wasserstein distancebatch effectpool adjacent violators algorithmquantile regressionsingle cell RNA-seq","189":"Concentration inequalityHuber losslow-rank matrixpre-averagingspasrity","190":"Copula modelFamily historyMarginal hazard functionMultivariate survival analysis","191":"Bivariate SmoothingJoint ModelingLifetime and Survival AnalysisMedical CostSEER Medicare","192":"Cox regressionEmpirical processMixed effects modelPseudo-maximum likelihood estimation","193":"","194":"hidden populationinjection drug usenetwork inferencepopulation size","195":"Bayesian nonparametricsChinese restaurant processCluster analysisNonexchangeable priorProduct partition model","196":"Convergence rateDifferentiable manifoldGeometryLocal regressionObject dataShape statistics","197":"Functional data analysisImage data analysisSingle index modelVarying-coefficient model","198":"Excess riskFunctional regressionGeneralized scalar-on-image regressionPredictionTotal variation","199":"AlzheimerFeaturePrincipal component analysisRegressionSpatialSupervised","200":"Competing risksCumulative incidence functionMarked recurrent eventsNonparametric estimation","201":"","202":"Accelerated failure time modelFrailtyInformative censoringMarginal modelsSemiparametric methods","203":"Quantile correlationQuantile partial correlationScreeningVariable selection","204":"Convergence RateMartingale residualsOptimal Treatment RegimeRKHSResidualsUniversal consistency","205":"Bayesian AnalysisGroup SequentialPiecewise Exponential ModelRadiation OncologyRandomized Comparative TrialUtility Elicitation","206":"","207":"Dynamic Treatment RegimePersonalized MedicineSequential Decision MakingSequential Multiple Assignment Randomized Trial","208":"Change-plane analysisDoubly robust testSample size calculationSemiparametric modelSubgroup analysis","209":"Correlated test statisticsDetection boundaryGenetic association testingHigher criticismMultiple hypothesis testingSignal detection","210":"Marked Counting ProcessPartial LikelihoodRecurrent Event ProcessSemi-parametric models","211":"case-control studyetiologic heterogeneityk-means clusteringlogistic regressionmultiple imputation","212":"Directed acyclic graphsLasso estimationNeighborhood selectionProbabilistic graphical modelStructure equation model","213":"activationconnectivityfMRImulti-subject","214":"Multi-class Linear Discriminant AnalysisPairwise Sure Independence ScreeningStrong Screening ConsistencySure Independence Screening","215":"Corpus CallosumPrincipal Curves and SurfacesThin Plate Splines","216":"High-dimensionaladaptivehierarchical group lassostructured sparsity","217":"Constrained minimizationgenomic data integrationlow-rank matrixmatrix completionsingular value decompositionstructured matrix completion","218":"","219":"Bayesian adaptive designdose findingpartial least squarespersonalized dose findingpersonalized medicine","220":"Human immunodeficiency virusMediationNonparametric identificationUnobserved confounding","221":"","222":"","223":"L0 penalizationcancer genomic dataintegrative analysissparsity structurevariable selection","224":"Case-cohort samplingCensored quantile regressionLength-biased dataResamplingStratified case-cohort samplingSurvival time","225":"Bayesian Model AveragingMCMCModel SelectionNon Local PriorsShrinkage","226":"Bayes factorNull hypothesis significance testPosterior model probabilityPublication biasReproducibilitySignificance test","227":"Gene-environment independenceGeneralized score testMESA neighborhood studyModel misspecificationRobustness","228":"Kernel smoothingLongitudinal marker processMultiple event processSurvival analysis","229":"LASSONadaraya-Watsonbandwidth selectionfeature selectionnonparametric regressionsolution path","230":"Additive modelGroup lassoHigh dimensionalityOrdinary differential equationVariable selection consistency","231":"Spot volatilityco-jumpshigh-frequency dataintegrated volatilityjumpsmarket microstructure noise","232":"Asymptotic theoryB-splinesCombining data setsHealthy Eating IndexLogistic regressionPartially linear single-index modelsSemiparametric modelsSingle-index models","233":"Dependent Dirichlet processG-ComputationGaussian processIn-verse probability of treatment weightingMarkov chain Monte Carlo","234":"Active learningClinical trialIndividualized treatment rulePersonalized medicineRisk bound","235":"Conditional autoregressive modelFunctional data analysisFunctional regressionSpatial functional dataWhole-organ histology and genetic maps","236":"","237":"Dynamic treatment regimesMulti-stage chemotherapy regimesO-learningQ-learning","238":"","239":"","240":"Information synthesisMeta-analysisSEER cancer registriesSubgroup analysis","241":"Decision-makingDynamic synchronyGaussian processesSpike trains","242":"","243":"Bayesian Factor ModelsConditional Autoregressive ModelsConfirmatory Factor AnalysisDrug Target PredictionMCMCMicroarrayNetwork Biology","244":"PCASVDfunctional data analysisimage analysissingular value decomposition","245":"Case-control studyEmpirical likelihoodGeneralized regression estimatorMisspecified modelProfile-likelihood","246":"Gene regulatory networkHigh-dimensional statisticsPrecision matrix estimationSparsitySupport recoveryeQTL","247":"Disease subtype discoveryK-meansLassoMeta-analysisUnsupervised machine learning","248":"","249":"Dynamic programmingHidden Markov modelsSegmentation.","250":"Compartmental modelingFunctional response modelKinetic modelingNeuroimaging.","251":"Asymptotic efficiencyOracle inequalitiesSolution pathWeak oracle property","252":"array CGHasymptotic consistencychange-pointschoice of priordynamic programmingmarginal likelihoodmodel selectionsingle-molecule experiments","253":"accelerated failure time modelaccelerated recurrence time modelcensored quantile regressioncounting processrecurrent eventsvarying covariate effects","254":"classificationdensity estimationfeature augmentationfeature selectionhigh dimensional spacenonlinear decision boundaryparallel computing","255":"BayesianConvergence rateHigh dimensionalL1LassoPenalized regressionRegularizationShrinkage prior","256":"","257":"Central subspaceDirect sum envelopeGroupwise dimension reductionMultiple-index modelsSliced inverse regression","258":"ConsistencyError BoundRandom ForestsReinforcement LearningTrees","259":"Conditional distance correlationConditional independence testLocal bootstrapU(V) process with random kernel","260":"asymptotic approximationdependent random variablesempirical nullfactor analysishigh dimensional datastrong correlation","261":"","262":"","263":"Asymptotic relative efficiencyHigh dimensional multivariate dataHotelling T2 testNonparametric multivariate test","264":"hypothesis testjoint modellatent variable modelpredictionrelational data","265":"Area under the curveDirichlet process mixtureGold standardOrder restricted analysis","266":"convex optimizationdeflationdomain selectionfunctional principal component analysisinterpretabilityorthogonality","267":"backcross experimentsconditional samplingconsistencyconvergencemissing data mechanismmodel selectionregression","268":"","269":"ClassificationConvergence rateNonparametric BayesTensor factorizationUltra high-dimensionalVariable selection","270":null,"271":"GWAScase-control studiesestimating equationsquantile regressionsecondary phenotype","272":"Bayesian modelingGaussian processHierarchical modelsMarkov chain Monte CarloNearest neighborsPredictive processReduced-rank modelsSparse precision matricesSpatial cross-covariance functions","273":"GraphLassoModel selectionPredictionSparse regression","274":"Informative missingnessLongitudinal dataMixed dataNon-future dependencePattern mixture modelSensitivityShrinkage","275":"Alternating direction method of multipliersCholesky decompositionCopula modelOptimal transformationPrediction intervalQuantile regressionRank correlation","276":"FRETHMM (hidden Markov model)MCMC (Markov Chain Monte Carlo)Protein targetingconformational changehierarchical modelmodel checking","277":"Double robustnessEpanechnikov kernelPersonalized treatment","278":"Hierarchical Feature selectionImputationMissing dataRegularizationSemiparametric AFT model","279":"Bayesian NonparametricCovariance EstimationDirichlet Process MixtureGaussian processMixed ModelOrnstein-Uhlenbeck ProcessStudy of Women Across the Nation (SWAN)","280":"False selection rateGeneralized linear modelsSparsitySure screeningVariable selection","281":"Asymptotic efficiencyChange-pointHormone profilePiecewise linear model","282":"Bayesian survival analysiscluster-correlated dataillness-death modelsreversible jump Markov chain Monte Carlosemi-competing risksshared frailty","283":"","284":"DC AlgorithmDose FindingIndividualized Dose RuleRisk BoundWeighted Support Vector Regression","285":"Goodness-of-matchLASSOOrdinary least-squares estimationPortfolio trackingRepresentative portfolioSample quantile","286":"Functional data analysisLinguisticsMultivariate linear mixed modelsPhonetic analysisRegistration","287":"RNA-seqdifferential isoform expressiondifferential isoform usageisoformpenalized regression","288":"Alternating direction method of multipliersAttention deficit hyperactivity disorderCorpus callosumOffset-normal shape distributionShape clustering","289":"Convergence ratesCox modelKernel weighted estimationSparse longitudinal covariates","290":"ASEGene expressionHaplotypeMaximum likelihoodeQTL study","291":"Exponential familiesLocal dependenceM-dependenceModel degeneracySocial networksWeak dependence","292":"","293":"heritability ratioimaging geneticsmultivariate regressionprojection regressionsparsewild bootstrap","294":"Additive conditional independenceadditive precision operatorconditional independencecopulacovariance operatorgaussian graphical modelnonparanormal graphical modelreproducing kernel Hilbert space","295":"Feature screeningconsistency in rankingsure screening propertyultrahigh dimensional data analysis","296":"Causal inferenceConfoundingEndogeneitySparse regressionTwo-stage least squaresVariable selection","297":"Adaptive DesignBayesian DesignDynamic Treatment RegimeLatent Probit ModelPhase I-II Clinical TrialQ-Learning","298":"Association studiesGene-level testsLinear regressionQuantitative traitsRare variantsSequencing studies","299":"Categorical Data AnalysisClassification and ClusteringMathematical StatisticsModel Selection\/Variable Selection","300":"copy number variationheteroscedasticitylarge ptwo-sample","301":"Fine balancenetwork optimizationoptimal matchingsparse networks","302":"Aggregate covariatesHeterogeneityMultiple trialsNormal regression modelsRandom effectsvariable selection","303":"ClassificationDynamic treatment regimesPersonalized medicineQ-learningReinforcement learningRisk BoundSupport vector machine","304":"Gene-set analysisGenetic associationGenetic pathwaysKernel PCAKernel machine regressionPrincipal component analysisRisk prediction","305":"Dirichlet process mixtureIdentifiabilityIdentifying restrictionsSensitivity analysis","306":"Alternating logistic regressionClustered binary dataComposite likelihoodMarkov transition modelPairwise likelihood","307":"External validation studyFunctional measurement error modelingGeneralized linear modelsLikelihood methodMeasurement errorMisclassificationRegression calibrationSemiparametric regressionSimulation extrapolation algorithmStructural measurement error modeling","308":"combining informationcomplex evidence synthesisconfidence distributionefficiencygeneralized estimating equationsheterogeneous studiesindirect evidenceindividual participant datamultivariate meta-analysis","309":"Bayesian nonparametricBregman divergenceFeature allocationIndian buffet processNext-generation sequencingTumor heterogeneity","310":"Chain-binomial modelEM algorithmEpidemiologySurvival analysis","311":"Asymptotic propertiesclinical trialgeneralized linear modelspersonalized medicinepowerresponse-adaptive randomization","312":"Complex normalConfigurationLandmarkOrdinary Procrustes analysisQuaternion","313":"clusteringhomogeneitysparsity","314":"Bayesian inferenceG-Wishart priorGaussian graphical modelMarkov random fieldprotein network","315":"Abundance estimationAcoustic surveyClosed populationMeasurement errorVisual survey","316":"Potts modelbrain mappingdynamic systemeffective connectivityordinary differential equation (ODE)","317":"high-dimensional inferencehigh-throughput biologyinter-laboratory comparisonsmeasurement errornon-ignorable missing datastructured covariance","318":"Bayesian non-parametricsCancerGenomicsMCMCPredictive Probability FunctionsRandom PartitionsSpecies Sampling Priors","319":"","320":"Accelerated failure time modelCox modelLength-biased dataLikelihood functionProportional odds modelScore equation","321":"Case-control studiesCopy number variantsGenome-wide association studiesRetrospective likelihoodSemiparametric efficiencySingle nucleotide polymorphisms","322":"Functional data analysisLongitudinal dataNonparametric statisticsScalar-on-function regressionVariable-domain functional regressionVarying-coeffcient model","323":"Generalized methods of momentsNonparametric correctionSurvival","324":"Simultaneous pursuit of sparseness and clusteringmultiple networksnon-convexpredictionsignaling network inference","325":"BICGWASL1-penaltyMM AlgorithmPenalized Bernoulli LikelihoodSimultaneous Modeling of SNPs","326":"combining informationconfidence distributionexact inferencep-value functionzero events","327":"principal componentsquantile regression with time series errorsstatistical paleoclimate reconstructionsuncertainty quantification","328":"","329":"BayesianBig dataCategorical dataContingency tableLow rankMatrix completionPARAFACTensor factorization","330":"Bayesian Variable SelectionDirichlet ProcessExpression Quantitative Trait LociHierarchical ModelInteraction Detection","331":"Mahalanobis distancecausal inferencecovariate balanceexperimental designrandomizationtreatment allocation","332":"G-computationcausal inferenceconfoundingdouble robustnessinverse probability weightingsequential regressiontargeted minimum loss based estimationtreatmentality","333":"Matrix normalMaximum likelihoodNetworkshypothesis testing","334":"Bayesian learningSocial networksfinite population learninglearning ratesmulti-agent inferenceperfect learning","335":"BootstrapFamily-wise error rateMarginal regressionNon-regular asymptoticsScreening covariates","336":"Evolutionary hybrid algorithmGeneralized nonlinear modelInfluenza viral dynamicsNumerical error theory","337":"Biomedical imagingclosed curvescyclic basisdeformationfunctional datahierarchical modelingmultiscale","338":"Functional data analysisGaussian processMuscle forceOrdinary differential equationsStochastic differential equations","339":"BOLD contrastBalanced designCausal inferenceLongitudinal dataMeasurement errorfMRI","340":"Asymptotic normalityFunctional principal component analysisJumping surface modelKernelSpatial varying coefficient modelWald test","341":"","342":"Forward selectionGWASHeredity conditionInteractionSure Screening","343":"Hard-thresholdingPenalized likelihoodSparsity-constrained optimizationSure screening propertyUltra-high dimensionality","344":"AddictionComorbidityGenome-wide association studyInverse probability weightingSubstance dependence","345":"Bayesian Adaptive Clinical DesignData Augmentation AlgorithmDose-FindingMissing DataPhase I-II Clinical Trial DesignPiecewise Exponential Model","346":"Composite likelihoodFunctional dataLatent processPoint processSemi-parametric methodsSpatio-temporal dataSplinesStrong mixing","347":null,"348":"Generalized low rank regressionGenetic variantHigh dimensionImaging phenotypeMarkov chain Monte CarloPenalized method","349":"ABC intervalsCpLassobaggingbootstrap smoothingimportance samplingmodel averaging","350":"Birth-death processEM algorithmMM algorithmcontinuous-time Markov chainmaximum likelihood estimationmicrosatellite evolution","351":"Conditional permutationFalse positive ratesSparsitySure independence screeningVariable selection","352":null,"353":"Gaussian graphical modelgeneralized linear modellassolog-concave density estimationordinary differential equationsquasi-likelihoodsregularizationshape restricted regressionsolution path","354":"Adaptive designMultivariate testNeuroimagingPower analysis","355":"Asymptotic theoryBayes factorLarge pModel selectionPosterior consistencyStochastic search variable selectionSubset selectiong-priorsmall n","356":"Adaptive group LASSODifferential equationsHigh-dimensional dataSparse additive modelsTime course microarray dataVariable selection","357":"absolute riskattributable riskcohort datacolorectal cancerexternal disease incidence rate ","358":"Functional Dirichlet processFunctional data analysisNonparametric BayesPrior elicitationRandom curvesRandom probability measure","359":"AttenuationBayes ruleBinary regressionConvolutionDiscriminant analysisKernel discriminant analysisLASSOLinear regressionMaximum likelihood ruleModel selectionRidge regression","360":"Elliptical distributionHigh dimensional statisticsPrincipal component analysisRobust statistics","361":"Big dataHigh-dimensional testingMidpower analysisOptimal kernel constructionPearson-normal kernelPower lemma","362":"Circadian rhythmFeedback RelationshipHPA axisKalman filterPeriodic splines","363":"Bayesian variable selectionLatent variable modelMarkov chain Monte CarloMethylmercurySparsity","364":"Bayesian statisticscausal inferencecomparative effectivenessmodel averagingpropensity score","365":"Spatial epidemiologyestimating equationspatial point process","366":"Feature selectionranking consistencysure screening propertyvarying coefficient models","367":"Autoregressive modelingHidden Markov ModelHigh-throughput SequencingMixture RegressionVariable Selection","368":"Chemical mappingDeconvolutionEM algorithmNegative Binomial cluster modelNucleosome positioningPoisson cluster model","369":"ROC analysisbreast cancerfamilial risk predictionfrailty modelmultivariate survivalrisk index","370":"Efficiency AugmentationKaplan MeierLandmark PredictionSemi-competing RisksSurvival Analysis","371":"Case-cohort designEM algorithmKernel estimationNested case-control samplingNonparametric likelihoodSemiparametric efficiency","372":"Normal mixturecausal inferenceconfidence intervalinterferencerandomization","373":"MarkovREMclassificationsleeptime series","374":"Functional dataGEELongitudinal dataPenalized splineSemiparametric models","375":"Case-control studymatchingobservational studysensitivity analysis","376":"Dynamic modelMultivariate categorical dataNonparametric BayesPanel dataParafacProbabilistic tensor factorizationStick-breaking","377":"Biomarker studyInterval EstimationInverse Probability WeightingNested case-control studyResampling methods, Risk PredictionSimultaneous Confidence BandSurvival Model","378":"","379":"Akaike information criterionBayesian information criterionFunctional data analysisKernel smoothingPrincipal components","380":"Adaptive lassoChromatographyCurve registrationHerbal medicineVariable selection","381":"Adaptive confidence bandaverage coveragecoverage probabilityexcess masslower boundsnoncovered pointsnonparametric regressionwaveletswhite noise model","382":"cross-sectional samplinginvarianceleft truncationlifetime riskloss to follow-uppoint processesstratified sampling","383":"Asymptotic theoryBasis function expansionBayesian methodDifferential equationsMeasurement errorParameter cascading","384":"EM algorithmKernel regressionMixture of regression modelsNonparametric regression","385":"Classification and ClusteringGlivenko-Cantelli classesSieve Maximum Likelihood EstimationSliced Inverse RegressionStatistical Learning","386":"Bayesian Hierarchical ModelImaging GeneticsMarkov Random FieldNeuroimagingSingle-nucleotide polymorphismVariable Selectionfunctional Magnetic Resonance Imaging","387":null,"388":"Attachment levelDirichlet processKernel convolutionNon-normalityNon-stationarity","389":"Change-pointsCounting processTime-varying coefficient","390":null,"391":"Bayesian statisticsClinical trialDose-escalation studyDynamic treatment regimeNon-mixture cure model","392":"As-treatedBoundsCausal inferenceExclusion restrictionIgnorance regionIntention to treatPrincipal stratificationSelection biasSurvival analysis","393":"Allan varianceKalman filterSignal processingTime series","394":"Network analysisnonconvex minimizationpredictionstructured data","395":"Longitudinal Count DataOver-dispersionRandom effectSerial correlationTeenage driving","396":"AstrostatisticsBlind Source DetectionMultiple Testing","397":"Cross-training-evaluationLasso procedurePersonalized medicinePredictionRidge regressionStratified medicineSubgroup analysisVariable selection","398":"EM algorithmHealth ABC studyLatent Markov modelcontinuation ratio modelgeneralized linear model","399":"Classification rulesDiagnostic and Statistical Manual of Mental DisordersLarge margin classificationMissing dataStatistical learningSupport vector machine","400":"Backward and forward recurrence timeCross-sectional samplingRandom truncationRenewal processes","401":"Bat SyllableBayesian AnalysisChirpFunctional Data AnalysisFunctional Mixed ModelIsomorphic TransformationNonstationary Time SeriesSoftwareSpectrogramVariable Overlap","402":"Direct\/indirect effectsinterferencemultilevel modelssocial interactions","403":"Extended rank likelihoodFactor analysisHigh dimensionalLatent variablesParameter expansionSemiparametric","404":"Asymptotic efficiencyBreakdown pointConstrained optimizationEfficient estimationEmpirical likelihoodExponential tiltingLeast trimmed squaresRobust regressionWeighted least squares","405":"Breakdown pointInfluence functionRobust regressionVariable selection","406":"","407":"Case-cohort designCounting processCox modelEstimating equationsImportance samplingLength-biasProportional odds modelRegressionSurvival dataTruncation","408":"Cardiovascular outcomesCase series modelsEnd stage renal diseaseInfectionMeasurement errorNon-homogeneous Poisson processTime-varying exposure onsetUnited States Renal Data System","409":"Administrative DataBayesian AnalysisMissing DataRecord LinkageStatistical Matching","410":"Bayes ClassifierCross ValidationDynamic Treatment RegimeIndividualized Treatment RuleRKHSRisk BoundWeighted Support Vector Machine","411":"Centroid classifierCross-validationFourier transformInverse transformSpatial data","412":"Bayesian hierarchical modelCohort component modelFertility rateInternational migrationMarkov chain Monte CarloMortality rate","413":"","414":"ClassifierDimension reductionNonparametric BayesVariable selection","415":"Bayesian variable selectionConsistencyCredible regionLASSOStochastic search","416":"","417":"","418":"","419":"Bayesian nonparametric regressionNested Gaussian processesNested smoothing splinePenalized sum-of-squareReproducing kernel Hilbert spaceStochastic differential equations","420":"conformal predictiondistribution freefinite samplekernel densityprediction sets","421":"Brain imagingdimension reductiongeneralized linear model (GLM)magnetic resonance imaging (MRI)multidimensional arraytensor regression","422":"Antiretroviral failureHIV\/AIDSROCconstrained optimizationresource limitedtripartite classification","423":"","424":"Brain connectivityCausal inferenceFunctional data analysisInstrumental variableMediationStructural equation modelsfMRI","425":"","426":"Euler discretizationautocorrelationdata augmentationextrapolationlikelihoodmissing datastochastic differential equation","427":null,"428":"ConfidentialityContingency tableDisclosureGrade of membershipLatent class","429":"","430":"","431":"","432":"","433":"","434":"Distance correlationsure screening propertyultrahigh dimensionalityvariable selection","435":"","436":"","437":"","438":"","439":"","440":"","441":"","442":"","443":"ClassificationContingency tableFactor analysisLatent variableMutual informationNonnegative tensor factorizationNonparametric BayesPolytomous regression","444":"Intrinsic least squares estimatorMedial representationSemiparametric modelWald statistic","445":"causal inferencedirect effectmicrobicideposttreatment variables","446":"","447":"","448":"","449":"","450":"Bayesian inferenceExtreme directionsExtreme pointsParameter restrictionPolyhedral region","451":"Evidence factorfine balanceoptimal subset matchingsensitivity analysis","452":"Attributable effectsinterference between unitsplacementsrandomized experiment","453":"Fourier transformFunctional magnetic resonance imagingLocal spatial covarianceMulti-scale correlationSpectrum","454":"","455":"Multiple hypothesis testingarbitrary dependence structurefalse discovery rategenome-wide association studieshigh dimensional inference","456":"Conditional random fieldGaussian graphical modelsLasso and adaptive lassoOracle propertyReproducing kernel Hilbert spaceSparsistencySparsityvon Mises expansion","457":"Curve RegistrationDrug UseFunctional DataGeneralized Linear ModelsIndividual TrajectoriesLongitudinal DataMCMCUnimodal Smoothing","458":null,"459":"","460":"Adaptive LASSODantzig selectorElastic netIntrinsic Bayes factorIntrinsic priorNonlocal priorNonnegative garroteOracleg-prior","461":"Causal inferenceCoronary heart diseaseEpidemiologyG-estimationInverse probability weighting","462":"Bayesian nonparametricsGibbs samplerHierarchical network analysisLatent space model","463":"Estimating equationsNonparametric regressionRobustnessSemiparametric methodsSliced inverse regression","464":"Bayes factorDirichlet processGoodness-of-fit testLatent classMixture modelMotif dataProduct multinomialUnordered categorical","465":"","466":"","467":null,"468":"","469":"","470":"","471":"","472":"","473":"","474":"","475":"","476":"","477":"","478":"","479":"","480":"","481":"","482":"","483":"","484":"Climate changeOzoneSemiparametric Bayesian methodsSpatial data","485":"","486":"Backfitting algorithmEnvironmental epidemiologyParticulate matterSpatio-temporal dataSpecification test","487":"Blind source separationDiscrete Fourier transformSpectral analysisTime seriesWhittle likelihood","488":"CAR modelG-Wishart distributionMarkov chain Monte Carlo (MCMC) simulationSpatial statistics","489":"GC contentMappabilityMixture modelNegative binomial regressionNext generation sequencing","490":"AgglomerativeDendrogramUnsupervised learning","491":"AMLGraphical modelsMixture modelsPOERJ-MCMCRPPA","492":"Bayesian inferenceBiomassForestryMarkov chain Monte CarloMultivariate spatial processSpatial modelsSpatial predictive process","493":"Degrees of freedomLASSONonconvex optimizationRegularization surfaceSparse regressionVariable selection","494":"ElectroencephalographySignal extraction","495":"Bayesian nonparametricsDynamic factor analysisHigh-dimensionalInfectious diseaseJoint modelMultidimensional longitudinal dataMultivariate functional dataPredictive model","496":"Asymptotic normalityConsistencyHypothesis testingJackknifeNonparametric statisticsOrder consistency","497":"","498":"","499":"","500":"","501":"","502":"","503":"","504":"","505":"BayesianNational Comorbidity Surveybivariatehierarchical modelspredictionserious emotional distresssmall-area estimationsurvey","506":"","507":"","508":"","509":"","510":"","511":"","512":"","513":"","514":"","515":"","516":"","517":"","518":"","519":"","520":"","521":"","522":"","523":"","524":"","525":"","526":"","527":"Nonparametric BayesOption pricingPoint-referenced countsRandom probability measureRandom stochastic processes","528":"DNA copy numberLikelihood ratio selectionmultiple testingsignal detection","529":"","530":"","531":"","532":"","533":"","534":"","535":"Latent Non-random Mixing ModelNegative Binomial DistributionPersonal Network SizeSocial NetworksSurvey Design","536":"","537":"","538":"","539":"","540":"","541":"","542":"","543":"","544":"","545":"","546":"Diffusion tensor imagingLikelihood ratio testManifold-valued dataMultiple testingRandom matrixSatterthwaite approximation","547":"","548":"Logarithm transformationMultiplicative regression modelRandom weightingRelative error","549":"","550":"","551":"","552":"","553":"","554":"","555":"","556":"","557":"","558":"Conditional distribution estimationHypothesis testingKernel stick-breaking processMixture of expertsStochastic search variable selection","559":"","560":"","561":"","562":"","563":"","564":"","565":"","566":"","567":"","568":"","569":"","570":"","571":"","572":"","573":"","574":"","575":"","576":"","577":"","578":"","579":"Bayesian clusteringDirichlet processEarly pregnancy lossFunctional predictorJoint modelingProgesterone","580":"","581":"","582":"","583":"","584":"","585":"","586":"","587":"Auxiliary informationBiased samplingCausal inferenceObservational studySurvey sampling","588":"","589":"","590":"","591":"","592":"","593":"","594":"","595":"","596":"","597":"","598":"","599":"Bivariate clusteringDirichlet processFunctional predictorsGrowth mixture modelJoint modelingLatent class trajectory","600":"Acquiescence biasItem nonresponseUnfolding brackets","601":"Markov Chain Monte Carlo methodsNatural history modeldisease progressionlatent variableslongitudinal responseprostate specific antigen","602":"","603":"","604":"","605":null,"606":"","607":"Aggregate dataDemographyHealthy life expectancyLife tablesMorbidityMortalityStationarity","608":"","609":"","610":null,"611":"","612":"","613":"","614":"","615":"","616":"","617":"","618":"","619":"","620":"Borrow-strength methodFrailtyInformative censoringJoint modelNonstationary Poisson process","621":"Causal inferenceHIVNeedle exchangePartially controlled studiesPotential outcomesPrincipal stratification","622":"","623":"AmericasDemographic FactorsDeveloped CountriesEconomic FactorsImmigrantsInternational MigrationMigrantsMigrationNorth AmericaNorthern AmericaPopulationPopulation DynamicsSocial WelfareSocioeconomic FactorsUnited States","624":"Age FactorsAmericasBehaviorBiologyBody WeightCultural BackgroundDemographic FactorsDeveloped CountriesDifferential MortalityEthnic GroupsMortalityNorth AmericaNorthern AmericaPhysiologyPopulationPopulation CharacteristicsPopulation DynamicsSex FactorsSmokingUnited States","625":"Arab CountriesAsiaCultural BackgroundDemographic FactorsDeveloping CountriesError SourcesEstimation TechnicsEthnic GroupsIraqMeasurementMethodological StudiesPopulationPopulation CharacteristicsPopulation ForecastPopulation ProjectionResearch MethodologyWestern Asia","626":"Coherent signed rank testEqual percent bias reducingMatching with dosesNonbipartite matchingObservational studiesOptimal matchingOrdinal logit modelPropensity score","627":"","628":"FrailtyIntensity functionLatent variableProportional rate modelRate function","629":"Correlated survival dataFrailtyKaplan-Meier estimateLongitudinal designsRecurrent event","630":"AmericasBrazilChild MortalityChild SurvivalCommunityDemographic FactorsDeveloping CountriesEstimation TechnicsFamily And HouseholdFamily CharacteristicsGeographic FactorsLatin AmericaLength Of LifeModels, TheoreticalMortalityPopulationPopulation DynamicsResearch MethodologyResidence CharacteristicsSouth AmericaSpatial DistributionSurvivorship","631":"Demographic FactorsFertilityMathematical ModelMenstrual CycleMenstruationModels, TheoreticalPopulationPopulation DynamicsReproductionResearch Methodology","632":"BehaviorBiasCancerDiseasesEpidemiologyError SourcesHealthLiterature ReviewMeasurementNeoplasmsPublic HealthResearch MethodologySmokingWorld","633":"Administrative DistrictsAge DistributionAge FactorsAmericasBiasComparative StudiesCountiesDemographic FactorsDeveloped CountriesError SourcesEstimation TechnicsEvaluationFloridaGeographic FactorsMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation ForecastPopulation ProjectionStudiesUnited States","634":"Age FactorsAmericasComparative StudiesDemographic FactorsDeveloped CountriesEstimation TechnicsMethodological StudiesNorth AmericaNorthern AmericaPolicyPopulationPopulation CharacteristicsPopulation ForecastStudiesUnited States","635":"AmericasCensusCensus MethodsData AdjustmentDeveloped CountriesError SourcesEstimation TechnicsLogistic ModelMathematical ModelMeasurementModels, TheoreticalNonrespondentsNorth AmericaNorthern AmericaPopulation StatisticsProbabilityResearch MethodologySampling StudiesStatistical StudiesStudiesSurveysUndercountUnited States","636":"AmericasCensusCensus MethodsData AdjustmentData CollectionData SourcesDemographic FactorsDeveloped CountriesError SourcesEstimation TechnicsHeterogeneityMathematical ModelMeasurementModels, TheoreticalNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation StatisticsResearch MethodologyUndercountUnited States","637":"AmericasBiasCensusCensus MethodsData AdjustmentData AnalysisData QualityDemographic FactorsDeveloped CountriesError SourcesEstimation TechnicsMeasurementMigrationNorth AmericaNorthern AmericaPopulationPopulation DynamicsPopulation StatisticsResearch MethodologySampling StudiesStudiesSurveysUnited States","638":"AmericasCensusCensus MethodsData AdjustmentDeveloped CountriesError SourcesEvaluationMeasurementMichiganNorth AmericaNorthern AmericaPopulation StatisticsResearch MethodologySampling StudiesStudiesSurvey MethodologySurveysTexasUnited States","639":"AmericasBiasCensusCensus MethodsData AdjustmentData CollectionDeveloped CountriesDual Data CollectionError SourcesEstimation TechnicsEvaluationMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulation StatisticsResearch MethodologySampling StudiesStudiesSurveysUnited States","640":"AfricaAfrica South Of The SaharaAmericasAsiaBreast FeedingColombiaData AnalysisDeveloping CountriesEnglish Speaking AfricaEstimation TechnicsEvaluationFrench Speaking AfricaHealthIndonesiaInfant NutritionLatin AmericaMaliMathematical ModelModels, TheoreticalNigeriaNutritionPeruResearch MethodologySouth AmericaSoutheastern AsiaSouthern AsiaSri LankaWestern Africa","641":"AmericasCensusCensus MethodsData AdjustmentData AnalysisData CollectionData QualityDeveloped CountriesDual Data CollectionError SourcesEstimation TechnicsMeasurementNorth AmericaNorthern AmericaPopulation StatisticsResearch MethodologySampling StudiesStudiesSurvey MethodologySurveysUndercountUnited States","642":"AmericasCensusCensus MethodsData AdjustmentData AnalysisData QualityDeveloped CountriesError SourcesEvaluationMeasurementNorth AmericaNorthern AmericaPolitical FactorsPopulation StatisticsResearch MethodologyUndercountUnited States","643":"AmericasCensusCensus MethodsData AnalysisData QualityDemographic AccountingDemographic AnalysisDeveloped CountriesError SourcesEstimation TechnicsEvaluationMeasurementNorth AmericaNorthern AmericaPopulation StatisticsResearch MethodologyUnited States","644":"AmericasCensusCensus MethodsData AdjustmentDeveloped CountriesError SourcesEstimation TechnicsEvaluationMeasurementNorth AmericaNorthern AmericaPopulation StatisticsResearch MethodologySampling StudiesStudiesSurvey MethodologySurveysUnited States","645":"Analysis of varianceMultivariate linear modelsNoncentral distributionRepeated measuresSample size determination","646":"Age FactorsAge Specific Fertility RateBirth RateComparative StudiesContraceptionDemographic AnalysisDemographic FactorsFamily PlanningFertilityFertility MeasurementsFertility RateMarital FertilityPopulationPopulation CharacteristicsPopulation DynamicsResearch MethodologyStudiesWorld","647":"AmericasCensusCensus MethodsData CollectionDeveloped CountriesError SourcesEstimation TechnicsFamily And HouseholdHouseholdsInterviewsMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulation StatisticsResearch MethodologySampling StudiesStudiesSurvey MethodologySurveysUnited States","648":"AmericasBehaviorComparative StudiesDecision MakingDemographic FactorsDeveloped CountriesFamily And HouseholdFamily CharacteristicsFamily SizeFamily Size, DesiredFertilityHusband-wife ComparisonsModels, TheoreticalNorth AmericaNorthern AmericaPopulationPopulation DynamicsResearch MethodologyStatistical StudiesStudiesUnited States","649":"AmericasCensusData CollectionDeveloped CountriesDual Data CollectionError SourcesEstimation TechnicsMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulation Growth EstimationPopulation StatisticsResearch MethodologyUndercountUnited States","650":"Causes Of DeathData AdjustmentDemographic FactorsDiseasesError SourcesHeart DiseasesMeasurementMethodological StudiesModels, TheoreticalMortalityPopulationPopulation DynamicsResearch MethodologyWorld","651":"AmericasData AdjustmentData CollectionDemographic AnalysisDeveloped CountriesDual Data CollectionError SourcesMeasurementNorth AmericaNorthern AmericaPolitical FactorsResearch MethodologyUndercountUnited States","652":"AmericasDeveloped CountriesEstimation TechnicsMethodological StudiesNorth AmericaNorthern AmericaPopulation Growth EstimationResearch MethodologyUnited States","653":"AmericasCohort AnalysisDeveloped CountriesMarriageMarriage PatternsModels, TheoreticalNorth AmericaNorthern AmericaNuptialityResearch MethodologyUnited States","654":"AmericasDeath RateDemographic FactorsDeveloped CountriesError SourcesEstimation TechnicsEvaluationEvaluation ReportMeasurementModels, TheoreticalMortalityNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation DynamicsPopulation ForecastProbabilityResearch MethodologySex FactorsStatistical StudiesStudiesUnited States","655":"AmericasDeveloped CountriesError SourcesEstimation TechnicsEvaluationEvaluation MethodologyHistorical SurveyMeasurementNorth AmericaNorthern AmericaPopulation ForecastPopulation ProjectionResearch MethodologyUnited States","656":"AmericasDemographic FactorsDeveloped CountriesEvaluationFecundabilityFecundityFertilityFertility MeasurementsHigh Fertility PopulationModels, TheoreticalNorth AmericaNorthern AmericaPopulationPopulation DynamicsReproductionResearch MethodologyTime FactorsUnited States","657":"AmericasCensusComparative StudiesCultural BackgroundDemographic FactorsDeveloped CountriesEthnic GroupsEvaluationEvaluation ReportHispanicsMeasurementModels, TheoreticalNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation StatisticsResearch MethodologyStudiesUnited States","658":"AmericasCaliforniaCancerComparative StudiesDeveloped CountriesDiseasesError SourcesMeasurementModels, TheoreticalNeoplasmsNorth AmericaNorthern AmericaReliabilityResearch MethodologyStudiesUnited States","659":"AmericasBlacksCensusCultural BackgroundData AdjustmentDemographic FactorsDeveloped CountriesError SourcesEthnic GroupsHispanicsMeasurementNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation StatisticsResearch MethodologyUndercountUnited States","660":"Birth RateDemographic FactorsDeveloped CountriesEconomic FactorsEstimation TechnicsEuropeFertilityFertility MeasurementsIncomeModels, TheoreticalNorthern EuropePopulationPopulation DynamicsPopulation ForecastResearch MethodologyScandinaviaSocioeconomic FactorsSwedenTime Factors","661":"AmericasCensusCultural BackgroundDemographic FactorsDeveloped CountriesError SourcesEstimation TechnicsEthnic GroupsMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation StatisticsResearch MethodologyUndercountUnited States","662":"CommunicationCritiqueData AnalysisDemographic FactorsDeveloping CountriesEvaluationFertilityFertility MeasurementsFertility SurveysLanguageMethodological StudiesPopulationPopulation DynamicsResearch MethodologyRetrospective StudiesStudiesWorld Fertility Surveys","663":"Age FactorsAge Specific Fertility RateAmericasBiasBirth RateCensusCensus MethodsComparative StudiesCultural BackgroundData AnalysisDemographic FactorsDeveloped CountriesError SourcesEstimation TechnicsEthnic GroupsFertilityFertility MeasurementsFertility RateMaternal AgeMeasurementModels, TheoreticalMultivariate AnalysisNorth AmericaNorthern AmericaParental AgePopulationPopulation CharacteristicsPopulation DynamicsPopulation ProjectionPopulation StatisticsResearch MethodologyStudiesTime FactorsUnited StatesWhites--women","664":"Age FactorsAmericasCancerCauses Of DeathCultural BackgroundDeath RateDemographic FactorsDeveloped CountriesDiseasesEthnic GroupsGeographic FactorsMapsMethodological StudiesMortalityNeoplasmsNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation DynamicsSex FactorsSpatial DistributionUnited StatesWhites","665":"AmericasComparative StudiesData CollectionData SourcesDemographic FactorsDestinationDeveloped CountriesDeveloping CountriesDistanceEstimation TechnicsEvaluationGeographic FactorsIndirect Estimation TechnicsMeasurementMigrationNorth AmericaNorthern AmericaOriginPopulationPopulation DynamicsReliabilityResearch MethodologySampling StudiesStudiesSurvey MethodologySurveysUnited States","666":"AmericasCancerCauses Of DeathDemographic FactorsDeveloped CountriesDeveloping CountriesDifferential MortalityDiseasesGeographic FactorsMethodological StudiesMissouriModels, TheoreticalMortalityNeoplasmsNorth AmericaNorthern AmericaPopulationPopulation DynamicsPulmonary EffectsResearch MethodologyUnited States","667":"Administrative DistrictsAmericasBiasComparative StudiesCountiesDeveloped CountriesDeveloping CountriesError SourcesEstimation TechnicsEvaluationEvaluation ReportGeographic FactorsMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulationPopulation ForecastPopulation ProjectionResearch MethodologyUnited States","668":"Cohort AnalysisComparative StudiesDemographic FactorsEstimation TechnicsMeasurementMethodological StudiesModels, TheoreticalPopulationPopulation DynamicsPopulation ForecastPopulation ProjectionReliabilityResearch MethodologyTime FactorsWorld","669":"AmericasBirth OrderComparative StudiesDemographic FactorsDeveloped CountriesDeveloping CountriesEstimation TechnicsFamily And HouseholdFamily CharacteristicsFamily RelationshipsFertilityMethodological StudiesModels, TheoreticalNorth AmericaNorthern AmericaPopulationPopulation DynamicsPopulation ForecastProbabilityResearch MethodologyStatistical StudiesStudiesTime FactorsUnited States","670":"AdultAge FactorsAge Specific Death RateAgedAmericasCancerCauses Of DeathDeath RateDemographic AnalysisDemographic FactorsDeveloped CountriesDeveloping CountriesDifferential MortalityDiseasesHeterogeneityMathematical ModelModels, TheoreticalMortalityNeoplasmsNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation DynamicsResearch MethodologyUnited States","671":"AmericasCensusData CollectionDeveloped CountriesDeveloping CountriesDual Data CollectionError SourcesMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulation StatisticsResearch MethodologySampling StudiesStudiesSurveysUndercountUnited States","672":"AmericasComparative StudiesDeveloped CountriesDeveloping CountriesEstimation TechnicsEvaluationEvaluation ReportGeographic FactorsHousingMethodological StudiesNorth AmericaNorthern AmericaPopulationPopulation Growth EstimationResearch MethodologyResidence CharacteristicsSpatial DistributionUnited States","673":"AsiaBiasBiologyBirth WeightBody WeightBreast FeedingChild HealthDemographic FactorsDeveloping CountriesError SourcesHealthInfant MortalityInfant NutritionMalaysiaMathematical ModelMeasurementModels, TheoreticalMortalityNutritionPhysiologyPopulationPopulation DynamicsResearch MethodologyResearch ReportSoutheastern Asia","674":"Age FactorsAmericasDeveloped CountriesDeveloping CountriesEstimation TechnicsEthnic GroupsMethodological StudiesNorth AmericaNorthern AmericaOrganization And AdministrationPopulation Growth EstimationPopulation ProjectionProgram DesignProgramsResearch MethodologySex FactorsTarget PopulationUnited States","675":"Demographic FactorsDeveloped CountriesEstimation TechnicsEuropeMethodological StudiesModels, TheoreticalMultiregional AnalysisNorthern EuropePopulationPopulation DynamicsPopulation ProjectionResearch MethodologyScandinaviaSwedenTime Factors","676":"AmericasCohort AnalysisDemographic FactorsDeveloped CountriesDeveloping CountriesEstimation TechnicsEthnic GroupsFertilityFertility MeasurementsHeterogeneityMaternal AgeMathematical ModelModels, TheoreticalNorth AmericaNorthern AmericaParityPopulationPopulation CharacteristicsPopulation DynamicsResearch MethodologyUnited States","677":"Age FactorsBirth RateCohort AnalysisDemographic FactorsDeveloped CountriesEstimation TechnicsEuropeFertilityFertility MeasurementsFertility RateMarital FertilityMethodological StudiesModels, TheoreticalNetherlandsPeriod AnalysisPopulationPopulation CharacteristicsPopulation DynamicsPopulation ProjectionResearch MethodologyTime FactorsWestern Europe","678":"AmericasComparative StudiesData AnalysisDemographic AnalysisDeveloped CountriesDeveloping CountriesError SourcesEstimation TechnicsMathematical ModelMeasurementMethodological StudiesModels, TheoreticalNorth AmericaNorthern AmericaPopulation ForecastPopulation ProjectionReliabilityResearch MethodologyUnited States","679":"AmericasDemographic FactorsDeveloped CountriesDeveloping CountriesEconomic ConditionsEconomic FactorsEconomic ModelEstimation TechnicsHuman ResourcesLabor ForceMacroeconomic FactorsMethodological StudiesMigrationMigration, InternalModels, TheoreticalNorth AmericaNorthern AmericaPopulationPopulation DynamicsPopulation ForecastResearch MethodologySocioeconomic FactorsTime FactorsUnited States","680":"AmericasArea AnalysisBiasComparative StudiesData AnalysisDemographic AnalysisDemographic FactorsDeveloped CountriesDeveloping CountriesError SourcesEstimation TechnicsFloridaGeographic FactorsHousingMeasurementMethodological StudiesNorth AmericaNorthern AmericaPopulationPopulation DynamicsPopulation Growth EstimationPopulation SizeReliabilityResearch MethodologyResidence CharacteristicsSpatial DistributionUnited States","681":"AmericasBlacksChild MortalityCultural BackgroundDemographic FactorsDemographyDeveloped CountriesDeveloping CountriesDifferential MortalityError SourcesEthnic GroupsHistorical DemographyHistorical SurveyMeasurementMortalityNorth AmericaNorthern AmericaPopulationPopulation CharacteristicsPopulation DynamicsReliabilityResearch MethodologySocial SciencesUnited StatesWhites","682":"AmericasBrazilChild MortalityDemographic FactorsDeveloped CountriesDeveloping CountriesEconomic DevelopmentEducational StatusEstimation TechnicsFertilityInfant MortalityLatin AmericaMathematical ModelMethodological StudiesModels, TheoreticalMortalityPopulationPopulation DynamicsReproductive BehaviorResearch MethodologyResearch ReportResidence CharacteristicsSouth America","683":"AmericasDeveloped CountriesDeveloping CountriesError SourcesEstimation TechnicsEvaluationEvaluation MethodologyMeasurementNorth AmericaNorthern AmericaPopulation ProjectionReliabilityResearch MethodologyUnited StatesWorld","684":"AustraliaBirth RateCritiqueDeveloped CountriesEstimation TechnicsEvaluationModels, TheoreticalOceaniaPopulation ForecastResearch Methodology","685":"AsiaBangladeshBirth IntervalsBirth RateDemographic FactorsDeveloping CountriesFertilityFertility MeasurementsFertility RateModels, TheoreticalPopulationPopulation DynamicsReproductive BehaviorResearch MethodologySeasonal VariationSouthern Asia","686":"Estimation TechnicsPopulation SizeResearch MethodologySampling StudiesStudiesTechnical Report","687":"Demographic FactorsDeveloped CountriesEstimation TechnicsEuropeFertilityNorthern EuropeNorwayPopulationPopulation DynamicsPopulation ForecastPopulation ProjectionResearch MethodologyScandinavia","688":"AsiaDeveloping CountriesIndiaSouthern Asia","689":"Demographic AnalysisDeveloped CountriesDivorceEuropeLife Table MethodLife TablesMarital StatusMarriageMarriage PatternsNorthern EuropeNuptialityResearch MethodologyScandinaviaSweden","690":"AmericasDeveloped CountriesGeographic FactorsNorth AmericaNorthern AmericaPopulationSpatial DistributionUnited States","691":"AmericasDeveloped CountriesGeographic FactorsNorth AmericaNorthern AmericaPopulationSpatial DistributionUnited States","692":"AmericasDeveloped CountriesGeographic FactorsNorth AmericaNorthern AmericaPopulationSpatial DistributionUnited States","693":"Estimation TechnicsPopulation Growth EstimationResearch Methodology","694":"AmericasCaribbeanDemographic FactorsDeveloping CountriesFertilityJamaicaMethods Of Historical DemographyNorth AmericaPopulationPopulation Dynamics","695":"Abortion SeekersAbortion, Induced--statisticsAmericasDeveloped CountriesFamily PlanningFertility Control, PostconceptionNorth AmericaNorthern AmericaResearch MethodologySampling StudiesStudiesSurveysUnited States","696":"AmericasCauses Of DeathDemographic FactorsDeveloped CountriesMortalityNorth AmericaNorthern AmericaPopulationPopulation DynamicsUnited States","697":"DemographyEstimation TechnicsMarkov ChainMathematical ModelModels, TheoreticalNorth AmericaPopulation Growth EstimationProbabilityResearch MethodologyStatistical StudiesStudiesUnited States","698":"Models, TheoreticalRepeated Rounds Of SurveyResearch MethodologySampling StudiesStudiesSurvey MethodologySurveysTime Factors","699":"Demographic FactorsModels, TheoreticalPopulationPopulation DynamicsResearch Methodology","700":"Birth IntervalsDemographic FactorsEstimation TechnicsFecundabilityFecundityFertilityFertility MeasurementsFirst Birth IntervalsModels, TheoreticalOpen Live-birth IntervalsPopulationPopulation DynamicsReproductionResearch Methodology","701":"Age FactorsDemographic FactorsMarital StatusMathematical ModelModels, TheoreticalNuptialityPopulationPopulation CharacteristicsResearch MethodologyWomen","702":"Age Specific Death RateAge Specific Fertility RateBirth RateCohort AnalysisError SourcesEstimation TechnicsMeasurementModels, TheoreticalMortalityPopulation Growth EstimationReliabilityResearch MethodologyResearch ReportUnited States","703":"Birth IntervalsData AnalysisDemographic AnalysisDemographic FactorsEvaluationEvaluation MethodologyFertilityFertility MeasurementsLife Table MethodOpen Live-birth IntervalsPopulationPopulation DynamicsResearch MethodologyStatisticsTechnical Report","704":"Age Specific Death RateAsiaDeath RateDemographic FactorsIndiaLife Table MethodMortalityPakistanPopulationPopulation CharacteristicsPopulation DynamicsResearch MethodologySex DistributionSex FactorsSex RatioSoutheastern AsiaSouthern AsiaSri LankaStatistical StudiesStudies","705":"Age FactorsAge Specific Fertility RateBirth RateCensusDemographic FactorsFertilityFertility MeasurementsInfant MortalityLife Table MethodModels, TheoreticalMortalityNet Reproduction RatePopulationPopulation CharacteristicsPopulation DynamicsPopulation GrowthReproductive PeriodResearch MethodologySex FactorsTechnical Report","706":"Data CollectionQuestionnaire DesignRandomized Response TechnicResearch MethodologySampling StudiesStudiesSurvey MethodologySurveys","707":"CensusCensus MethodsCodingError SourcesInformationInformation ProcessingMeasurementPopulation StatisticsReliabilityResearch MethodologyResearch ReportUnited States","708":"Age FactorsBlacksCensus MethodsDemographic FactorsEstimation TechnicsMeasurementMenMortality--changesPopulationPopulation DynamicsReliabilityResearch MethodologyResearch ReportUnited StatesWhitesWomen","709":"STATISTICS","710":"STATISTICS","711":"STATISTICS","712":"STATISTICS","713":"GENETICS","714":"STATISTICS","715":"POPULATIONSTATISTICS","716":"SEXSTATISTICS","717":"OLD AGESTATISTICS","718":"STATISTICS","719":"SOCIAL SECURITYSTATISTICS","720":"STATISTICS","721":"STATISTICS","722":"STATISTICS","723":"STATISTICS","724":"STATISTICS","725":"POPULATION","726":"STATISTICS","727":"STATISTICS","728":"POPULATION","729":"STATISTICS","730":"STATISTICS","731":"STATISTICS","732":"STATISTICS\/methods","733":"STATISTICS\/methods","734":"STATISTICS\/methods","735":"STATISTICS\/methods","736":"POPULATION\/in United StatesSTATISTICS","737":"POPULATION\/in United States","738":"POPULATION\/in Greece","739":"STATISTICS","740":"STATISTICS","741":"STATISTICS","742":"POPULATIONSTATISTICS","743":"INSURANCE\/sickness","744":"HOLLERITH MACHINESMATHEMATICS\/mechanicalSTATISTICS\/machines","745":"STATISTICSU.S. CENSUS BUREAUVITAL STATISTICS\/in U.S.","746":"STATISTICS","747":"STATISTICS\/sampling","748":"FERTILITY\/statisticsVITAL STATISTICS\/birthrate","749":"CLIMATE\/micro","750":"STATISTICS","751":"STATISTICS\/sampling","752":"MEDICINE\/stateSTATISTICS"},"language":{"0":"eng","1":"eng","2":"eng","3":"eng","4":"eng","5":"eng","6":"eng","7":"eng","8":"eng","9":"eng","10":"eng","11":"eng","12":"eng","13":"eng","14":"eng","15":"eng","16":"eng","17":"eng","18":"eng","19":"eng","20":"eng","21":"eng","22":"eng","23":"eng","24":"eng","25":"eng","26":"eng","27":"eng","28":"eng","29":"eng","30":"eng","31":"eng","32":"eng","33":"eng","34":"eng","35":"eng","36":"eng","37":"eng","38":"eng","39":"eng","40":"eng","41":"eng","42":"eng","43":"eng","44":"eng","45":"eng","46":"eng","47":"eng","48":"eng","49":"eng","50":"eng","51":"eng","52":"eng","53":"eng","54":"eng","55":"eng","56":"eng","57":"eng","58":"eng","59":"eng","60":"eng","61":"eng","62":"eng","63":"eng","64":"eng","65":"eng","66":"eng","67":"eng","68":"eng","69":"eng","70":"eng","71":"eng","72":"eng","73":"eng","74":"eng","75":"eng","76":"eng","77":"eng","78":"eng","79":"eng","80":"eng","81":"eng","82":"eng","83":"eng","84":"eng","85":"eng","86":"eng","87":"eng","88":"eng","89":"eng","90":"eng","91":"eng","92":"eng","93":"eng","94":"eng","95":"eng","96":"eng","97":"eng","98":"eng","99":"eng","100":"eng","101":"eng","102":"eng","103":"eng","104":"eng","105":"eng","106":"eng","107":"eng","108":"eng","109":"eng","110":"eng","111":"eng","112":"eng","113":"eng","114":"eng","115":"eng","116":"eng","117":"eng","118":"eng","119":"eng","120":"eng","121":"eng","122":"eng","123":"eng","124":"eng","125":"eng","126":"eng","127":"eng","128":"eng","129":"eng","130":"eng","131":"eng","132":"eng","133":"eng","134":"eng","135":"eng","136":"eng","137":"eng","138":"eng","139":"eng","140":"eng","141":"eng","142":"eng","143":"eng","144":"eng","145":"eng","146":"eng","147":"eng","148":"eng","149":"eng","150":"eng","151":"eng","152":"eng","153":"eng","154":"eng","155":"eng","156":"eng","157":"eng","158":"eng","159":"eng","160":"eng","161":"eng","162":"eng","163":"eng","164":"eng","165":"eng","166":"eng","167":"eng","168":"eng","169":"eng","170":"eng","171":"eng","172":"eng","173":"eng","174":"eng","175":"eng","176":"eng","177":"eng","178":"eng","179":"eng","180":"eng","181":"eng","182":"eng","183":"eng","184":"eng","185":"eng","186":"eng","187":"eng","188":"eng","189":"eng","190":"eng","191":"eng","192":"eng","193":"eng","194":"eng","195":"eng","196":"eng","197":"eng","198":"eng","199":"eng","200":"eng","201":"eng","202":"eng","203":"eng","204":"eng","205":"eng","206":"eng","207":"eng","208":"eng","209":"eng","210":"eng","211":"eng","212":"eng","213":"eng","214":"eng","215":"eng","216":"eng","217":"eng","218":"eng","219":"eng","220":"eng","221":"eng","222":"eng","223":"eng","224":"eng","225":"eng","226":"eng","227":"eng","228":"eng","229":"eng","230":"eng","231":"eng","232":"eng","233":"eng","234":"eng","235":"eng","236":"eng","237":"eng","238":"eng","239":"eng","240":"eng","241":"eng","242":"eng","243":"eng","244":"eng","245":"eng","246":"eng","247":"eng","248":"eng","249":"eng","250":"eng","251":"eng","252":"eng","253":"eng","254":"eng","255":"eng","256":"eng","257":"eng","258":"eng","259":"eng","260":"eng","261":"eng","262":"eng","263":"eng","264":"eng","265":"eng","266":"eng","267":"eng","268":"eng","269":"eng","270":"eng","271":"eng","272":"eng","273":"eng","274":"eng","275":"eng","276":"eng","277":"eng","278":"eng","279":"eng","280":"eng","281":"eng","282":"eng","283":"eng","284":"eng","285":"eng","286":"eng","287":"eng","288":"eng","289":"eng","290":"eng","291":"eng","292":"eng","293":"eng","294":"eng","295":"eng","296":"eng","297":"eng","298":"eng","299":"eng","300":"eng","301":"eng","302":"eng","303":"eng","304":"eng","305":"eng","306":"eng","307":"eng","308":"eng","309":"eng","310":"eng","311":"eng","312":"eng","313":"eng","314":"eng","315":"eng","316":"eng","317":"eng","318":"eng","319":"eng","320":"eng","321":"eng","322":"eng","323":"eng","324":"eng","325":"eng","326":"eng","327":"eng","328":"eng","329":"eng","330":"eng","331":"eng","332":"eng","333":"eng","334":"eng","335":"eng","336":"eng","337":"eng","338":"eng","339":"eng","340":"eng","341":"eng","342":"eng","343":"eng","344":"eng","345":"eng","346":"eng","347":"eng","348":"eng","349":"eng","350":"eng","351":"eng","352":"eng","353":"eng","354":"eng","355":"eng","356":"eng","357":"eng","358":"eng","359":"eng","360":"eng","361":"eng","362":"eng","363":"eng","364":"eng","365":"eng","366":"eng","367":"eng","368":"eng","369":"eng","370":"eng","371":"eng","372":"eng","373":"eng","374":"eng","375":"eng","376":"eng","377":"eng","378":"eng","379":"eng","380":"eng","381":"eng","382":"eng","383":"eng","384":"eng","385":"eng","386":"eng","387":"eng","388":"eng","389":"eng","390":"eng","391":"eng","392":"eng","393":"eng","394":"eng","395":"eng","396":"eng","397":"eng","398":"eng","399":"eng","400":"eng","401":"eng","402":"eng","403":"eng","404":"eng","405":"eng","406":"eng","407":"eng","408":"eng","409":"eng","410":"eng","411":"eng","412":"eng","413":"eng","414":"eng","415":"eng","416":"eng","417":"eng","418":"eng","419":"eng","420":"eng","421":"eng","422":"eng","423":"eng","424":"eng","425":"eng","426":"eng","427":"eng","428":"eng","429":"eng","430":"eng","431":"eng","432":"eng","433":"eng","434":"eng","435":"eng","436":"eng","437":"eng","438":"eng","439":"eng","440":"eng","441":"eng","442":"eng","443":"eng","444":"eng","445":"eng","446":"eng","447":"eng","448":"eng","449":"eng","450":"eng","451":"eng","452":"eng","453":"eng","454":"eng","455":"eng","456":"eng","457":"eng","458":"eng","459":"eng","460":"eng","461":"eng","462":"eng","463":"eng","464":"eng","465":"eng","466":"eng","467":"eng","468":"eng","469":"eng","470":"eng","471":"eng","472":"eng","473":"eng","474":"eng","475":"eng","476":"eng","477":"eng","478":"eng","479":"eng","480":"eng","481":"eng","482":"eng","483":"eng","484":"eng","485":"eng","486":"eng","487":"eng","488":"eng","489":"eng","490":"eng","491":"eng","492":"eng","493":"eng","494":"eng","495":"eng","496":"eng","497":"eng","498":"eng","499":"eng","500":"eng","501":"eng","502":"eng","503":"eng","504":"eng","505":"eng","506":"eng","507":"eng","508":"eng","509":"eng","510":"eng","511":"eng","512":"eng","513":"eng","514":"eng","515":"eng","516":"eng","517":"eng","518":"eng","519":"eng","520":"eng","521":"eng","522":"eng","523":"eng","524":"eng","525":"eng","526":"eng","527":"eng","528":"eng","529":"eng","530":"eng","531":"eng","532":"eng","533":"eng","534":"eng","535":"eng","536":"eng","537":"eng","538":"eng","539":"eng","540":"eng","541":"eng","542":"eng","543":"eng","544":"eng","545":"eng","546":"eng","547":"eng","548":"eng","549":"eng","550":"eng","551":"eng","552":"eng","553":"eng","554":"eng","555":"eng","556":"eng","557":"eng","558":"eng","559":"eng","560":"eng","561":"eng","562":"eng","563":"eng","564":"eng","565":"eng","566":"eng","567":"eng","568":"eng","569":"eng","570":"eng","571":"eng","572":"eng","573":"eng","574":"eng","575":"eng","576":"eng","577":"eng","578":"eng","579":"eng","580":"eng","581":"eng","582":"eng","583":"eng","584":"eng","585":"eng","586":"eng","587":"eng","588":"eng","589":"eng","590":"eng","591":"eng","592":"eng","593":"eng","594":"eng","595":"eng","596":"eng","597":"eng","598":"eng","599":"eng","600":"eng","601":"eng","602":"eng","603":"eng","604":"eng","605":"eng","606":"eng","607":"eng","608":"eng","609":"eng","610":"eng","611":"eng","612":"eng","613":"eng","614":"eng","615":"eng","616":"eng","617":"eng","618":"eng","619":"eng","620":"eng","621":"eng","622":"eng","623":"eng","624":"eng","625":"eng","626":"eng","627":"eng","628":"eng","629":"eng","630":"eng","631":"eng","632":"eng","633":"eng","634":"eng","635":"eng","636":"eng","637":"eng","638":"eng","639":"eng","640":"eng","641":"eng","642":"eng","643":"eng","644":"eng","645":"eng","646":"eng","647":"eng","648":"eng","649":"eng","650":"eng","651":"eng","652":"eng","653":"eng","654":"eng","655":"eng","656":"eng","657":"eng","658":"eng","659":"eng","660":"eng","661":"eng","662":"eng","663":"eng","664":"eng","665":"eng","666":"eng","667":"eng","668":"eng","669":"eng","670":"eng","671":"eng","672":"eng","673":"eng","674":"eng","675":"eng","676":"eng","677":"eng","678":"eng","679":"eng","680":"eng","681":"eng","682":"eng","683":"eng","684":"eng","685":"eng","686":"eng","687":"eng","688":"eng","689":"eng","690":"eng","691":"eng","692":"eng","693":"eng","694":"eng","695":"eng","696":"eng","697":"eng","698":"eng","699":"eng","700":"eng","701":"eng","702":"eng","703":"eng","704":"eng","705":"eng","706":"eng","707":"eng","708":"eng","709":"eng","710":"eng","711":"eng","712":"eng","713":"eng","714":"eng","715":"eng","716":"eng","717":"eng","718":"eng","719":"eng","720":"eng","721":"eng","722":"eng","723":"eng","724":"eng","725":"eng","726":"eng","727":"eng","728":"eng","729":"eng","730":"eng","731":"eng","732":"eng","733":"eng","734":"eng","735":"eng","736":"eng","737":"eng","738":"eng","739":"eng","740":"eng","741":"eng","742":"eng","743":"eng","744":"eng","745":"eng","746":"eng","747":"eng","748":"eng","749":"eng","750":"eng","751":"eng","752":"eng"},"pmid":{"0":35529781,"1":35400115,"2":35350190,"3":34955572,"4":34898761,"5":34898760,"6":34840367,"7":34776561,"8":34744216,"9":34650315,"10":34629570,"11":34548714,"12":34538987,"13":34531624,"14":34531623,"15":34483404,"16":34483403,"17":34421157,"18":34366505,"19":34354320,"20":34321704,"21":34305211,"22":34262233,"23":34248232,"24":34239216,"25":34239215,"26":34177008,"27":34177007,"28":34168390,"29":34121784,"30":34040267,"31":34024961,"32":33994604,"33":33814653,"34":33767517,"35":33737759,"36":33731969,"37":33731968,"38":33627920,"39":33568877,"40":33487782,"41":33424062,"42":33408427,"43":35515712,"44":35340357,"45":35261417,"46":35241863,"47":33311818,"48":33311817,"49":33299261,"50":33281249,"51":33139964,"52":33088006,"53":33060872,"54":33060871,"55":33041403,"56":33041402,"57":33041401,"58":33012903,"59":33012902,"60":33012901,"61":33012900,"62":33012899,"63":33012898,"64":32981992,"65":32981991,"66":32973370,"67":32952237,"68":32952236,"69":32952235,"70":32788818,"71":32773912,"72":32742045,"73":32742044,"74":32165869,"75":31942084,"76":31937981,"77":31929665,"78":31929664,"79":31920211,"80":34824484,"81":34413553,"82":34321703,"83":34219848,"84":34168389,"85":34113054,"86":34108777,"87":34012183,"88":33867602,"89":33716361,"90":33716360,"91":33716359,"92":33716358,"93":33716357,"94":33716356,"95":31777410,"96":31708716,"97":31692981,"98":31662589,"99":31649413,"100":31595098,"101":31588157,"102":31564761,"103":31564760,"104":31485091,"105":31474783,"106":31452558,"107":31447493,"108":31391793,"109":31354180,"110":33814652,"111":31235987,"112":31217649,"113":31190691,"114":31178611,"115":32905476,"116":31156284,"117":31130762,"118":31073256,"119":31057192,"120":30799887,"121":30799886,"122":30774169,"123":30739967,"124":30739966,"125":30739965,"126":35431372,"127":34385718,"128":34305210,"129":34290464,"130":33828349,"131":33311819,"132":33100436,"133":33033420,"134":32952234,"135":32863480,"136":32863479,"137":32405108,"138":32265576,"139":32255883,"140":30555194,"141":30546175,"142":30467447,"143":30467446,"144":30416233,"145":30416231,"146":30386004,"147":30294054,"148":30220754,"149":30122795,"150":30111897,"151":30093737,"152":30083023,"153":30078922,"154":30078921,"155":30078920,"156":32256246,"157":30034062,"158":30034061,"159":30034060,"160":29983475,"161":29930437,"162":29881130,"163":29479125,"164":29479124,"165":29456460,"166":29430070,"167":29430069,"168":29430068,"169":29416191,"170":29416190,"171":29375169,"172":29348701,"173":33093735,"174":31741544,"175":31548755,"176":31537952,"177":31456598,"178":31452557,"179":31354179,"180":31249430,"181":31086426,"182":31011234,"183":31007315,"184":30956371,"185":30906087,"186":30906086,"187":30906085,"188":30906084,"189":30906083,"190":30906082,"191":30853736,"192":30853735,"193":30853734,"194":30828120,"195":29276318,"196":29225385,"197":29200540,"198":29151658,"199":29151657,"200":28966418,"201":28966417,"202":28943684,"203":28943683,"204":28943682,"205":28943681,"206":28919653,"207":28890584,"208":28804182,"209":28736464,"210":28694552,"211":28603323,"212":28239216,"213":28138206,"214":28127109,"215":28090127,"216":28042189,"217":28042188,"218":33033419,"219":32863478,"220":32042214,"221":30505043,"222":30416232,"223":30100648,"224":30078919,"225":29881129,"226":29861517,"227":29780190,"228":29731529,"229":29628539,"230":29618851,"231":29606780,"232":29520120,"233":28018015,"234":28018014,"235":28018013,"236":28008210,"237":28003710,"238":28003709,"239":27990036,"240":27990035,"241":27990034,"242":27867238,"243":27647944,"244":27616801,"245":27570323,"246":27499564,"247":27330233,"248":27284211,"249":27226674,"250":27226673,"251":27217599,"252":27212739,"253":27212738,"254":27185970,"255":27019543,"256":26989274,"257":26973362,"258":26903687,"259":26877569,"260":26858467,"261":26855460,"262":26855459,"263":26848205,"264":26848204,"265":26839441,"266":26806987,"267":26783375,"268":32394991,"269":31983790,"270":31745375,"271":30686848,"272":29720777,"273":29503486,"274":29104333,"275":29097827,"276":28943680,"277":28757669,"278":28435175,"279":28366967,"280":28360436,"281":28316356,"282":28303074,"283":28286352,"284":28255189,"285":26692592,"286":26692591,"287":26617424,"288":26604425,"289":26576066,"290":26568645,"291":26560142,"292":26527845,"293":26527844,"294":26401064,"295":26392643,"296":26392642,"297":26366026,"298":26366025,"299":26294801,"300":26279594,"301":26273117,"302":26257452,"303":26236062,"304":26236061,"305":26236060,"306":26195849,"307":26190876,"308":26190875,"309":26170513,"310":26146425,"311":26120220,"312":26109745,"313":26085701,"314":26078481,"315":26063947,"316":25983358,"317":25954056,"318":25870462,"319":25729117,"320":25663727,"321":25663726,"322":25663725,"323":25663724,"324":25642006,"325":25642005,"326":25620825,"327":25587203,"328":25568502,"329":31210707,"330":29056798,"331":27695149,"332":27110045,"333":27087713,"334":27076691,"335":27073292,"336":25544787,"337":25544786,"338":25541568,"339":25506108,"340":25435598,"341":25419019,"342":25386043,"343":25382886,"344":25382885,"345":25382884,"346":25368436,"347":25368435,"348":25349462,"349":25346558,"350":25328261,"351":25309009,"352":25284918,"353":25242834,"354":25125767,"355":25071298,"356":25061254,"357":25018574,"358":24976662,"359":24976661,"360":24932056,"361":24764609,"362":24729646,"363":24729645,"364":24696528,"365":24683281,"366":24678135,"367":24678134,"368":24678133,"369":24678132,"370":24659838,"371":24659837,"372":24659836,"373":24504359,"374":24497670,"375":24482549,"376":24482548,"377":24436503,"378":24409003,"379":24376287,"380":26388659,"381":26269661,"382":26139951,"383":24363476,"384":24363475,"385":24319303,"386":24298194,"387":24293778,"388":24288421,"389":24288420,"390":24222928,"391":24222927,"392":24187408,"393":24174689,"394":24098061,"395":24076760,"396":24068849,"397":24058223,"398":24058222,"399":24039320,"400":24000265,"401":23997376,"402":23997375,"403":23990691,"404":23976805,"405":23913996,"406":23734068,"407":23667280,"408":23650442,"409":23645944,"410":23630406,"411":23606778,"412":23579202,"413":23543940,"414":23539471,"415":23482517,"416":23329858,"417":23293405,"418":23293404,"419":25328260,"420":25237208,"421":24791032,"422":24672142,"423":23264708,"424":25076802,"425":23243329,"426":25328259,"427":25301976,"428":25214699,"429":23125470,"430":23082036,"431":22956855,"432":22904583,"433":22844170,"434":25249709,"435":22745516,"436":22736876,"437":22523437,"438":22505787,"439":22467997,"440":22423170,"441":22375092,"442":22368314,"443":23908561,"444":23794769,"445":23667279,"446":22347760,"447":22323840,"448":22308015,"449":22279246,"450":27099406,"451":26664027,"452":26504255,"453":25400305,"454":24976660,"455":24729644,"456":24574574,"457":24504416,"458":24489419,"459":24489418,"460":24363474,"461":24347749,"462":24288419,"463":23828688,"464":23606777,"465":22162896,"466":22140288,"467":23997374,"468":22121305,"469":22053123,"470":22021943,"471":21987597,"472":21984854,"473":21931466,"474":21765566,"475":21760653,"476":23741079,"477":21706069,"478":21687809,"479":21666842,"480":21625372,"481":21516191,"482":21512611,"483":21472041,"484":23459794,"485":21318119,"486":28751799,"487":27524847,"488":26924867,"489":26478641,"490":26257451,"491":26246646,"492":26139950,"493":25580042,"494":24415813,"495":23704802,"496":23399559,"497":23204614,"498":22973069,"499":22844171,"500":22844169,"501":22754050,"502":22505788,"503":21165171,"504":21152360,"505":25477700,"506":21113386,"507":21113353,"508":21052523,"509":20882122,"510":20811510,"511":23144520,"512":20802823,"513":20711441,"514":20706564,"515":20689721,"516":20689644,"517":20676354,"518":20676229,"519":20664713,"520":20657760,"521":20625442,"522":20617121,"523":20448838,"524":20428335,"525":20396628,"526":20354594,"527":23559690,"528":23543902,"529":20351800,"530":20336182,"531":20333278,"532":20305802,"533":20305737,"534":20221312,"535":23729943,"536":21753862,"537":20174601,"538":20161093,"539":20161053,"540":20151036,"541":20148190,"542":20076767,"543":20057925,"544":20054431,"545":20046976,"546":35386273,"547":26640307,"548":24013644,"549":22865944,"550":22844168,"551":22798701,"552":22076026,"553":21494314,"554":20016767,"555":19960120,"556":19960103,"557":19956350,"558":23580793,"559":22888178,"560":21904418,"561":19890478,"562":19881892,"563":19830267,"564":19759835,"565":19750145,"566":21818167,"567":21057599,"568":19693282,"569":19655032,"570":19623271,"571":19562044,"572":19436775,"573":19430598,"574":19305513,"575":19300533,"576":20589222,"577":19180247,"578":19169433,"579":32523237,"580":21127725,"581":19081806,"582":19081744,"583":21603124,"584":21218139,"585":20539836,"586":20526381,"587":23729942,"588":22180690,"589":20407599,"590":18584067,"591":22375091,"592":20376193,"593":18418466,"594":18418465,"595":18392118,"596":21572926,"597":19830254,"598":19802353,"599":32148338,"600":29056797,"601":24453387,"602":20209021,"603":19655041,"604":19444331,"605":25620824,"606":19662105,"607":26279593,"608":21031152,"609":19707537,"610":19122791,"611":19079638,"612":20396669,"613":19759840,"614":20221318,"615":19169424,"616":20157621,"617":19096723,"618":20198124,"619":19838312,"620":24068850,"621":25705065,"622":21151838,"623":12155404,"624":12155402,"625":12155401,"626":25525284,"627":20019887,"628":24204084,"629":24244058,"630":12155403,"631":12155400,"632":12155399,"633":12155398,"634":12155397,"635":12155420,"636":12155419,"637":12155418,"638":12155417,"639":12155416,"640":12155396,"641":12155395,"642":12155394,"643":12155393,"644":12155392,"645":24790282,"646":12155415,"647":12155414,"648":12155413,"649":12155391,"650":12155390,"651":12155389,"652":12155388,"653":12155412,"654":12155387,"655":12155386,"656":12155385,"657":12155383,"658":12155382,"659":12155411,"660":12155384,"661":12155381,"662":12155380,"663":12155379,"664":12155378,"665":12155377,"666":12155410,"667":12155376,"668":12155424,"669":12155408,"670":12155405,"671":12155423,"672":12155422,"673":12155406,"674":12155421,"675":12155407,"676":12155409,"677":12267306,"678":12340317,"679":12340316,"680":12340389,"681":12340388,"682":12280322,"683":12265583,"684":12312151,"685":12264415,"686":12263447,"687":12263159,"688":12310719,"689":12310718,"690":12310708,"691":12310707,"692":12310706,"693":12146262,"694":12146261,"695":12260868,"696":12262745,"697":12333697,"698":12146224,"699":12146223,"700":12309324,"701":12276098,"702":12309295,"703":12264800,"704":12332319,"705":12146215,"706":12261830,"707":12335707,"708":12278071,"709":15407518,"710":15407517,"711":15395362,"712":15395361,"713":15395360,"714":15395359,"715":15395358,"716":15395357,"717":18148059,"718":18148058,"719":18139353,"720":18139352,"721":18139351,"722":18139350,"723":18131516,"724":18131515,"725":18131514,"726":18131513,"727":18131512,"728":18131511,"729":18123073,"730":18107568,"731":18107567,"732":18888138,"733":18888137,"734":18888136,"735":18888135,"736":20260627,"737":20260626,"738":20260625,"739":20252916,"740":20252915,"741":20252914,"742":20252913,"743":20295618,"744":20295617,"745":20295616,"746":20279351,"747":20279350,"748":20279349,"749":20990584,"750":20990583,"751":20990582,"752":21024545},"pubdate_year":{"0":2022,"1":2022,"2":2022,"3":2021,"4":2021,"5":2021,"6":2021,"7":2021,"8":2021,"9":2021,"10":2021,"11":2021,"12":2021,"13":2021,"14":2021,"15":2021,"16":2021,"17":2021,"18":2021,"19":2021,"20":2021,"21":2021,"22":2021,"23":2021,"24":2021,"25":2021,"26":2021,"27":2021,"28":2021,"29":2021,"30":2021,"31":2021,"32":2021,"33":2021,"34":2021,"35":2021,"36":2021,"37":2021,"38":2020,"39":2020,"40":2020,"41":2020,"42":2020,"43":2021,"44":2021,"45":2021,"46":2021,"47":2020,"48":2020,"49":2020,"50":2020,"51":2020,"52":2020,"53":2020,"54":2020,"55":2020,"56":2020,"57":2020,"58":2020,"59":2020,"60":2020,"61":2020,"62":2020,"63":2020,"64":2020,"65":2020,"66":2020,"67":2020,"68":2020,"69":2020,"70":2020,"71":2020,"72":2020,"73":2020,"74":2019,"75":2019,"76":2019,"77":2019,"78":2019,"79":2019,"80":2020,"81":2020,"82":2020,"83":2020,"84":2020,"85":2020,"86":2020,"87":2020,"88":2020,"89":2020,"90":2020,"91":2020,"92":2020,"93":2020,"94":2020,"95":2019,"96":2019,"97":2019,"98":2019,"99":2019,"100":2019,"101":2019,"102":2019,"103":2019,"104":2019,"105":2019,"106":2019,"107":2019,"108":2019,"109":2019,"110":2019,"111":2019,"112":2019,"113":2019,"114":2019,"115":2019,"116":2019,"117":2019,"118":2019,"119":2019,"120":2018,"121":2018,"122":2018,"123":2018,"124":2018,"125":2018,"126":2019,"127":2019,"128":2019,"129":2019,"130":2019,"131":2019,"132":2019,"133":2019,"134":2019,"135":2019,"136":2019,"137":2019,"138":2019,"139":2019,"140":2018,"141":2018,"142":2018,"143":2018,"144":2018,"145":2018,"146":2018,"147":2018,"148":2018,"149":2018,"150":2018,"151":2018,"152":2018,"153":2018,"154":2018,"155":2018,"156":2019,"157":2018,"158":2018,"159":2018,"160":2018,"161":2018,"162":2018,"163":2017,"164":"","165":2017,"166":2017,"167":2017,"168":2017,"169":2017,"170":2017,"171":2017,"172":2017,"173":2018,"174":2018,"175":2018,"176":2018,"177":2018,"178":2018,"179":2018,"180":2018,"181":2018,"182":2018,"183":2018,"184":2018,"185":2018,"186":2018,"187":2018,"188":2018,"189":2018,"190":2018,"191":2018,"192":2018,"193":2018,"194":2018,"195":2017,"196":2017,"197":2017,"198":2017,"199":2017,"200":2017,"201":2017,"202":2017,"203":2017,"204":2017,"205":2017,"206":"","207":2017,"208":2017,"209":2017,"210":2017,"211":2017,"212":2016,"213":2016,"214":2016,"215":2016,"216":2016,"217":2016,"218":2017,"219":2017,"220":2017,"221":2017,"222":2017,"223":2017,"224":2017,"225":2017,"226":2017,"227":2017,"228":2017,"229":2017,"230":2017,"231":2017,"232":2017,"233":2016,"234":2016,"235":2016,"236":2016,"237":2016,"238":2016,"239":2016,"240":2016,"241":2016,"242":"","243":"","244":2016,"245":2016,"246":2016,"247":2016,"248":2016,"249":2016,"250":2016,"251":2016,"252":2016,"253":2016,"254":"","255":2015,"256":2015,"257":2015,"258":"","259":"","260":2015,"261":2015,"262":"","263":"","264":"","265":2015,"266":"","267":2015,"268":2016,"269":2016,"270":2016,"271":2016,"272":2016,"273":2016,"274":2016,"275":2016,"276":2016,"277":2016,"278":2016,"279":2016,"280":2016,"281":2016,"282":2016,"283":"","284":2016,"285":2015,"286":2015,"287":"","288":2015,"289":"","290":"","291":2015,"292":"","293":"","294":2014,"295":2015,"296":"","297":2015,"298":2015,"299":"","300":2015,"301":2015,"302":2015,"303":"","304":2015,"305":2015,"306":2015,"307":2015,"308":"","309":2015,"310":2015,"311":2015,"312":2015,"313":"","314":2015,"315":2015,"316":2015,"317":2015,"318":2014,"319":2014,"320":2014,"321":2014,"322":2014,"323":2014,"324":2014,"325":2014,"326":2014,"327":"","328":2014,"329":2015,"330":2015,"331":2015,"332":2016,"333":2015,"334":2015,"335":2015,"336":2014,"337":2014,"338":2014,"339":2014,"340":2014,"341":2014,"342":2014,"343":2014,"344":2014,"345":2014,"346":2014,"347":2014,"348":2014,"349":2014,"350":2014,"351":2014,"352":2014,"353":2014,"354":2014,"355":2014,"356":2014,"357":2014,"358":2014,"359":2014,"360":2014,"361":2014,"362":2014,"363":2014,"364":2014,"365":2014,"366":2014,"367":2014,"368":2014,"369":2013,"370":2014,"371":2014,"372":2014,"373":2013,"374":2013,"375":2013,"376":2013,"377":2013,"378":2013,"379":2013,"380":"","381":2014,"382":"","383":2013,"384":2013,"385":2013,"386":2013,"387":2013,"388":2013,"389":2012,"390":2013,"391":2013,"392":2013,"393":2013,"394":2013,"395":2013,"396":2013,"397":2013,"398":2013,"399":2013,"400":2012,"401":2013,"402":2013,"403":2013,"404":2013,"405":2013,"406":2012,"407":2013,"408":2012,"409":2013,"410":2012,"411":2012,"412":2013,"413":2012,"414":2013,"415":2012,"416":2012,"417":2012,"418":2012,"419":2013,"420":2013,"421":2013,"422":2013,"423":2012,"424":2012,"425":2012,"426":2012,"427":2012,"428":2012,"429":2012,"430":2012,"431":2012,"432":2012,"433":2011,"434":2012,"435":2012,"436":2012,"437":2011,"438":2011,"439":2011,"440":2011,"441":2011,"442":2011,"443":2012,"444":2012,"445":2012,"446":2011,"447":2011,"448":2011,"449":2011,"450":"","451":"","452":"","453":2012,"454":2012,"455":2012,"456":2012,"457":2012,"458":2012,"459":2012,"460":2012,"461":2012,"462":2012,"463":2012,"464":2012,"465":2011,"466":2011,"467":2011,"468":2011,"469":2011,"470":2010,"471":2011,"472":2011,"473":2010,"474":2011,"475":2011,"476":2011,"477":2011,"478":2011,"479":2011,"480":2010,"481":2010,"482":2010,"483":2010,"484":2011,"485":2010,"486":2011,"487":"","488":"","489":"","490":"","491":"","492":"","493":2011,"494":2011,"495":2011,"496":2011,"497":2011,"498":2011,"499":2011,"500":2011,"501":2011,"502":2011,"503":2010,"504":2010,"505":2010,"506":2010,"507":2010,"508":2010,"509":2010,"510":2010,"511":2010,"512":2010,"513":2010,"514":2009,"515":2010,"516":2010,"517":2010,"518":2010,"519":2010,"520":2009,"521":2009,"522":2009,"523":2009,"524":2009,"525":2010,"526":2009,"527":2010,"528":2010,"529":2009,"530":2009,"531":2009,"532":2009,"533":2009,"534":2009,"535":2010,"536":2010,"537":2009,"538":2009,"539":2009,"540":2009,"541":2009,"542":2009,"543":2009,"544":2008,"545":2009,"546":2010,"547":2010,"548":2010,"549":2010,"550":2010,"551":2010,"552":2010,"553":2010,"554":2008,"555":2009,"556":2009,"557":2008,"558":2009,"559":2009,"560":2009,"561":2009,"562":2009,"563":2009,"564":2009,"565":2009,"566":2009,"567":2009,"568":2008,"569":2009,"570":2009,"571":2009,"572":2008,"573":2009,"574":2008,"575":2008,"576":2009,"577":2008,"578":2008,"579":2009,"580":2009,"581":2007,"582":2008,"583":2008,"584":2008,"585":2008,"586":2008,"587":2008,"588":2008,"589":2008,"590":2007,"591":2008,"592":2008,"593":2005,"594":2005,"595":2007,"596":2008,"597":2008,"598":2008,"599":2008,"600":2008,"601":2008,"602":2008,"603":2008,"604":2008,"605":2007,"606":2007,"607":2007,"608":2007,"609":2007,"610":2007,"611":2007,"612":2007,"613":2007,"614":2006,"615":2006,"616":2006,"617":2006,"618":2005,"619":2005,"620":2004,"621":2004,"622":2003,"623":1999,"624":1997,"625":1997,"626":2001,"627":2001,"628":2001,"629":1999,"630":1997,"631":1996,"632":1996,"633":1995,"634":1994,"635":1993,"636":1993,"637":1993,"638":1993,"639":1993,"640":1993,"641":1993,"642":1993,"643":1993,"644":1993,"645":1992,"646":1992,"647":1992,"648":1992,"649":1991,"650":1991,"651":1991,"652":1991,"653":1990,"654":1990,"655":1990,"656":1990,"657":1990,"658":1990,"659":1989,"660":1989,"661":1989,"662":1989,"663":1989,"664":1989,"665":1988,"666":1988,"667":1987,"668":1986,"669":1986,"670":1986,"671":1986,"672":1986,"673":1986,"674":1986,"675":1986,"676":1985,"677":1985,"678":1985,"679":1985,"680":1984,"681":1984,"682":1983,"683":1983,"684":1981,"685":1981,"686":1980,"687":1980,"688":1979,"689":1979,"690":1979,"691":1979,"692":1979,"693":1979,"694":1979,"695":1978,"696":1978,"697":1974,"698":1974,"699":1974,"700":1974,"701":1972,"702":1972,"703":1970,"704":1969,"705":1967,"706":1965,"707":1962,"708":1961,"709":1949,"710":1949,"711":1949,"712":1949,"713":1949,"714":1949,"715":1949,"716":1949,"717":1949,"718":1949,"719":1949,"720":1949,"721":1949,"722":1949,"723":1949,"724":1949,"725":1949,"726":1949,"727":1949,"728":1949,"729":1948,"730":1948,"731":1948,"732":1948,"733":1948,"734":1948,"735":1948,"736":1947,"737":1947,"738":1947,"739":1947,"740":1947,"741":1947,"742":1947,"743":1947,"744":1947,"745":1947,"746":1946,"747":1946,"748":1946,"749":1946,"750":1946,"751":1946,"752":1946}}